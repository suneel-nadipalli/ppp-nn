{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b54829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch, sys\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5493ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_for_runner(df, xp_features, xt_parts, cpr_setting='semi_synthetic', scenario='population', model_type='nn'):\n",
    "    \"\"\"\n",
    "    Prepares and encodes data for the specified CPR setting, scenario, and model type.\n",
    "    \"\"\"\n",
    "    working_df = df.copy()\n",
    "\n",
    "    # ====== Apply CPR Feature Setting ======\n",
    "    if cpr_setting == 'fixed':\n",
    "        # Apply fixed CPR logic here if you have one\n",
    "        pass  # ðŸ”¥ Add your fixed CPR logic if needed\n",
    "\n",
    "    # ====== Define Features ======\n",
    "    categorical_features = [col for col in xp_features if col not in ['religious_importance', 'dementia_worry']]\n",
    "    for part in xt_parts.values():\n",
    "        categorical_features.extend(part)\n",
    "    categorical_features = list(set(categorical_features))\n",
    "\n",
    "    df_encoded = working_df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "        encoders[col] = le\n",
    "\n",
    "    target_encoder = LabelEncoder()\n",
    "    df_encoded['choice'] = target_encoder.fit_transform(df_encoded['choice'])\n",
    "\n",
    "    baseline_features = [col for col in df_encoded.columns if col != 'choice']\n",
    "\n",
    "    # ðŸ‘‰ IMPORTANT: Keep X as a DataFrame (not NumPy)\n",
    "    X = df_encoded[baseline_features]\n",
    "    y = df_encoded['choice']\n",
    "\n",
    "    # ====== Apply Scenario ======\n",
    "    if scenario == 'agnostic':\n",
    "        X_copy = X.copy()\n",
    "        for col in X.columns:\n",
    "            if col in encoders:  # Categorical feature\n",
    "                X_copy[col] = 0  # Valid embedding index\n",
    "            else:  # Continuous feature\n",
    "                X_copy[col] = 0.0  # Neutral value for continuous features\n",
    "        X = X_copy\n",
    "    elif scenario == 'population':\n",
    "        if model_type == 'simple':\n",
    "            xp_indices = [i for i, col in enumerate(X.columns) if col in xp_features]\n",
    "            X.iloc[:, xp_indices] = 0\n",
    "        # NN: no adjustment\n",
    "    elif scenario == 'individual':\n",
    "        if model_type == 'nn':\n",
    "            for feature in xp_features:\n",
    "                if feature in encoders:\n",
    "                    # For categorical features: set to 0 (safe embedding index)\n",
    "                    X[feature] = 0\n",
    "                else:\n",
    "                    # For continuous features: set to 1 (any constant is fine)\n",
    "                    X[feature] = 1\n",
    "\n",
    "        # Simple models: no adjustment\n",
    "\n",
    "    return X, y, X.columns.tolist(), encoders, target_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66e95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def train_model_with_val(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30, device='cpu'):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    all_train_labels = []\n",
    "    all_train_preds = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for Xp_batch, Xt_parts_batch, y_batch in train_loader:\n",
    "            Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "            Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xp_batch, Xt_parts_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_train_labels.extend(y_batch.cpu().numpy())\n",
    "            all_train_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xp_batch, Xt_parts_batch, y_batch in val_loader:\n",
    "                Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "                Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                logits = model(Xp_batch, Xt_parts_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Generate final train loss report\n",
    "    train_loss_report = classification_report(all_train_labels, all_train_preds, zero_division=0, output_dict=True)\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies, train_loss_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bd31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def test_model_with_loss_report(model, test_loader, criterion, target_encoder, device='cpu'):\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_losses = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xp_batch, Xt_parts_batch, y_batch in test_loader:\n",
    "            Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "            Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(Xp_batch, Xt_parts_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            per_sample_loss = -torch.log(torch.gather(probs, 1, y_batch.unsqueeze(1))).squeeze(1)\n",
    "            all_losses.extend(per_sample_loss.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(all_losses)\n",
    "    accuracy = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=target_encoder.classes_, output_dict=True)\n",
    "\n",
    "    # Add loss per class\n",
    "    class_losses = {label: [] for label in np.unique(all_labels)}\n",
    "    for true_label, sample_loss in zip(all_labels, all_losses):\n",
    "        class_losses[true_label].append(sample_loss)\n",
    "\n",
    "    for cls_idx, losses in class_losses.items():\n",
    "        class_report[target_encoder.classes_[cls_idx]]['loss'] = np.mean(losses) if losses else 0.0\n",
    "\n",
    "    return avg_loss, accuracy, class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f4c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, log_loss, accuracy_score\n",
    "\n",
    "def train_simple_model_with_loss_report(model, X_train, y_train, target_encoder):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_probs = model.predict_proba(X_train)\n",
    "    train_preds = model.predict(X_train)\n",
    "\n",
    "    train_loss = log_loss(y_train, train_probs)\n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "\n",
    "    train_loss_report = classification_report(y_train, train_preds, zero_division=0, output_dict=True, target_names=target_encoder.classes_)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(\"\\n===== Train Classification Report =====\\n\")\n",
    "    print(train_loss_report)\n",
    "\n",
    "    return train_loss, train_acc, train_loss_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e49231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_model_with_loss_report(model, X_test, y_test, target_encoder):\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    test_loss = log_loss(y_test, test_probs)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "    test_loss_report = classification_report(y_test, test_preds, zero_division=0, output_dict=True, target_names=target_encoder.classes_)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "    print(\"\\n===== Test Classification Report =====\\n\")\n",
    "    print(test_loss_report)\n",
    "\n",
    "    return test_loss, test_acc, test_loss_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a869e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiClassTreatmentDataset(Dataset):\n",
    "    def __init__(self, X, y, xp_features, xt_parts):\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y.reset_index(drop=True)\n",
    "        self.xp_features = xp_features\n",
    "        self.xt_parts = xt_parts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "\n",
    "        # Extract Xp\n",
    "        Xp = {feature: torch.tensor(row[feature], dtype=torch.float32).unsqueeze(0) for feature in self.xp_features}\n",
    "\n",
    "        # Extract Xt parts\n",
    "        Xt_parts = {}\n",
    "        for part_name, features in self.xt_parts.items():\n",
    "            Xt_parts[part_name] = {feature: torch.tensor(row[feature], dtype=torch.float32).unsqueeze(0) for feature in features}\n",
    "\n",
    "        # Target\n",
    "        y_target = torch.tensor(self.y.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        return Xp, Xt_parts, y_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ac2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModularMultiClassTreatmentModel(nn.Module):\n",
    "    def __init__(self, xp_features, xt_parts, embedding_sizes, num_classes):\n",
    "        super(ModularMultiClassTreatmentModel, self).__init__()\n",
    "\n",
    "        self.xp_features = xp_features\n",
    "        self.xt_parts = xt_parts\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # ===== Embeddings =====\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for feature, num_categories in embedding_sizes.items():\n",
    "            self.embeddings[feature] = nn.Embedding(num_embeddings=num_categories, embedding_dim=4)\n",
    "\n",
    "        # ===== Patient Encoder =====\n",
    "        patient_input_dim = self.calculate_total_embedding_dim(xp_features)\n",
    "        self.patient_encoder = nn.Sequential(\n",
    "            nn.Linear(patient_input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # ===== Xt Part Encoders =====\n",
    "        self.part_encoders = nn.ModuleDict()\n",
    "        for part_name, features in xt_parts.items():\n",
    "            part_input_dim = self.calculate_total_embedding_dim(features)\n",
    "            self.part_encoders[part_name] = nn.Sequential(\n",
    "                nn.Linear(part_input_dim, 64),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # ===== Attention Mechanism =====\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        # ===== Final Decision Head =====\n",
    "        self.decision_head = nn.Sequential(\n",
    "            nn.Linear(64 + 64, 128),  # patient vector + weighted Xt vector\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)  # Multi-class output\n",
    "        )\n",
    "\n",
    "    def calculate_total_embedding_dim(self, features):\n",
    "        dim = 0\n",
    "        for feature in features:\n",
    "            dim += 4 if feature in self.embeddings else 1  # Categorical: embedding | Continuous: 1-dim\n",
    "        return dim\n",
    "\n",
    "    def forward(self, Xp, Xt_parts):\n",
    "        # ===== Process Patient Features =====\n",
    "        patient_embeds = []\n",
    "        for feature in self.xp_features:\n",
    "            if feature in self.embeddings:\n",
    "                patient_embeds.append(self.embeddings[feature](Xp[feature].long()).squeeze(1))\n",
    "            else:\n",
    "                patient_embeds.append(Xp[feature].unsqueeze(1))  # ðŸ”¥ FIXED HERE\n",
    "\n",
    "        patient_embeds = torch.cat(patient_embeds, dim=1)\n",
    "        patient_vector = self.patient_encoder(patient_embeds)\n",
    "\n",
    "        # ===== Process Xt Parts =====\n",
    "        part_vectors = []\n",
    "        attention_scores = []\n",
    "\n",
    "        for part_name, features in self.xt_parts.items():\n",
    "            part_embeds = []\n",
    "            for feature in features:\n",
    "                if feature in self.embeddings:\n",
    "                    part_embeds.append(self.embeddings[feature](Xt_parts[part_name][feature].long()).squeeze(1))\n",
    "                else:\n",
    "                    part_embeds.append(Xt_parts[part_name][feature].unsqueeze(1))  # ðŸ”¥ FIXED HERE\n",
    "\n",
    "            part_embeds = torch.cat(part_embeds, dim=1)\n",
    "            part_vector = self.part_encoders[part_name](part_embeds)\n",
    "            part_vectors.append(part_vector)\n",
    "\n",
    "            attn_score = self.attention(part_vector)\n",
    "            attention_scores.append(attn_score)\n",
    "\n",
    "        # ===== Attention Weighted Sum =====\n",
    "        attention_scores = torch.cat(attention_scores, dim=1)\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        weighted_Xt = torch.stack(part_vectors, dim=1)  # Shape: [batch_size, num_parts, 64]\n",
    "        weighted_Xt = (attn_weights.unsqueeze(2) * weighted_Xt).sum(dim=1)\n",
    "\n",
    "        # ===== Final Decision =====\n",
    "        combined = torch.cat([patient_vector, weighted_Xt], dim=1)\n",
    "        logits = self.decision_head(combined)\n",
    "\n",
    "        return logits  # Raw logits for CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa475be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def run_full_experiment(df, xp_features, xt_parts, num_folds=1, num_epochs=1):\n",
    "    scenarios = ['agnostic', 'population', 'individual']\n",
    "    cpr_settings = ['semi_synthetic', 'fixed']\n",
    "    model_types = ['nn', 'simple']\n",
    "\n",
    "    report_collector = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {\n",
    "        \"summary\": {},  # To be computed later\n",
    "        \"folds\": {}\n",
    "    })))\n",
    "\n",
    "    for cpr_setting in cpr_settings:\n",
    "        for scenario in scenarios:\n",
    "            for model_type in model_types:\n",
    "                print(f\"\\n===== Running: {cpr_setting} | {scenario} | {model_type} =====\")\n",
    "\n",
    "                X, y, feature_names, encoders, target_encoder = prepare_data_for_runner(\n",
    "                    df, cpr_setting=cpr_setting, scenario=scenario, model_type=model_type,\n",
    "                    xp_features=xp_features, xt_parts=xt_parts\n",
    "                )\n",
    "\n",
    "                kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "                fold_counter = 1\n",
    "                for train_index, test_index in kf.split(X, y):\n",
    "                    print(f\"\\n--- Fold {fold_counter} ---\")\n",
    "\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                    if model_type == 'nn':\n",
    "                        train_dataset = MultiClassTreatmentDataset(pd.DataFrame(X_train, columns=feature_names), pd.Series(y_train), xp_features, xt_parts)\n",
    "                        test_dataset = MultiClassTreatmentDataset(pd.DataFrame(X_test, columns=feature_names), pd.Series(y_test), xp_features, xt_parts)\n",
    "\n",
    "                        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "                        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "                        embedding_sizes = {}\n",
    "                        for col in feature_names:\n",
    "                            if col in encoders:\n",
    "                                embedding_sizes[col] = len(encoders[col].classes_)\n",
    "\n",
    "                        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                        model = ModularMultiClassTreatmentModel(xp_features, xt_parts, embedding_sizes, num_classes=len(target_encoder.classes_))\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "                        train_losses, train_accuracies, val_losses, val_accuracies, train_loss_report = train_model_with_val(\n",
    "                            model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=num_epochs, device=device\n",
    "                        )\n",
    "\n",
    "                        test_loss, test_acc, test_loss_report = test_model_with_loss_report(\n",
    "                            model, test_loader, criterion, target_encoder, device=device\n",
    "                        )\n",
    "\n",
    "                        report_collector[cpr_setting][scenario][model_type][\"folds\"][str(fold_counter)] = {\n",
    "                            \"accuracy\": test_acc,\n",
    "                            \"precision\": None,  # Optional: Fill if calculated\n",
    "                            \"recall\": None,     # Optional: Fill if calculated\n",
    "                            \"f1_score\": None,   # Optional: Fill if calculated\n",
    "                            \"train_loss_history\": train_losses,\n",
    "                            \"train_acc_history\": train_accuracies,\n",
    "                            \"val_loss_history\": val_losses,\n",
    "                            \"val_acc_history\": val_accuracies,\n",
    "                            \"classification_report\": {\n",
    "                                \"accuracy_report\": train_loss_report,  # Use this if it's per-class accuracy\n",
    "                                \"loss_report\": test_loss_report        # Use this if it's per-class loss\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                    else:\n",
    "                        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "                        model = LogisticRegression(max_iter=1000)\n",
    "                        train_loss, train_acc, train_loss_report = train_simple_model_with_loss_report(model, X_train, y_train, target_encoder)\n",
    "                        test_loss, test_acc, test_loss_report = test_simple_model_with_loss_report(model, X_test, y_test, target_encoder)\n",
    "\n",
    "                        report_collector[cpr_setting][scenario][model_type][\"folds\"][str(fold_counter)] = {\n",
    "                            \"accuracy\": test_acc,\n",
    "                            \"precision\": None,  # Optional: Fill if calculated\n",
    "                            \"recall\": None,     # Optional: Fill if calculated\n",
    "                            \"f1_score\": None,   # Optional: Fill if calculated\n",
    "                            \"train_loss_history\": [train_loss],\n",
    "                            \"train_acc_history\": [train_acc],\n",
    "                            \"val_loss_history\": [],\n",
    "                            \"val_acc_history\": [],\n",
    "                            \"classification_report\": {\n",
    "                                \"accuracy_report\": train_loss_report,\n",
    "                                \"loss_report\": test_loss_report\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                    fold_counter += 1\n",
    "\n",
    "    print(\"\\nâœ… All runs complete! You can now proceed to compute summaries and generate HTML reports.\")\n",
    "\n",
    "    return report_collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5f72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997ad3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running: semi_synthetic | agnostic | nn =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch [1/30] Train Loss: 0.5853 | Train Acc: 0.7420 | Val Loss: 0.6097 | Val Acc: 0.7673\n",
      "Epoch [2/30] Train Loss: 0.5638 | Train Acc: 0.7649 | Val Loss: 4.1249 | Val Acc: 0.7673\n",
      "Epoch [3/30] Train Loss: 0.5544 | Train Acc: 0.7652 | Val Loss: 203.0509 | Val Acc: 0.7673\n",
      "Epoch [4/30] Train Loss: 0.5548 | Train Acc: 0.7660 | Val Loss: 96.1202 | Val Acc: 0.7673\n",
      "Epoch [5/30] Train Loss: 0.5538 | Train Acc: 0.7652 | Val Loss: 104.6798 | Val Acc: 0.7673\n",
      "Epoch [6/30] Train Loss: 0.5548 | Train Acc: 0.7652 | Val Loss: 129.1451 | Val Acc: 0.7673\n",
      "Epoch [7/30] Train Loss: 0.5554 | Train Acc: 0.7645 | Val Loss: 116.4091 | Val Acc: 0.7673\n",
      "Epoch [8/30] Train Loss: 0.5468 | Train Acc: 0.7660 | Val Loss: 7.7944 | Val Acc: 0.7673\n",
      "Epoch [9/30] Train Loss: 0.5516 | Train Acc: 0.7656 | Val Loss: 3.8832 | Val Acc: 0.7673\n",
      "Epoch [10/30] Train Loss: 0.5470 | Train Acc: 0.7660 | Val Loss: 64.3776 | Val Acc: 0.7673\n",
      "Epoch [11/30] Train Loss: 0.5488 | Train Acc: 0.7660 | Val Loss: 27.0081 | Val Acc: 0.7673\n",
      "Epoch [12/30] Train Loss: 0.5442 | Train Acc: 0.7675 | Val Loss: 20.6304 | Val Acc: 0.7673\n",
      "Epoch [13/30] Train Loss: 0.5478 | Train Acc: 0.7671 | Val Loss: 0.9539 | Val Acc: 0.2327\n",
      "Epoch [14/30] Train Loss: 0.5471 | Train Acc: 0.7660 | Val Loss: 3.0800 | Val Acc: 0.7673\n",
      "Epoch [15/30] Train Loss: 0.5477 | Train Acc: 0.7656 | Val Loss: 14.1933 | Val Acc: 0.7673\n",
      "Epoch [16/30] Train Loss: 0.5452 | Train Acc: 0.7660 | Val Loss: 19.5554 | Val Acc: 0.7673\n",
      "Epoch [17/30] Train Loss: 0.5477 | Train Acc: 0.7652 | Val Loss: 15.3896 | Val Acc: 0.7673\n",
      "Epoch [18/30] Train Loss: 0.5483 | Train Acc: 0.7637 | Val Loss: 8.0635 | Val Acc: 0.7673\n",
      "Epoch [19/30] Train Loss: 0.5498 | Train Acc: 0.7649 | Val Loss: 10.3840 | Val Acc: 0.7673\n",
      "Epoch [20/30] Train Loss: 0.5473 | Train Acc: 0.7668 | Val Loss: 9.5441 | Val Acc: 0.7673\n",
      "Epoch [21/30] Train Loss: 0.5488 | Train Acc: 0.7649 | Val Loss: 2.5875 | Val Acc: 0.7673\n",
      "Epoch [22/30] Train Loss: 0.5457 | Train Acc: 0.7656 | Val Loss: 1.6443 | Val Acc: 0.7673\n",
      "Epoch [23/30] Train Loss: 0.5440 | Train Acc: 0.7675 | Val Loss: 3.1088 | Val Acc: 0.7673\n",
      "Epoch [24/30] Train Loss: 0.5456 | Train Acc: 0.7664 | Val Loss: 0.7741 | Val Acc: 0.2327\n",
      "Epoch [25/30] Train Loss: 0.5440 | Train Acc: 0.7675 | Val Loss: 0.6821 | Val Acc: 0.7673\n",
      "Epoch [26/30] Train Loss: 0.5456 | Train Acc: 0.7660 | Val Loss: 3.1399 | Val Acc: 0.2327\n",
      "Epoch [27/30] Train Loss: 0.5497 | Train Acc: 0.7652 | Val Loss: 1.2942 | Val Acc: 0.7673\n",
      "Epoch [28/30] Train Loss: 0.5456 | Train Acc: 0.7668 | Val Loss: 3.2159 | Val Acc: 0.7673\n",
      "Epoch [29/30] Train Loss: 0.5455 | Train Acc: 0.7664 | Val Loss: 0.6336 | Val Acc: 0.7673\n",
      "Epoch [30/30] Train Loss: 0.5465 | Train Acc: 0.7656 | Val Loss: 9.3275 | Val Acc: 0.7673\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2 ---\n",
      "Epoch [1/30] Train Loss: 0.6215 | Train Acc: 0.6822 | Val Loss: 0.6089 | Val Acc: 0.7673\n",
      "Epoch [2/30] Train Loss: 0.5574 | Train Acc: 0.7641 | Val Loss: 0.5832 | Val Acc: 0.7673\n",
      "Epoch [3/30] Train Loss: 0.5545 | Train Acc: 0.7664 | Val Loss: 13.7002 | Val Acc: 0.7673\n",
      "Epoch [4/30] Train Loss: 0.5560 | Train Acc: 0.7656 | Val Loss: 28.1591 | Val Acc: 0.7673\n",
      "Epoch [5/30] Train Loss: 0.5549 | Train Acc: 0.7664 | Val Loss: 27.7063 | Val Acc: 0.7673\n",
      "Epoch [6/30] Train Loss: 0.5536 | Train Acc: 0.7656 | Val Loss: 18.1242 | Val Acc: 0.7673\n",
      "Epoch [7/30] Train Loss: 0.5481 | Train Acc: 0.7649 | Val Loss: 56.7586 | Val Acc: 0.7673\n",
      "Epoch [8/30] Train Loss: 0.5505 | Train Acc: 0.7652 | Val Loss: 85.1215 | Val Acc: 0.2327\n",
      "Epoch [9/30] Train Loss: 0.5490 | Train Acc: 0.7649 | Val Loss: 1.1608 | Val Acc: 0.7673\n",
      "Epoch [10/30] Train Loss: 0.5524 | Train Acc: 0.7671 | Val Loss: 6.6608 | Val Acc: 0.7673\n",
      "Epoch [11/30] Train Loss: 0.5472 | Train Acc: 0.7671 | Val Loss: 10.1888 | Val Acc: 0.7673\n",
      "Epoch [12/30] Train Loss: 0.5488 | Train Acc: 0.7668 | Val Loss: 0.8857 | Val Acc: 0.7673\n",
      "Epoch [13/30] Train Loss: 0.5487 | Train Acc: 0.7649 | Val Loss: 6.5311 | Val Acc: 0.7673\n",
      "Epoch [14/30] Train Loss: 0.5499 | Train Acc: 0.7645 | Val Loss: 1.4356 | Val Acc: 0.7673\n",
      "Epoch [15/30] Train Loss: 0.5458 | Train Acc: 0.7660 | Val Loss: 7.4659 | Val Acc: 0.7673\n",
      "Epoch [16/30] Train Loss: 0.5466 | Train Acc: 0.7660 | Val Loss: 1.3716 | Val Acc: 0.7673\n",
      "Epoch [17/30] Train Loss: 0.5472 | Train Acc: 0.7668 | Val Loss: 3.6341 | Val Acc: 0.7673\n",
      "Epoch [18/30] Train Loss: 0.5468 | Train Acc: 0.7652 | Val Loss: 7.7648 | Val Acc: 0.7673\n",
      "Epoch [19/30] Train Loss: 0.5439 | Train Acc: 0.7683 | Val Loss: 4.2956 | Val Acc: 0.7673\n",
      "Epoch [20/30] Train Loss: 0.5464 | Train Acc: 0.7656 | Val Loss: 11.1839 | Val Acc: 0.7673\n",
      "Epoch [21/30] Train Loss: 0.5444 | Train Acc: 0.7664 | Val Loss: 5.0080 | Val Acc: 0.7673\n",
      "Epoch [22/30] Train Loss: 0.5434 | Train Acc: 0.7675 | Val Loss: 7.8908 | Val Acc: 0.7673\n",
      "Epoch [23/30] Train Loss: 0.5473 | Train Acc: 0.7664 | Val Loss: 0.9994 | Val Acc: 0.7673\n",
      "Epoch [24/30] Train Loss: 0.5455 | Train Acc: 0.7660 | Val Loss: 1.2099 | Val Acc: 0.7673\n",
      "Epoch [25/30] Train Loss: 0.5485 | Train Acc: 0.7664 | Val Loss: 1.4371 | Val Acc: 0.7673\n",
      "Epoch [26/30] Train Loss: 0.5471 | Train Acc: 0.7656 | Val Loss: 0.8039 | Val Acc: 0.7673\n",
      "Epoch [27/30] Train Loss: 0.5472 | Train Acc: 0.7649 | Val Loss: 0.6398 | Val Acc: 0.7673\n",
      "Epoch [28/30] Train Loss: 0.5459 | Train Acc: 0.7660 | Val Loss: 2.4281 | Val Acc: 0.7673\n",
      "Epoch [29/30] Train Loss: 0.5458 | Train Acc: 0.7671 | Val Loss: 1.2646 | Val Acc: 0.7673\n",
      "Epoch [30/30] Train Loss: 0.5470 | Train Acc: 0.7660 | Val Loss: 0.5350 | Val Acc: 0.7673\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 3 ---\n",
      "Epoch [1/30] Train Loss: 0.5695 | Train Acc: 0.7561 | Val Loss: 0.5421 | Val Acc: 0.7658\n",
      "Epoch [2/30] Train Loss: 0.5543 | Train Acc: 0.7668 | Val Loss: 20.2833 | Val Acc: 0.2342\n",
      "Epoch [3/30] Train Loss: 0.5478 | Train Acc: 0.7675 | Val Loss: 1.1939 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.5534 | Train Acc: 0.7660 | Val Loss: 44.9340 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.5523 | Train Acc: 0.7687 | Val Loss: 94.5606 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.5497 | Train Acc: 0.7664 | Val Loss: 38.7383 | Val Acc: 0.7658\n",
      "Epoch [7/30] Train Loss: 0.5518 | Train Acc: 0.7664 | Val Loss: 42.6356 | Val Acc: 0.7658\n",
      "Epoch [8/30] Train Loss: 0.5468 | Train Acc: 0.7687 | Val Loss: 13.0636 | Val Acc: 0.7658\n",
      "Epoch [9/30] Train Loss: 0.5509 | Train Acc: 0.7675 | Val Loss: 16.9170 | Val Acc: 0.7658\n",
      "Epoch [10/30] Train Loss: 0.5509 | Train Acc: 0.7660 | Val Loss: 7.0573 | Val Acc: 0.7658\n",
      "Epoch [11/30] Train Loss: 0.5476 | Train Acc: 0.7660 | Val Loss: 11.1643 | Val Acc: 0.2342\n",
      "Epoch [12/30] Train Loss: 0.5534 | Train Acc: 0.7637 | Val Loss: 5.6599 | Val Acc: 0.7658\n",
      "Epoch [13/30] Train Loss: 0.5452 | Train Acc: 0.7668 | Val Loss: 4.4573 | Val Acc: 0.7658\n",
      "Epoch [14/30] Train Loss: 0.5497 | Train Acc: 0.7660 | Val Loss: 3.1915 | Val Acc: 0.2342\n",
      "Epoch [15/30] Train Loss: 0.5475 | Train Acc: 0.7660 | Val Loss: 4.8805 | Val Acc: 0.7658\n",
      "Epoch [16/30] Train Loss: 0.5451 | Train Acc: 0.7675 | Val Loss: 3.9972 | Val Acc: 0.7658\n",
      "Epoch [17/30] Train Loss: 0.5464 | Train Acc: 0.7656 | Val Loss: 10.4664 | Val Acc: 0.7658\n",
      "Epoch [18/30] Train Loss: 0.5486 | Train Acc: 0.7649 | Val Loss: 4.8021 | Val Acc: 0.7658\n",
      "Epoch [19/30] Train Loss: 0.5457 | Train Acc: 0.7660 | Val Loss: 2.0929 | Val Acc: 0.7658\n",
      "Epoch [20/30] Train Loss: 0.5458 | Train Acc: 0.7671 | Val Loss: 5.3400 | Val Acc: 0.7658\n",
      "Epoch [21/30] Train Loss: 0.5440 | Train Acc: 0.7660 | Val Loss: 3.3213 | Val Acc: 0.7658\n",
      "Epoch [22/30] Train Loss: 0.5454 | Train Acc: 0.7660 | Val Loss: 1.7848 | Val Acc: 0.7658\n",
      "Epoch [23/30] Train Loss: 0.5447 | Train Acc: 0.7675 | Val Loss: 4.4286 | Val Acc: 0.7658\n",
      "Epoch [24/30] Train Loss: 0.5428 | Train Acc: 0.7660 | Val Loss: 1.9123 | Val Acc: 0.7658\n",
      "Epoch [25/30] Train Loss: 0.5456 | Train Acc: 0.7668 | Val Loss: 6.1250 | Val Acc: 0.2342\n",
      "Epoch [26/30] Train Loss: 0.5470 | Train Acc: 0.7660 | Val Loss: 10.1701 | Val Acc: 0.7658\n",
      "Epoch [27/30] Train Loss: 0.5460 | Train Acc: 0.7671 | Val Loss: 6.4386 | Val Acc: 0.7658\n",
      "Epoch [28/30] Train Loss: 0.5480 | Train Acc: 0.7656 | Val Loss: 1.1971 | Val Acc: 0.7658\n",
      "Epoch [29/30] Train Loss: 0.5458 | Train Acc: 0.7668 | Val Loss: 0.6026 | Val Acc: 0.7658\n",
      "Epoch [30/30] Train Loss: 0.5452 | Train Acc: 0.7668 | Val Loss: 0.5316 | Val Acc: 0.7658\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 4 ---\n",
      "Epoch [1/30] Train Loss: 0.6038 | Train Acc: 0.6936 | Val Loss: 0.5505 | Val Acc: 0.7658\n",
      "Epoch [2/30] Train Loss: 0.5625 | Train Acc: 0.7652 | Val Loss: 7.0503 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.5552 | Train Acc: 0.7668 | Val Loss: 57.8419 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.5529 | Train Acc: 0.7660 | Val Loss: 43.8575 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.5480 | Train Acc: 0.7668 | Val Loss: 0.5634 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.5551 | Train Acc: 0.7660 | Val Loss: 172.3379 | Val Acc: 0.2342\n",
      "Epoch [7/30] Train Loss: 0.5494 | Train Acc: 0.7668 | Val Loss: 8.5290 | Val Acc: 0.7658\n",
      "Epoch [8/30] Train Loss: 0.5587 | Train Acc: 0.7637 | Val Loss: 1.1602 | Val Acc: 0.2342\n",
      "Epoch [9/30] Train Loss: 0.5497 | Train Acc: 0.7671 | Val Loss: 8.0638 | Val Acc: 0.7658\n",
      "Epoch [10/30] Train Loss: 0.5481 | Train Acc: 0.7649 | Val Loss: 15.3664 | Val Acc: 0.7658\n",
      "Epoch [11/30] Train Loss: 0.5485 | Train Acc: 0.7649 | Val Loss: 8.1711 | Val Acc: 0.7658\n",
      "Epoch [12/30] Train Loss: 0.5461 | Train Acc: 0.7656 | Val Loss: 19.8931 | Val Acc: 0.7658\n",
      "Epoch [13/30] Train Loss: 0.5477 | Train Acc: 0.7664 | Val Loss: 2.7132 | Val Acc: 0.2342\n",
      "Epoch [14/30] Train Loss: 0.5457 | Train Acc: 0.7671 | Val Loss: 9.4596 | Val Acc: 0.7658\n",
      "Epoch [15/30] Train Loss: 0.5476 | Train Acc: 0.7668 | Val Loss: 19.5119 | Val Acc: 0.7658\n",
      "Epoch [16/30] Train Loss: 0.5461 | Train Acc: 0.7675 | Val Loss: 1.6920 | Val Acc: 0.2342\n",
      "Epoch [17/30] Train Loss: 0.5506 | Train Acc: 0.7649 | Val Loss: 18.5389 | Val Acc: 0.7658\n",
      "Epoch [18/30] Train Loss: 0.5464 | Train Acc: 0.7656 | Val Loss: 1.3583 | Val Acc: 0.7658\n",
      "Epoch [19/30] Train Loss: 0.5477 | Train Acc: 0.7656 | Val Loss: 24.0338 | Val Acc: 0.2342\n",
      "Epoch [20/30] Train Loss: 0.5444 | Train Acc: 0.7683 | Val Loss: 4.1225 | Val Acc: 0.2342\n",
      "Epoch [21/30] Train Loss: 0.5449 | Train Acc: 0.7671 | Val Loss: 0.5484 | Val Acc: 0.7658\n",
      "Epoch [22/30] Train Loss: 0.5483 | Train Acc: 0.7660 | Val Loss: 0.5745 | Val Acc: 0.7658\n",
      "Epoch [23/30] Train Loss: 0.5447 | Train Acc: 0.7664 | Val Loss: 12.5256 | Val Acc: 0.7658\n",
      "Epoch [24/30] Train Loss: 0.5433 | Train Acc: 0.7683 | Val Loss: 0.5463 | Val Acc: 0.7658\n",
      "Epoch [25/30] Train Loss: 0.5472 | Train Acc: 0.7652 | Val Loss: 9.5434 | Val Acc: 0.7658\n",
      "Epoch [26/30] Train Loss: 0.5464 | Train Acc: 0.7660 | Val Loss: 14.4737 | Val Acc: 0.7658\n",
      "Epoch [27/30] Train Loss: 0.5456 | Train Acc: 0.7671 | Val Loss: 6.1529 | Val Acc: 0.7658\n",
      "Epoch [28/30] Train Loss: 0.5470 | Train Acc: 0.7652 | Val Loss: 14.4042 | Val Acc: 0.7658\n",
      "Epoch [29/30] Train Loss: 0.5466 | Train Acc: 0.7671 | Val Loss: 3.5935 | Val Acc: 0.7658\n",
      "Epoch [30/30] Train Loss: 0.5451 | Train Acc: 0.7660 | Val Loss: 0.6423 | Val Acc: 0.7658\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 5 ---\n",
      "Epoch [1/30] Train Loss: 0.5859 | Train Acc: 0.7325 | Val Loss: 0.5641 | Val Acc: 0.7658\n",
      "Epoch [2/30] Train Loss: 0.5572 | Train Acc: 0.7664 | Val Loss: 23.7251 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.5550 | Train Acc: 0.7671 | Val Loss: 8.8869 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.5524 | Train Acc: 0.7668 | Val Loss: 66.1171 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.5468 | Train Acc: 0.7671 | Val Loss: 21.4608 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.5540 | Train Acc: 0.7652 | Val Loss: 5.8913 | Val Acc: 0.7658\n",
      "Epoch [7/30] Train Loss: 0.5519 | Train Acc: 0.7652 | Val Loss: 11.6092 | Val Acc: 0.7658\n",
      "Epoch [8/30] Train Loss: 0.5454 | Train Acc: 0.7675 | Val Loss: 0.5596 | Val Acc: 0.7658\n",
      "Epoch [9/30] Train Loss: 0.5498 | Train Acc: 0.7664 | Val Loss: 16.8648 | Val Acc: 0.7658\n",
      "Epoch [10/30] Train Loss: 0.5423 | Train Acc: 0.7683 | Val Loss: 47.7040 | Val Acc: 0.7658\n",
      "Epoch [11/30] Train Loss: 0.5492 | Train Acc: 0.7656 | Val Loss: 25.8315 | Val Acc: 0.7658\n",
      "Epoch [12/30] Train Loss: 0.5435 | Train Acc: 0.7671 | Val Loss: 27.2300 | Val Acc: 0.7658\n",
      "Epoch [13/30] Train Loss: 0.5491 | Train Acc: 0.7660 | Val Loss: 0.6853 | Val Acc: 0.7658\n",
      "Epoch [14/30] Train Loss: 0.5465 | Train Acc: 0.7660 | Val Loss: 0.7845 | Val Acc: 0.7658\n",
      "Epoch [15/30] Train Loss: 0.5446 | Train Acc: 0.7664 | Val Loss: 1.6992 | Val Acc: 0.7658\n",
      "Epoch [16/30] Train Loss: 0.5452 | Train Acc: 0.7660 | Val Loss: 18.4091 | Val Acc: 0.7658\n",
      "Epoch [17/30] Train Loss: 0.5441 | Train Acc: 0.7664 | Val Loss: 7.3595 | Val Acc: 0.7658\n",
      "Epoch [18/30] Train Loss: 0.5520 | Train Acc: 0.7649 | Val Loss: 2.3840 | Val Acc: 0.7658\n",
      "Epoch [19/30] Train Loss: 0.5448 | Train Acc: 0.7683 | Val Loss: 2.0728 | Val Acc: 0.7658\n",
      "Epoch [20/30] Train Loss: 0.5464 | Train Acc: 0.7664 | Val Loss: 4.5875 | Val Acc: 0.7658\n",
      "Epoch [21/30] Train Loss: 0.5448 | Train Acc: 0.7675 | Val Loss: 8.1012 | Val Acc: 0.7658\n",
      "Epoch [22/30] Train Loss: 0.5466 | Train Acc: 0.7664 | Val Loss: 1.4811 | Val Acc: 0.7658\n",
      "Epoch [23/30] Train Loss: 0.5457 | Train Acc: 0.7671 | Val Loss: 0.7856 | Val Acc: 0.7658\n",
      "Epoch [24/30] Train Loss: 0.5460 | Train Acc: 0.7664 | Val Loss: 0.9937 | Val Acc: 0.7658\n",
      "Epoch [25/30] Train Loss: 0.5455 | Train Acc: 0.7660 | Val Loss: 0.5441 | Val Acc: 0.7658\n",
      "Epoch [26/30] Train Loss: 0.5465 | Train Acc: 0.7660 | Val Loss: 0.5840 | Val Acc: 0.7658\n",
      "Epoch [27/30] Train Loss: 0.5454 | Train Acc: 0.7656 | Val Loss: 3.7699 | Val Acc: 0.7658\n",
      "Epoch [28/30] Train Loss: 0.5443 | Train Acc: 0.7664 | Val Loss: 7.8727 | Val Acc: 0.7658\n",
      "Epoch [29/30] Train Loss: 0.5409 | Train Acc: 0.7687 | Val Loss: 8.8586 | Val Acc: 0.7658\n",
      "Epoch [30/30] Train Loss: 0.5437 | Train Acc: 0.7671 | Val Loss: 0.9266 | Val Acc: 0.7658\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running: semi_synthetic | agnostic | simple =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train Loss: 0.5439 | Train Accuracy: 0.7661\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7661411411411412, 'recall': 1.0, 'f1-score': 0.8675876726886291, 'support': 2041.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 623.0}, 'accuracy': 0.7661411411411412, 'macro avg': {'precision': 0.3830705705705706, 'recall': 0.5, 'f1-score': 0.43379383634431457, 'support': 2664.0}, 'weighted avg': {'precision': 0.58697224814905, 'recall': 0.7661411411411412, 'f1-score': 0.6646946095936532, 'support': 2664.0}}\n",
      "Test Loss: 0.5426 | Test Accuracy: 0.7673\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7672672672672672, 'recall': 1.0, 'f1-score': 0.8683092608326253, 'support': 511.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 155.0}, 'accuracy': 0.7672672672672672, 'macro avg': {'precision': 0.3836336336336336, 'recall': 0.5, 'f1-score': 0.43415463041631264, 'support': 666.0}, 'weighted avg': {'precision': 0.5886990594197802, 'recall': 0.7672672672672672, 'f1-score': 0.6662252737019092, 'support': 666.0}}\n",
      "\n",
      "--- Fold 2 ---\n",
      "Train Loss: 0.5439 | Train Accuracy: 0.7661\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7661411411411412, 'recall': 1.0, 'f1-score': 0.8675876726886291, 'support': 2041.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 623.0}, 'accuracy': 0.7661411411411412, 'macro avg': {'precision': 0.3830705705705706, 'recall': 0.5, 'f1-score': 0.43379383634431457, 'support': 2664.0}, 'weighted avg': {'precision': 0.58697224814905, 'recall': 0.7661411411411412, 'f1-score': 0.6646946095936532, 'support': 2664.0}}\n",
      "Test Loss: 0.5426 | Test Accuracy: 0.7673\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7672672672672672, 'recall': 1.0, 'f1-score': 0.8683092608326253, 'support': 511.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 155.0}, 'accuracy': 0.7672672672672672, 'macro avg': {'precision': 0.3836336336336336, 'recall': 0.5, 'f1-score': 0.43415463041631264, 'support': 666.0}, 'weighted avg': {'precision': 0.5886990594197802, 'recall': 0.7672672672672672, 'f1-score': 0.6662252737019092, 'support': 666.0}}\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train Loss: 0.5435 | Train Accuracy: 0.7665\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7665165165165165, 'recall': 1.0, 'f1-score': 0.8678283042923927, 'support': 2042.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 622.0}, 'accuracy': 0.7665165165165165, 'macro avg': {'precision': 0.38325825825825827, 'recall': 0.5, 'f1-score': 0.43391415214619633, 'support': 2664.0}, 'weighted avg': {'precision': 0.5875475700926152, 'recall': 0.7665165165165165, 'f1-score': 0.6652047287406403, 'support': 2664.0}}\n",
      "Test Loss: 0.5443 | Test Accuracy: 0.7658\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7657657657657657, 'recall': 1.0, 'f1-score': 0.8673469387755102, 'support': 510.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 156.0}, 'accuracy': 0.7657657657657657, 'macro avg': {'precision': 0.38288288288288286, 'recall': 0.5, 'f1-score': 0.4336734693877551, 'support': 666.0}, 'weighted avg': {'precision': 0.5863972080188297, 'recall': 0.7657657657657657, 'f1-score': 0.6641845927560213, 'support': 666.0}}\n",
      "\n",
      "--- Fold 4 ---\n",
      "Train Loss: 0.5435 | Train Accuracy: 0.7665\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7665165165165165, 'recall': 1.0, 'f1-score': 0.8678283042923927, 'support': 2042.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 622.0}, 'accuracy': 0.7665165165165165, 'macro avg': {'precision': 0.38325825825825827, 'recall': 0.5, 'f1-score': 0.43391415214619633, 'support': 2664.0}, 'weighted avg': {'precision': 0.5875475700926152, 'recall': 0.7665165165165165, 'f1-score': 0.6652047287406403, 'support': 2664.0}}\n",
      "Test Loss: 0.5443 | Test Accuracy: 0.7658\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7657657657657657, 'recall': 1.0, 'f1-score': 0.8673469387755102, 'support': 510.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 156.0}, 'accuracy': 0.7657657657657657, 'macro avg': {'precision': 0.38288288288288286, 'recall': 0.5, 'f1-score': 0.4336734693877551, 'support': 666.0}, 'weighted avg': {'precision': 0.5863972080188297, 'recall': 0.7657657657657657, 'f1-score': 0.6641845927560213, 'support': 666.0}}\n",
      "\n",
      "--- Fold 5 ---\n",
      "Train Loss: 0.5435 | Train Accuracy: 0.7665\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7665165165165165, 'recall': 1.0, 'f1-score': 0.8678283042923927, 'support': 2042.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 622.0}, 'accuracy': 0.7665165165165165, 'macro avg': {'precision': 0.38325825825825827, 'recall': 0.5, 'f1-score': 0.43391415214619633, 'support': 2664.0}, 'weighted avg': {'precision': 0.5875475700926152, 'recall': 0.7665165165165165, 'f1-score': 0.6652047287406403, 'support': 2664.0}}\n",
      "Test Loss: 0.5443 | Test Accuracy: 0.7658\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7657657657657657, 'recall': 1.0, 'f1-score': 0.8673469387755102, 'support': 510.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 156.0}, 'accuracy': 0.7657657657657657, 'macro avg': {'precision': 0.38288288288288286, 'recall': 0.5, 'f1-score': 0.4336734693877551, 'support': 666.0}, 'weighted avg': {'precision': 0.5863972080188297, 'recall': 0.7657657657657657, 'f1-score': 0.6641845927560213, 'support': 666.0}}\n",
      "\n",
      "===== Running: semi_synthetic | population | nn =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch [1/30] Train Loss: 0.4292 | Train Acc: 0.7969 | Val Loss: 0.2805 | Val Acc: 0.8649\n",
      "Epoch [2/30] Train Loss: 0.2633 | Train Acc: 0.8841 | Val Loss: 0.2147 | Val Acc: 0.9039\n",
      "Epoch [3/30] Train Loss: 0.1852 | Train Acc: 0.9230 | Val Loss: 0.1523 | Val Acc: 0.9414\n",
      "Epoch [4/30] Train Loss: 0.1479 | Train Acc: 0.9402 | Val Loss: 0.1198 | Val Acc: 0.9489\n",
      "Epoch [5/30] Train Loss: 0.1035 | Train Acc: 0.9577 | Val Loss: 0.0928 | Val Acc: 0.9550\n",
      "Epoch [6/30] Train Loss: 0.0818 | Train Acc: 0.9676 | Val Loss: 0.0739 | Val Acc: 0.9655\n",
      "Epoch [7/30] Train Loss: 0.0655 | Train Acc: 0.9790 | Val Loss: 0.0607 | Val Acc: 0.9730\n",
      "Epoch [8/30] Train Loss: 0.0496 | Train Acc: 0.9859 | Val Loss: 0.0519 | Val Acc: 0.9865\n",
      "Epoch [9/30] Train Loss: 0.0408 | Train Acc: 0.9889 | Val Loss: 0.0639 | Val Acc: 0.9775\n",
      "Epoch [10/30] Train Loss: 0.0284 | Train Acc: 0.9920 | Val Loss: 0.0560 | Val Acc: 0.9850\n",
      "Epoch [11/30] Train Loss: 0.0276 | Train Acc: 0.9916 | Val Loss: 0.0444 | Val Acc: 0.9880\n",
      "Epoch [12/30] Train Loss: 0.0201 | Train Acc: 0.9950 | Val Loss: 0.0495 | Val Acc: 0.9865\n",
      "Epoch [13/30] Train Loss: 0.0172 | Train Acc: 0.9954 | Val Loss: 0.0495 | Val Acc: 0.9895\n",
      "Epoch [14/30] Train Loss: 0.0182 | Train Acc: 0.9950 | Val Loss: 0.0449 | Val Acc: 0.9880\n",
      "Epoch [15/30] Train Loss: 0.0125 | Train Acc: 0.9985 | Val Loss: 0.0462 | Val Acc: 0.9865\n",
      "Epoch [16/30] Train Loss: 0.0113 | Train Acc: 0.9977 | Val Loss: 0.0542 | Val Acc: 0.9880\n",
      "Epoch [17/30] Train Loss: 0.0106 | Train Acc: 0.9981 | Val Loss: 0.0458 | Val Acc: 0.9910\n",
      "Epoch [18/30] Train Loss: 0.0086 | Train Acc: 0.9973 | Val Loss: 0.0491 | Val Acc: 0.9910\n",
      "Epoch [19/30] Train Loss: 0.0093 | Train Acc: 0.9981 | Val Loss: 0.0477 | Val Acc: 0.9910\n",
      "Epoch [20/30] Train Loss: 0.0127 | Train Acc: 0.9962 | Val Loss: 0.0514 | Val Acc: 0.9910\n",
      "Epoch [21/30] Train Loss: 0.0086 | Train Acc: 0.9985 | Val Loss: 0.0476 | Val Acc: 0.9895\n",
      "Epoch [22/30] Train Loss: 0.0056 | Train Acc: 0.9996 | Val Loss: 0.0478 | Val Acc: 0.9895\n",
      "Epoch [23/30] Train Loss: 0.0056 | Train Acc: 0.9996 | Val Loss: 0.0429 | Val Acc: 0.9910\n",
      "Epoch [24/30] Train Loss: 0.0066 | Train Acc: 0.9989 | Val Loss: 0.0424 | Val Acc: 0.9925\n",
      "Epoch [25/30] Train Loss: 0.0057 | Train Acc: 0.9989 | Val Loss: 0.0438 | Val Acc: 0.9910\n",
      "Epoch [26/30] Train Loss: 0.0055 | Train Acc: 0.9989 | Val Loss: 0.0430 | Val Acc: 0.9925\n",
      "Epoch [27/30] Train Loss: 0.0035 | Train Acc: 1.0000 | Val Loss: 0.0454 | Val Acc: 0.9895\n",
      "Epoch [28/30] Train Loss: 0.0060 | Train Acc: 0.9989 | Val Loss: 0.0448 | Val Acc: 0.9895\n",
      "Epoch [29/30] Train Loss: 0.0041 | Train Acc: 0.9992 | Val Loss: 0.0448 | Val Acc: 0.9925\n",
      "Epoch [30/30] Train Loss: 0.0036 | Train Acc: 0.9992 | Val Loss: 0.0442 | Val Acc: 0.9910\n",
      "Training complete!\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch [1/30] Train Loss: 0.4056 | Train Acc: 0.8232 | Val Loss: 0.2683 | Val Acc: 0.8904\n",
      "Epoch [2/30] Train Loss: 0.2616 | Train Acc: 0.8841 | Val Loss: 0.1848 | Val Acc: 0.9129\n",
      "Epoch [3/30] Train Loss: 0.1916 | Train Acc: 0.9230 | Val Loss: 0.1220 | Val Acc: 0.9399\n",
      "Epoch [4/30] Train Loss: 0.1463 | Train Acc: 0.9474 | Val Loss: 0.0762 | Val Acc: 0.9700\n",
      "Epoch [5/30] Train Loss: 0.0998 | Train Acc: 0.9668 | Val Loss: 0.0572 | Val Acc: 0.9775\n",
      "Epoch [6/30] Train Loss: 0.0775 | Train Acc: 0.9741 | Val Loss: 0.0407 | Val Acc: 0.9910\n",
      "Epoch [7/30] Train Loss: 0.0631 | Train Acc: 0.9783 | Val Loss: 0.0367 | Val Acc: 0.9835\n",
      "Epoch [8/30] Train Loss: 0.0525 | Train Acc: 0.9832 | Val Loss: 0.0290 | Val Acc: 0.9835\n",
      "Epoch [9/30] Train Loss: 0.0446 | Train Acc: 0.9855 | Val Loss: 0.0306 | Val Acc: 0.9865\n",
      "Epoch [10/30] Train Loss: 0.0298 | Train Acc: 0.9920 | Val Loss: 0.0242 | Val Acc: 0.9940\n",
      "Epoch [11/30] Train Loss: 0.0219 | Train Acc: 0.9950 | Val Loss: 0.0137 | Val Acc: 0.9955\n",
      "Epoch [12/30] Train Loss: 0.0116 | Train Acc: 1.0000 | Val Loss: 0.0118 | Val Acc: 0.9970\n",
      "Epoch [13/30] Train Loss: 0.0115 | Train Acc: 0.9977 | Val Loss: 0.0096 | Val Acc: 0.9970\n",
      "Epoch [14/30] Train Loss: 0.0119 | Train Acc: 0.9962 | Val Loss: 0.0102 | Val Acc: 0.9970\n",
      "Epoch [15/30] Train Loss: 0.0115 | Train Acc: 0.9966 | Val Loss: 0.0107 | Val Acc: 0.9970\n",
      "Epoch [16/30] Train Loss: 0.0095 | Train Acc: 0.9989 | Val Loss: 0.0086 | Val Acc: 0.9955\n",
      "Epoch [17/30] Train Loss: 0.0088 | Train Acc: 0.9977 | Val Loss: 0.0101 | Val Acc: 0.9970\n",
      "Epoch [18/30] Train Loss: 0.0109 | Train Acc: 0.9970 | Val Loss: 0.0106 | Val Acc: 0.9970\n",
      "Epoch [19/30] Train Loss: 0.0137 | Train Acc: 0.9962 | Val Loss: 0.0051 | Val Acc: 0.9970\n",
      "Epoch [20/30] Train Loss: 0.0064 | Train Acc: 0.9996 | Val Loss: 0.0071 | Val Acc: 0.9970\n",
      "Epoch [21/30] Train Loss: 0.0046 | Train Acc: 1.0000 | Val Loss: 0.0067 | Val Acc: 0.9970\n",
      "Epoch [22/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0058 | Val Acc: 0.9970\n",
      "Epoch [23/30] Train Loss: 0.0054 | Train Acc: 0.9989 | Val Loss: 0.0071 | Val Acc: 0.9970\n",
      "Epoch [24/30] Train Loss: 0.0051 | Train Acc: 0.9996 | Val Loss: 0.0056 | Val Acc: 0.9970\n",
      "Epoch [25/30] Train Loss: 0.0035 | Train Acc: 0.9996 | Val Loss: 0.0069 | Val Acc: 0.9970\n",
      "Epoch [26/30] Train Loss: 0.0046 | Train Acc: 0.9992 | Val Loss: 0.0073 | Val Acc: 0.9970\n",
      "Epoch [27/30] Train Loss: 0.0072 | Train Acc: 0.9977 | Val Loss: 0.0041 | Val Acc: 0.9970\n",
      "Epoch [28/30] Train Loss: 0.0034 | Train Acc: 1.0000 | Val Loss: 0.0069 | Val Acc: 0.9970\n",
      "Epoch [29/30] Train Loss: 0.0032 | Train Acc: 0.9992 | Val Loss: 0.0077 | Val Acc: 0.9970\n",
      "Epoch [30/30] Train Loss: 0.0033 | Train Acc: 0.9992 | Val Loss: 0.0087 | Val Acc: 0.9970\n",
      "Training complete!\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch [1/30] Train Loss: 0.4170 | Train Acc: 0.8125 | Val Loss: 0.3074 | Val Acc: 0.8769\n",
      "Epoch [2/30] Train Loss: 0.2732 | Train Acc: 0.8815 | Val Loss: 0.2265 | Val Acc: 0.9069\n",
      "Epoch [3/30] Train Loss: 0.1994 | Train Acc: 0.9223 | Val Loss: 0.1683 | Val Acc: 0.9384\n",
      "Epoch [4/30] Train Loss: 0.1398 | Train Acc: 0.9459 | Val Loss: 0.1340 | Val Acc: 0.9520\n",
      "Epoch [5/30] Train Loss: 0.1112 | Train Acc: 0.9611 | Val Loss: 0.1054 | Val Acc: 0.9595\n",
      "Epoch [6/30] Train Loss: 0.0835 | Train Acc: 0.9737 | Val Loss: 0.0957 | Val Acc: 0.9595\n",
      "Epoch [7/30] Train Loss: 0.0625 | Train Acc: 0.9745 | Val Loss: 0.0765 | Val Acc: 0.9715\n",
      "Epoch [8/30] Train Loss: 0.0518 | Train Acc: 0.9848 | Val Loss: 0.0778 | Val Acc: 0.9775\n",
      "Epoch [9/30] Train Loss: 0.0438 | Train Acc: 0.9855 | Val Loss: 0.0595 | Val Acc: 0.9775\n",
      "Epoch [10/30] Train Loss: 0.0297 | Train Acc: 0.9939 | Val Loss: 0.0646 | Val Acc: 0.9835\n",
      "Epoch [11/30] Train Loss: 0.0292 | Train Acc: 0.9920 | Val Loss: 0.0548 | Val Acc: 0.9835\n",
      "Epoch [12/30] Train Loss: 0.0230 | Train Acc: 0.9928 | Val Loss: 0.0477 | Val Acc: 0.9895\n",
      "Epoch [13/30] Train Loss: 0.0241 | Train Acc: 0.9939 | Val Loss: 0.0440 | Val Acc: 0.9880\n",
      "Epoch [14/30] Train Loss: 0.0163 | Train Acc: 0.9962 | Val Loss: 0.0478 | Val Acc: 0.9880\n",
      "Epoch [15/30] Train Loss: 0.0165 | Train Acc: 0.9966 | Val Loss: 0.0508 | Val Acc: 0.9880\n",
      "Epoch [16/30] Train Loss: 0.0144 | Train Acc: 0.9966 | Val Loss: 0.0541 | Val Acc: 0.9865\n",
      "Epoch [17/30] Train Loss: 0.0136 | Train Acc: 0.9973 | Val Loss: 0.0436 | Val Acc: 0.9865\n",
      "Epoch [18/30] Train Loss: 0.0157 | Train Acc: 0.9950 | Val Loss: 0.0656 | Val Acc: 0.9865\n",
      "Epoch [19/30] Train Loss: 0.0141 | Train Acc: 0.9970 | Val Loss: 0.0654 | Val Acc: 0.9880\n",
      "Epoch [20/30] Train Loss: 0.0135 | Train Acc: 0.9966 | Val Loss: 0.0471 | Val Acc: 0.9895\n",
      "Epoch [21/30] Train Loss: 0.0110 | Train Acc: 0.9966 | Val Loss: 0.0483 | Val Acc: 0.9895\n",
      "Epoch [22/30] Train Loss: 0.0111 | Train Acc: 0.9954 | Val Loss: 0.0520 | Val Acc: 0.9880\n",
      "Epoch [23/30] Train Loss: 0.0091 | Train Acc: 0.9981 | Val Loss: 0.0511 | Val Acc: 0.9880\n",
      "Epoch [24/30] Train Loss: 0.0059 | Train Acc: 0.9992 | Val Loss: 0.0521 | Val Acc: 0.9895\n",
      "Epoch [25/30] Train Loss: 0.0065 | Train Acc: 0.9985 | Val Loss: 0.0498 | Val Acc: 0.9895\n",
      "Epoch [26/30] Train Loss: 0.0067 | Train Acc: 0.9989 | Val Loss: 0.0517 | Val Acc: 0.9880\n",
      "Epoch [27/30] Train Loss: 0.0042 | Train Acc: 0.9996 | Val Loss: 0.0512 | Val Acc: 0.9880\n",
      "Epoch [28/30] Train Loss: 0.0053 | Train Acc: 0.9992 | Val Loss: 0.0480 | Val Acc: 0.9880\n",
      "Epoch [29/30] Train Loss: 0.0043 | Train Acc: 0.9992 | Val Loss: 0.0576 | Val Acc: 0.9880\n",
      "Epoch [30/30] Train Loss: 0.0060 | Train Acc: 0.9985 | Val Loss: 0.0572 | Val Acc: 0.9880\n",
      "Training complete!\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch [1/30] Train Loss: 0.3748 | Train Acc: 0.8369 | Val Loss: 0.2639 | Val Acc: 0.8949\n",
      "Epoch [2/30] Train Loss: 0.2309 | Train Acc: 0.8990 | Val Loss: 0.1927 | Val Acc: 0.9204\n",
      "Epoch [3/30] Train Loss: 0.1657 | Train Acc: 0.9375 | Val Loss: 0.1376 | Val Acc: 0.9429\n",
      "Epoch [4/30] Train Loss: 0.1121 | Train Acc: 0.9573 | Val Loss: 0.1128 | Val Acc: 0.9595\n",
      "Epoch [5/30] Train Loss: 0.0817 | Train Acc: 0.9729 | Val Loss: 0.0861 | Val Acc: 0.9655\n",
      "Epoch [6/30] Train Loss: 0.0680 | Train Acc: 0.9779 | Val Loss: 0.0701 | Val Acc: 0.9670\n",
      "Epoch [7/30] Train Loss: 0.0493 | Train Acc: 0.9844 | Val Loss: 0.0600 | Val Acc: 0.9790\n",
      "Epoch [8/30] Train Loss: 0.0352 | Train Acc: 0.9889 | Val Loss: 0.0538 | Val Acc: 0.9835\n",
      "Epoch [9/30] Train Loss: 0.0287 | Train Acc: 0.9905 | Val Loss: 0.0542 | Val Acc: 0.9850\n",
      "Epoch [10/30] Train Loss: 0.0239 | Train Acc: 0.9943 | Val Loss: 0.0426 | Val Acc: 0.9835\n",
      "Epoch [11/30] Train Loss: 0.0198 | Train Acc: 0.9958 | Val Loss: 0.0449 | Val Acc: 0.9835\n",
      "Epoch [12/30] Train Loss: 0.0156 | Train Acc: 0.9954 | Val Loss: 0.0438 | Val Acc: 0.9835\n",
      "Epoch [13/30] Train Loss: 0.0152 | Train Acc: 0.9973 | Val Loss: 0.0419 | Val Acc: 0.9865\n",
      "Epoch [14/30] Train Loss: 0.0129 | Train Acc: 0.9970 | Val Loss: 0.0461 | Val Acc: 0.9865\n",
      "Epoch [15/30] Train Loss: 0.0130 | Train Acc: 0.9958 | Val Loss: 0.0432 | Val Acc: 0.9865\n",
      "Epoch [16/30] Train Loss: 0.0088 | Train Acc: 0.9981 | Val Loss: 0.0437 | Val Acc: 0.9835\n",
      "Epoch [17/30] Train Loss: 0.0120 | Train Acc: 0.9966 | Val Loss: 0.0405 | Val Acc: 0.9835\n",
      "Epoch [18/30] Train Loss: 0.0084 | Train Acc: 0.9981 | Val Loss: 0.0370 | Val Acc: 0.9895\n",
      "Epoch [19/30] Train Loss: 0.0082 | Train Acc: 0.9985 | Val Loss: 0.0407 | Val Acc: 0.9835\n",
      "Epoch [20/30] Train Loss: 0.0036 | Train Acc: 0.9996 | Val Loss: 0.0452 | Val Acc: 0.9835\n",
      "Epoch [21/30] Train Loss: 0.0068 | Train Acc: 0.9985 | Val Loss: 0.0395 | Val Acc: 0.9865\n",
      "Epoch [22/30] Train Loss: 0.0074 | Train Acc: 0.9981 | Val Loss: 0.0400 | Val Acc: 0.9865\n",
      "Epoch [23/30] Train Loss: 0.0045 | Train Acc: 0.9992 | Val Loss: 0.0378 | Val Acc: 0.9865\n",
      "Epoch [24/30] Train Loss: 0.0045 | Train Acc: 0.9996 | Val Loss: 0.0418 | Val Acc: 0.9835\n",
      "Epoch [25/30] Train Loss: 0.0046 | Train Acc: 0.9992 | Val Loss: 0.0396 | Val Acc: 0.9865\n",
      "Epoch [26/30] Train Loss: 0.0050 | Train Acc: 0.9985 | Val Loss: 0.0404 | Val Acc: 0.9865\n",
      "Epoch [27/30] Train Loss: 0.0036 | Train Acc: 0.9992 | Val Loss: 0.0464 | Val Acc: 0.9835\n",
      "Epoch [28/30] Train Loss: 0.0037 | Train Acc: 0.9992 | Val Loss: 0.0451 | Val Acc: 0.9880\n",
      "Epoch [29/30] Train Loss: 0.0041 | Train Acc: 0.9992 | Val Loss: 0.0429 | Val Acc: 0.9880\n",
      "Epoch [30/30] Train Loss: 0.0041 | Train Acc: 0.9992 | Val Loss: 0.0452 | Val Acc: 0.9880\n",
      "Training complete!\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch [1/30] Train Loss: 0.4147 | Train Acc: 0.8068 | Val Loss: 0.2821 | Val Acc: 0.8468\n",
      "Epoch [2/30] Train Loss: 0.2514 | Train Acc: 0.8921 | Val Loss: 0.2045 | Val Acc: 0.9159\n",
      "Epoch [3/30] Train Loss: 0.1811 | Train Acc: 0.9303 | Val Loss: 0.1319 | Val Acc: 0.9520\n",
      "Epoch [4/30] Train Loss: 0.1386 | Train Acc: 0.9440 | Val Loss: 0.1039 | Val Acc: 0.9610\n",
      "Epoch [5/30] Train Loss: 0.0999 | Train Acc: 0.9630 | Val Loss: 0.0853 | Val Acc: 0.9580\n",
      "Epoch [6/30] Train Loss: 0.0789 | Train Acc: 0.9737 | Val Loss: 0.0779 | Val Acc: 0.9640\n",
      "Epoch [7/30] Train Loss: 0.0674 | Train Acc: 0.9764 | Val Loss: 0.0667 | Val Acc: 0.9670\n",
      "Epoch [8/30] Train Loss: 0.0476 | Train Acc: 0.9863 | Val Loss: 0.0705 | Val Acc: 0.9730\n",
      "Epoch [9/30] Train Loss: 0.0417 | Train Acc: 0.9874 | Val Loss: 0.0695 | Val Acc: 0.9745\n",
      "Epoch [10/30] Train Loss: 0.0323 | Train Acc: 0.9878 | Val Loss: 0.0625 | Val Acc: 0.9715\n",
      "Epoch [11/30] Train Loss: 0.0257 | Train Acc: 0.9924 | Val Loss: 0.0651 | Val Acc: 0.9730\n",
      "Epoch [12/30] Train Loss: 0.0260 | Train Acc: 0.9920 | Val Loss: 0.0560 | Val Acc: 0.9805\n",
      "Epoch [13/30] Train Loss: 0.0170 | Train Acc: 0.9973 | Val Loss: 0.0499 | Val Acc: 0.9790\n",
      "Epoch [14/30] Train Loss: 0.0168 | Train Acc: 0.9943 | Val Loss: 0.0549 | Val Acc: 0.9805\n",
      "Epoch [15/30] Train Loss: 0.0148 | Train Acc: 0.9973 | Val Loss: 0.0605 | Val Acc: 0.9820\n",
      "Epoch [16/30] Train Loss: 0.0137 | Train Acc: 0.9962 | Val Loss: 0.0573 | Val Acc: 0.9805\n",
      "Epoch [17/30] Train Loss: 0.0097 | Train Acc: 0.9981 | Val Loss: 0.0574 | Val Acc: 0.9805\n",
      "Epoch [18/30] Train Loss: 0.0072 | Train Acc: 0.9989 | Val Loss: 0.0533 | Val Acc: 0.9805\n",
      "Epoch [19/30] Train Loss: 0.0091 | Train Acc: 0.9981 | Val Loss: 0.0523 | Val Acc: 0.9850\n",
      "Epoch [20/30] Train Loss: 0.0086 | Train Acc: 0.9989 | Val Loss: 0.0534 | Val Acc: 0.9805\n",
      "Epoch [21/30] Train Loss: 0.0109 | Train Acc: 0.9970 | Val Loss: 0.0513 | Val Acc: 0.9865\n",
      "Epoch [22/30] Train Loss: 0.0062 | Train Acc: 0.9992 | Val Loss: 0.0531 | Val Acc: 0.9835\n",
      "Epoch [23/30] Train Loss: 0.0047 | Train Acc: 0.9996 | Val Loss: 0.0513 | Val Acc: 0.9820\n",
      "Epoch [24/30] Train Loss: 0.0050 | Train Acc: 0.9985 | Val Loss: 0.0553 | Val Acc: 0.9820\n",
      "Epoch [25/30] Train Loss: 0.0044 | Train Acc: 0.9985 | Val Loss: 0.0563 | Val Acc: 0.9820\n",
      "Epoch [26/30] Train Loss: 0.0061 | Train Acc: 0.9981 | Val Loss: 0.0504 | Val Acc: 0.9820\n",
      "Epoch [27/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0521 | Val Acc: 0.9820\n",
      "Epoch [28/30] Train Loss: 0.0027 | Train Acc: 1.0000 | Val Loss: 0.0534 | Val Acc: 0.9850\n",
      "Epoch [29/30] Train Loss: 0.0035 | Train Acc: 0.9996 | Val Loss: 0.0547 | Val Acc: 0.9850\n",
      "Epoch [30/30] Train Loss: 0.0037 | Train Acc: 0.9996 | Val Loss: 0.0511 | Val Acc: 0.9850\n",
      "Training complete!\n",
      "\n",
      "===== Running: semi_synthetic | population | simple =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train Loss: 0.2934 | Train Accuracy: 0.8600\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8564102564102564, 'recall': 0.9818716315531603, 'f1-score': 0.9148596210910751, 'support': 2041.0}, 'treatment': {'precision': 0.8858024691358025, 'recall': 0.4606741573033708, 'f1-score': 0.6061246040126715, 'support': 623.0}, 'accuracy': 0.859984984984985, 'macro avg': {'precision': 0.8711063627730294, 'recall': 0.7212728944282656, 'f1-score': 0.7604921125518733, 'support': 2664.0}, 'weighted avg': {'precision': 0.8632838857375893, 'recall': 0.859984984984985, 'f1-score': 0.8426592023073493, 'support': 2664.0}}\n",
      "Test Loss: 0.2938 | Test Accuracy: 0.8529\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8470588235294118, 'recall': 0.9863013698630136, 'f1-score': 0.9113924050632911, 'support': 511.0}, 'treatment': {'precision': 0.9014084507042254, 'recall': 0.4129032258064516, 'f1-score': 0.5663716814159292, 'support': 155.0}, 'accuracy': 0.8528528528528528, 'macro avg': {'precision': 0.8742336371168186, 'recall': 0.6996022978347326, 'f1-score': 0.7388820432396102, 'support': 666.0}, 'weighted avg': {'precision': 0.8597077607848113, 'recall': 0.8528528528528528, 'f1-score': 0.8310947891994156, 'support': 666.0}}\n",
      "\n",
      "--- Fold 2 ---\n",
      "Train Loss: 0.2966 | Train Accuracy: 0.8675\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8619210977701544, 'recall': 0.984811366976972, 'f1-score': 0.9192773839469471, 'support': 2041.0}, 'treatment': {'precision': 0.9066265060240963, 'recall': 0.48314606741573035, 'f1-score': 0.6303664921465969, 'support': 623.0}, 'accuracy': 0.8674924924924925, 'macro avg': {'precision': 0.8842738018971253, 'recall': 0.7339787171963512, 'f1-score': 0.774821938046772, 'support': 2664.0}, 'weighted avg': {'precision': 0.8723758535292406, 'recall': 0.8674924924924925, 'f1-score': 0.8517130124786221, 'support': 2664.0}}\n",
      "Test Loss: 0.2819 | Test Accuracy: 0.8814\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8763066202090593, 'recall': 0.9843444227005871, 'f1-score': 0.9271889400921659, 'support': 511.0}, 'treatment': {'precision': 0.9130434782608695, 'recall': 0.5419354838709678, 'f1-score': 0.680161943319838, 'support': 155.0}, 'accuracy': 0.8813813813813813, 'macro avg': {'precision': 0.8946750492349644, 'recall': 0.7631399532857774, 'f1-score': 0.8036754417060019, 'support': 666.0}, 'weighted avg': {'precision': 0.8848564895754716, 'recall': 0.8813813813813813, 'f1-score': 0.8696976720745822, 'support': 666.0}}\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train Loss: 0.2881 | Train Accuracy: 0.8664\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8602564102564103, 'recall': 0.9857982370225269, 'f1-score': 0.9187585577361935, 'support': 2042.0}, 'treatment': {'precision': 0.9104938271604939, 'recall': 0.4742765273311897, 'f1-score': 0.6236786469344608, 'support': 622.0}, 'accuracy': 0.8663663663663663, 'macro avg': {'precision': 0.8853751187084521, 'recall': 0.7300373821768583, 'f1-score': 0.7712186023353271, 'support': 2664.0}, 'weighted avg': {'precision': 0.8719860173563878, 'recall': 0.8663663663663663, 'f1-score': 0.8498622722562094, 'support': 2664.0}}\n",
      "Test Loss: 0.3152 | Test Accuracy: 0.8453\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8431703204047217, 'recall': 0.9803921568627451, 'f1-score': 0.9066183136899365, 'support': 510.0}, 'treatment': {'precision': 0.863013698630137, 'recall': 0.40384615384615385, 'f1-score': 0.5502183406113537, 'support': 156.0}, 'accuracy': 0.8453453453453453, 'macro avg': {'precision': 0.8530920095174294, 'recall': 0.6921191553544495, 'f1-score': 0.728418327150645, 'support': 666.0}, 'weighted avg': {'precision': 0.8478183189079722, 'recall': 0.8453453453453453, 'f1-score': 0.823137238914773, 'support': 666.0}}\n",
      "\n",
      "--- Fold 4 ---\n",
      "Train Loss: 0.2931 | Train Accuracy: 0.8664\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8602564102564103, 'recall': 0.9857982370225269, 'f1-score': 0.9187585577361935, 'support': 2042.0}, 'treatment': {'precision': 0.9104938271604939, 'recall': 0.4742765273311897, 'f1-score': 0.6236786469344608, 'support': 622.0}, 'accuracy': 0.8663663663663663, 'macro avg': {'precision': 0.8853751187084521, 'recall': 0.7300373821768583, 'f1-score': 0.7712186023353271, 'support': 2664.0}, 'weighted avg': {'precision': 0.8719860173563878, 'recall': 0.8663663663663663, 'f1-score': 0.8498622722562094, 'support': 2664.0}}\n",
      "Test Loss: 0.2942 | Test Accuracy: 0.8634\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8605851979345955, 'recall': 0.9803921568627451, 'f1-score': 0.916590284142988, 'support': 510.0}, 'treatment': {'precision': 0.8823529411764706, 'recall': 0.4807692307692308, 'f1-score': 0.6224066390041494, 'support': 156.0}, 'accuracy': 0.8633633633633634, 'macro avg': {'precision': 0.871469069555533, 'recall': 0.7305806938159879, 'f1-score': 0.7694984615735687, 'support': 666.0}, 'weighted avg': {'precision': 0.8656839486038636, 'recall': 0.8633633633633634, 'f1-score': 0.8476824032996564, 'support': 666.0}}\n",
      "\n",
      "--- Fold 5 ---\n",
      "Train Loss: 0.2923 | Train Accuracy: 0.8634\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8582408198121264, 'recall': 0.9843290891283056, 'f1-score': 0.916970802919708, 'support': 2042.0}, 'treatment': {'precision': 0.9006211180124224, 'recall': 0.4662379421221865, 'f1-score': 0.614406779661017, 'support': 622.0}, 'accuracy': 0.8633633633633634, 'macro avg': {'precision': 0.8794309689122743, 'recall': 0.7252835156252461, 'f1-score': 0.7656887912903625, 'support': 2664.0}, 'weighted avg': {'precision': 0.8681359194670003, 'recall': 0.8633633633633634, 'f1-score': 0.8463271007924911, 'support': 2664.0}}\n",
      "Test Loss: 0.3029 | Test Accuracy: 0.8709\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8642611683848798, 'recall': 0.9862745098039216, 'f1-score': 0.9212454212454212, 'support': 510.0}, 'treatment': {'precision': 0.9166666666666666, 'recall': 0.4935897435897436, 'f1-score': 0.6416666666666667, 'support': 156.0}, 'accuracy': 0.8708708708708709, 'macro avg': {'precision': 0.8904639175257731, 'recall': 0.7399321266968326, 'f1-score': 0.781456043956044, 'support': 666.0}, 'weighted avg': {'precision': 0.8765363301445777, 'recall': 0.8708708708708709, 'f1-score': 0.8557585057585056, 'support': 666.0}}\n",
      "\n",
      "===== Running: semi_synthetic | individual | nn =====\n",
      "\n",
      "--- Fold 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Local\\Temp\\ipykernel_32640\\1094098609.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = 0\n",
      "C:\\Users\\91939\\AppData\\Local\\Temp\\ipykernel_32640\\1094098609.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Train Loss: 0.3463 | Train Acc: 0.8521 | Val Loss: 0.3114 | Val Acc: 0.8604\n",
      "Epoch [2/30] Train Loss: 0.2500 | Train Acc: 0.8929 | Val Loss: 0.3095 | Val Acc: 0.8649\n",
      "Epoch [3/30] Train Loss: 0.1715 | Train Acc: 0.9405 | Val Loss: 1.3305 | Val Acc: 0.3003\n",
      "Epoch [4/30] Train Loss: 0.1236 | Train Acc: 0.9604 | Val Loss: 0.1290 | Val Acc: 0.9489\n",
      "Epoch [5/30] Train Loss: 0.0926 | Train Acc: 0.9668 | Val Loss: 0.0989 | Val Acc: 0.9700\n",
      "Epoch [6/30] Train Loss: 0.0741 | Train Acc: 0.9760 | Val Loss: 0.0909 | Val Acc: 0.9760\n",
      "Epoch [7/30] Train Loss: 0.0472 | Train Acc: 0.9859 | Val Loss: 0.0867 | Val Acc: 0.9745\n",
      "Epoch [8/30] Train Loss: 0.0437 | Train Acc: 0.9851 | Val Loss: 0.0854 | Val Acc: 0.9835\n",
      "Epoch [9/30] Train Loss: 0.0320 | Train Acc: 0.9931 | Val Loss: 0.0706 | Val Acc: 0.9850\n",
      "Epoch [10/30] Train Loss: 0.0418 | Train Acc: 0.9855 | Val Loss: 0.1119 | Val Acc: 0.9655\n",
      "Epoch [11/30] Train Loss: 0.0282 | Train Acc: 0.9916 | Val Loss: 0.0657 | Val Acc: 0.9850\n",
      "Epoch [12/30] Train Loss: 0.0211 | Train Acc: 0.9939 | Val Loss: 0.0569 | Val Acc: 0.9880\n",
      "Epoch [13/30] Train Loss: 0.0159 | Train Acc: 0.9970 | Val Loss: 0.0532 | Val Acc: 0.9880\n",
      "Epoch [14/30] Train Loss: 0.0164 | Train Acc: 0.9958 | Val Loss: 0.0541 | Val Acc: 0.9880\n",
      "Epoch [15/30] Train Loss: 0.0124 | Train Acc: 0.9973 | Val Loss: 0.0515 | Val Acc: 0.9910\n",
      "Epoch [16/30] Train Loss: 0.0070 | Train Acc: 0.9992 | Val Loss: 0.0540 | Val Acc: 0.9910\n",
      "Epoch [17/30] Train Loss: 0.0121 | Train Acc: 0.9954 | Val Loss: 0.0526 | Val Acc: 0.9925\n",
      "Epoch [18/30] Train Loss: 0.0117 | Train Acc: 0.9981 | Val Loss: 0.0497 | Val Acc: 0.9925\n",
      "Epoch [19/30] Train Loss: 0.0092 | Train Acc: 0.9981 | Val Loss: 0.0507 | Val Acc: 0.9925\n",
      "Epoch [20/30] Train Loss: 0.0077 | Train Acc: 0.9985 | Val Loss: 0.0564 | Val Acc: 0.9910\n",
      "Epoch [21/30] Train Loss: 0.0069 | Train Acc: 0.9985 | Val Loss: 0.0519 | Val Acc: 0.9925\n",
      "Epoch [22/30] Train Loss: 0.0080 | Train Acc: 0.9981 | Val Loss: 0.0549 | Val Acc: 0.9925\n",
      "Epoch [23/30] Train Loss: 0.0092 | Train Acc: 0.9970 | Val Loss: 0.0552 | Val Acc: 0.9925\n",
      "Epoch [24/30] Train Loss: 0.0054 | Train Acc: 0.9985 | Val Loss: 0.0531 | Val Acc: 0.9925\n",
      "Epoch [25/30] Train Loss: 0.0044 | Train Acc: 0.9996 | Val Loss: 0.0585 | Val Acc: 0.9925\n",
      "Epoch [26/30] Train Loss: 0.0046 | Train Acc: 0.9992 | Val Loss: 0.0556 | Val Acc: 0.9925\n",
      "Epoch [27/30] Train Loss: 0.0046 | Train Acc: 0.9996 | Val Loss: 0.0550 | Val Acc: 0.9925\n",
      "Epoch [28/30] Train Loss: 0.0054 | Train Acc: 0.9992 | Val Loss: 0.0586 | Val Acc: 0.9925\n",
      "Epoch [29/30] Train Loss: 0.0045 | Train Acc: 0.9992 | Val Loss: 0.0577 | Val Acc: 0.9925\n",
      "Epoch [30/30] Train Loss: 0.0051 | Train Acc: 0.9989 | Val Loss: 0.0544 | Val Acc: 0.9940\n",
      "Training complete!\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch [1/30] Train Loss: 0.3915 | Train Acc: 0.8350 | Val Loss: 0.3717 | Val Acc: 0.8934\n",
      "Epoch [2/30] Train Loss: 0.2571 | Train Acc: 0.8918 | Val Loss: 0.6998 | Val Acc: 0.6276\n",
      "Epoch [3/30] Train Loss: 0.1927 | Train Acc: 0.9276 | Val Loss: 0.2156 | Val Acc: 0.9414\n",
      "Epoch [4/30] Train Loss: 0.1311 | Train Acc: 0.9596 | Val Loss: 0.1153 | Val Acc: 0.9700\n",
      "Epoch [5/30] Train Loss: 0.0961 | Train Acc: 0.9665 | Val Loss: 0.0679 | Val Acc: 0.9865\n",
      "Epoch [6/30] Train Loss: 0.0680 | Train Acc: 0.9790 | Val Loss: 0.0514 | Val Acc: 0.9805\n",
      "Epoch [7/30] Train Loss: 0.0568 | Train Acc: 0.9817 | Val Loss: 0.0570 | Val Acc: 0.9820\n",
      "Epoch [8/30] Train Loss: 0.0466 | Train Acc: 0.9855 | Val Loss: 0.0363 | Val Acc: 0.9835\n",
      "Epoch [9/30] Train Loss: 0.0419 | Train Acc: 0.9874 | Val Loss: 0.0853 | Val Acc: 0.9790\n",
      "Epoch [10/30] Train Loss: 0.0317 | Train Acc: 0.9924 | Val Loss: 0.0195 | Val Acc: 0.9970\n",
      "Epoch [11/30] Train Loss: 0.0259 | Train Acc: 0.9928 | Val Loss: 0.0087 | Val Acc: 1.0000\n",
      "Epoch [12/30] Train Loss: 0.0183 | Train Acc: 0.9958 | Val Loss: 0.0051 | Val Acc: 1.0000\n",
      "Epoch [13/30] Train Loss: 0.0138 | Train Acc: 0.9970 | Val Loss: 0.0051 | Val Acc: 1.0000\n",
      "Epoch [14/30] Train Loss: 0.0160 | Train Acc: 0.9966 | Val Loss: 0.0046 | Val Acc: 1.0000\n",
      "Epoch [15/30] Train Loss: 0.0111 | Train Acc: 0.9981 | Val Loss: 0.0040 | Val Acc: 1.0000\n",
      "Epoch [16/30] Train Loss: 0.0108 | Train Acc: 0.9981 | Val Loss: 0.0032 | Val Acc: 1.0000\n",
      "Epoch [17/30] Train Loss: 0.0097 | Train Acc: 0.9992 | Val Loss: 0.0022 | Val Acc: 1.0000\n",
      "Epoch [18/30] Train Loss: 0.0100 | Train Acc: 0.9962 | Val Loss: 0.0029 | Val Acc: 1.0000\n",
      "Epoch [19/30] Train Loss: 0.0118 | Train Acc: 0.9970 | Val Loss: 0.0075 | Val Acc: 1.0000\n",
      "Epoch [20/30] Train Loss: 0.0079 | Train Acc: 0.9989 | Val Loss: 0.0013 | Val Acc: 1.0000\n",
      "Epoch [21/30] Train Loss: 0.0054 | Train Acc: 0.9996 | Val Loss: 0.0010 | Val Acc: 1.0000\n",
      "Epoch [22/30] Train Loss: 0.0054 | Train Acc: 0.9985 | Val Loss: 0.0010 | Val Acc: 1.0000\n",
      "Epoch [23/30] Train Loss: 0.0078 | Train Acc: 0.9981 | Val Loss: 0.0014 | Val Acc: 1.0000\n",
      "Epoch [24/30] Train Loss: 0.0063 | Train Acc: 0.9985 | Val Loss: 0.0008 | Val Acc: 1.0000\n",
      "Epoch [25/30] Train Loss: 0.0053 | Train Acc: 0.9996 | Val Loss: 0.0009 | Val Acc: 1.0000\n",
      "Epoch [26/30] Train Loss: 0.0064 | Train Acc: 0.9989 | Val Loss: 0.0008 | Val Acc: 1.0000\n",
      "Epoch [27/30] Train Loss: 0.0056 | Train Acc: 0.9985 | Val Loss: 0.0007 | Val Acc: 1.0000\n",
      "Epoch [28/30] Train Loss: 0.0037 | Train Acc: 1.0000 | Val Loss: 0.0006 | Val Acc: 1.0000\n",
      "Epoch [29/30] Train Loss: 0.0050 | Train Acc: 0.9989 | Val Loss: 0.0007 | Val Acc: 1.0000\n",
      "Epoch [30/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0006 | Val Acc: 1.0000\n",
      "Training complete!\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch [1/30] Train Loss: 0.3549 | Train Acc: 0.8510 | Val Loss: 0.2952 | Val Acc: 0.8408\n",
      "Epoch [2/30] Train Loss: 0.2349 | Train Acc: 0.9005 | Val Loss: 10.5299 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.1682 | Train Acc: 0.9432 | Val Loss: 80.3922 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.1144 | Train Acc: 0.9642 | Val Loss: 3.4145 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.0768 | Train Acc: 0.9783 | Val Loss: 0.0704 | Val Acc: 0.9820\n",
      "Epoch [6/30] Train Loss: 0.0572 | Train Acc: 0.9817 | Val Loss: 0.0695 | Val Acc: 0.9835\n",
      "Epoch [7/30] Train Loss: 0.0533 | Train Acc: 0.9829 | Val Loss: 0.0458 | Val Acc: 0.9850\n",
      "Epoch [8/30] Train Loss: 0.0399 | Train Acc: 0.9851 | Val Loss: 0.0335 | Val Acc: 0.9955\n",
      "Epoch [9/30] Train Loss: 0.0338 | Train Acc: 0.9897 | Val Loss: 0.0499 | Val Acc: 0.9805\n",
      "Epoch [10/30] Train Loss: 0.0357 | Train Acc: 0.9874 | Val Loss: 0.0338 | Val Acc: 0.9940\n",
      "Epoch [11/30] Train Loss: 0.0212 | Train Acc: 0.9954 | Val Loss: 0.0205 | Val Acc: 0.9970\n",
      "Epoch [12/30] Train Loss: 0.0174 | Train Acc: 0.9966 | Val Loss: 0.0244 | Val Acc: 0.9970\n",
      "Epoch [13/30] Train Loss: 0.0109 | Train Acc: 0.9992 | Val Loss: 0.0325 | Val Acc: 0.9940\n",
      "Epoch [14/30] Train Loss: 0.0103 | Train Acc: 0.9981 | Val Loss: 0.0250 | Val Acc: 0.9940\n",
      "Epoch [15/30] Train Loss: 0.0107 | Train Acc: 0.9977 | Val Loss: 0.0366 | Val Acc: 0.9940\n",
      "Epoch [16/30] Train Loss: 0.0076 | Train Acc: 0.9989 | Val Loss: 0.0293 | Val Acc: 0.9940\n",
      "Epoch [17/30] Train Loss: 0.0076 | Train Acc: 0.9992 | Val Loss: 0.0251 | Val Acc: 0.9940\n",
      "Epoch [18/30] Train Loss: 0.0076 | Train Acc: 0.9989 | Val Loss: 0.0425 | Val Acc: 0.9940\n",
      "Epoch [19/30] Train Loss: 0.0076 | Train Acc: 0.9973 | Val Loss: 0.0373 | Val Acc: 0.9940\n",
      "Epoch [20/30] Train Loss: 0.0094 | Train Acc: 0.9973 | Val Loss: 0.0426 | Val Acc: 0.9940\n",
      "Epoch [21/30] Train Loss: 0.0094 | Train Acc: 0.9970 | Val Loss: 0.0430 | Val Acc: 0.9940\n",
      "Epoch [22/30] Train Loss: 0.0062 | Train Acc: 0.9985 | Val Loss: 0.0368 | Val Acc: 0.9940\n",
      "Epoch [23/30] Train Loss: 0.0050 | Train Acc: 0.9996 | Val Loss: 0.0342 | Val Acc: 0.9940\n",
      "Epoch [24/30] Train Loss: 0.0045 | Train Acc: 0.9989 | Val Loss: 0.0350 | Val Acc: 0.9940\n",
      "Epoch [25/30] Train Loss: 0.0039 | Train Acc: 1.0000 | Val Loss: 0.0447 | Val Acc: 0.9940\n",
      "Epoch [26/30] Train Loss: 0.0040 | Train Acc: 0.9996 | Val Loss: 0.0410 | Val Acc: 0.9940\n",
      "Epoch [27/30] Train Loss: 0.0035 | Train Acc: 0.9996 | Val Loss: 0.0489 | Val Acc: 0.9940\n",
      "Epoch [28/30] Train Loss: 0.0073 | Train Acc: 0.9985 | Val Loss: 0.0449 | Val Acc: 0.9925\n",
      "Epoch [29/30] Train Loss: 0.0058 | Train Acc: 0.9985 | Val Loss: 0.0432 | Val Acc: 0.9940\n",
      "Epoch [30/30] Train Loss: 0.0026 | Train Acc: 1.0000 | Val Loss: 0.0476 | Val Acc: 0.9940\n",
      "Training complete!\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch [1/30] Train Loss: 0.4115 | Train Acc: 0.8056 | Val Loss: 0.3285 | Val Acc: 0.8604\n",
      "Epoch [2/30] Train Loss: 0.2529 | Train Acc: 0.8975 | Val Loss: 9.3247 | Val Acc: 0.2342\n",
      "Epoch [3/30] Train Loss: 0.1710 | Train Acc: 0.9444 | Val Loss: 37.9766 | Val Acc: 0.2342\n",
      "Epoch [4/30] Train Loss: 0.1112 | Train Acc: 0.9649 | Val Loss: 5.1716 | Val Acc: 0.2342\n",
      "Epoch [5/30] Train Loss: 0.0796 | Train Acc: 0.9748 | Val Loss: 0.1469 | Val Acc: 0.9565\n",
      "Epoch [6/30] Train Loss: 0.0572 | Train Acc: 0.9802 | Val Loss: 0.0945 | Val Acc: 0.9760\n",
      "Epoch [7/30] Train Loss: 0.0393 | Train Acc: 0.9897 | Val Loss: 0.0857 | Val Acc: 0.9775\n",
      "Epoch [8/30] Train Loss: 0.0313 | Train Acc: 0.9924 | Val Loss: 0.0918 | Val Acc: 0.9805\n",
      "Epoch [9/30] Train Loss: 0.0230 | Train Acc: 0.9950 | Val Loss: 0.0871 | Val Acc: 0.9820\n",
      "Epoch [10/30] Train Loss: 0.0198 | Train Acc: 0.9958 | Val Loss: 0.0786 | Val Acc: 0.9790\n",
      "Epoch [11/30] Train Loss: 0.0152 | Train Acc: 0.9962 | Val Loss: 0.0763 | Val Acc: 0.9835\n",
      "Epoch [12/30] Train Loss: 0.0115 | Train Acc: 0.9970 | Val Loss: 0.0818 | Val Acc: 0.9835\n",
      "Epoch [13/30] Train Loss: 0.0099 | Train Acc: 0.9985 | Val Loss: 0.0872 | Val Acc: 0.9850\n",
      "Epoch [14/30] Train Loss: 0.0097 | Train Acc: 0.9981 | Val Loss: 0.0797 | Val Acc: 0.9850\n",
      "Epoch [15/30] Train Loss: 0.0097 | Train Acc: 0.9981 | Val Loss: 0.0881 | Val Acc: 0.9835\n",
      "Epoch [16/30] Train Loss: 0.0057 | Train Acc: 0.9996 | Val Loss: 0.0957 | Val Acc: 0.9835\n",
      "Epoch [17/30] Train Loss: 0.0077 | Train Acc: 0.9985 | Val Loss: 0.0935 | Val Acc: 0.9850\n",
      "Epoch [18/30] Train Loss: 0.0057 | Train Acc: 0.9992 | Val Loss: 0.0906 | Val Acc: 0.9835\n",
      "Epoch [19/30] Train Loss: 0.0052 | Train Acc: 0.9996 | Val Loss: 0.0986 | Val Acc: 0.9835\n",
      "Epoch [20/30] Train Loss: 0.0093 | Train Acc: 0.9977 | Val Loss: 0.0921 | Val Acc: 0.9850\n",
      "Epoch [21/30] Train Loss: 0.0066 | Train Acc: 0.9992 | Val Loss: 0.0992 | Val Acc: 0.9850\n",
      "Epoch [22/30] Train Loss: 0.0047 | Train Acc: 0.9996 | Val Loss: 0.0959 | Val Acc: 0.9850\n",
      "Epoch [23/30] Train Loss: 0.0036 | Train Acc: 1.0000 | Val Loss: 0.0970 | Val Acc: 0.9850\n",
      "Epoch [24/30] Train Loss: 0.0028 | Train Acc: 1.0000 | Val Loss: 0.0978 | Val Acc: 0.9850\n",
      "Epoch [25/30] Train Loss: 0.0034 | Train Acc: 0.9996 | Val Loss: 0.1037 | Val Acc: 0.9850\n",
      "Epoch [26/30] Train Loss: 0.0039 | Train Acc: 0.9992 | Val Loss: 0.0962 | Val Acc: 0.9850\n",
      "Epoch [27/30] Train Loss: 0.0030 | Train Acc: 0.9996 | Val Loss: 0.0956 | Val Acc: 0.9850\n",
      "Epoch [28/30] Train Loss: 0.0035 | Train Acc: 0.9996 | Val Loss: 0.0917 | Val Acc: 0.9850\n",
      "Epoch [29/30] Train Loss: 0.0031 | Train Acc: 1.0000 | Val Loss: 0.0974 | Val Acc: 0.9850\n",
      "Epoch [30/30] Train Loss: 0.0029 | Train Acc: 0.9996 | Val Loss: 0.0919 | Val Acc: 0.9850\n",
      "Training complete!\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch [1/30] Train Loss: 0.3773 | Train Acc: 0.8296 | Val Loss: 0.3239 | Val Acc: 0.8649\n",
      "Epoch [2/30] Train Loss: 0.2527 | Train Acc: 0.8975 | Val Loss: 0.3304 | Val Acc: 0.8649\n",
      "Epoch [3/30] Train Loss: 0.1851 | Train Acc: 0.9284 | Val Loss: 0.1939 | Val Acc: 0.9279\n",
      "Epoch [4/30] Train Loss: 0.1373 | Train Acc: 0.9539 | Val Loss: 0.1315 | Val Acc: 0.9580\n",
      "Epoch [5/30] Train Loss: 0.1023 | Train Acc: 0.9596 | Val Loss: 0.1435 | Val Acc: 0.9429\n",
      "Epoch [6/30] Train Loss: 0.0803 | Train Acc: 0.9737 | Val Loss: 0.1253 | Val Acc: 0.9640\n",
      "Epoch [7/30] Train Loss: 0.0519 | Train Acc: 0.9844 | Val Loss: 0.1461 | Val Acc: 0.9595\n",
      "Epoch [8/30] Train Loss: 0.0444 | Train Acc: 0.9859 | Val Loss: 0.1130 | Val Acc: 0.9730\n",
      "Epoch [9/30] Train Loss: 0.0311 | Train Acc: 0.9924 | Val Loss: 0.1139 | Val Acc: 0.9760\n",
      "Epoch [10/30] Train Loss: 0.0239 | Train Acc: 0.9939 | Val Loss: 0.1355 | Val Acc: 0.9775\n",
      "Epoch [11/30] Train Loss: 0.0232 | Train Acc: 0.9939 | Val Loss: 0.1139 | Val Acc: 0.9805\n",
      "Epoch [12/30] Train Loss: 0.0177 | Train Acc: 0.9947 | Val Loss: 0.1269 | Val Acc: 0.9805\n",
      "Epoch [13/30] Train Loss: 0.0134 | Train Acc: 0.9973 | Val Loss: 0.1244 | Val Acc: 0.9805\n",
      "Epoch [14/30] Train Loss: 0.0109 | Train Acc: 0.9985 | Val Loss: 0.1199 | Val Acc: 0.9805\n",
      "Epoch [15/30] Train Loss: 0.0125 | Train Acc: 0.9973 | Val Loss: 0.1322 | Val Acc: 0.9805\n",
      "Epoch [16/30] Train Loss: 0.0105 | Train Acc: 0.9985 | Val Loss: 0.1295 | Val Acc: 0.9805\n",
      "Epoch [17/30] Train Loss: 0.0072 | Train Acc: 1.0000 | Val Loss: 0.1255 | Val Acc: 0.9805\n",
      "Epoch [18/30] Train Loss: 0.0088 | Train Acc: 0.9973 | Val Loss: 0.1192 | Val Acc: 0.9805\n",
      "Epoch [19/30] Train Loss: 0.0068 | Train Acc: 0.9992 | Val Loss: 0.1261 | Val Acc: 0.9805\n",
      "Epoch [20/30] Train Loss: 0.0071 | Train Acc: 0.9992 | Val Loss: 0.1273 | Val Acc: 0.9805\n",
      "Epoch [21/30] Train Loss: 0.0045 | Train Acc: 0.9996 | Val Loss: 0.1241 | Val Acc: 0.9805\n",
      "Epoch [22/30] Train Loss: 0.0051 | Train Acc: 0.9989 | Val Loss: 0.1293 | Val Acc: 0.9805\n",
      "Epoch [23/30] Train Loss: 0.0050 | Train Acc: 0.9985 | Val Loss: 0.1291 | Val Acc: 0.9805\n",
      "Epoch [24/30] Train Loss: 0.0038 | Train Acc: 1.0000 | Val Loss: 0.1380 | Val Acc: 0.9805\n",
      "Epoch [25/30] Train Loss: 0.0061 | Train Acc: 0.9992 | Val Loss: 0.1316 | Val Acc: 0.9805\n",
      "Epoch [26/30] Train Loss: 0.0051 | Train Acc: 0.9996 | Val Loss: 0.1366 | Val Acc: 0.9805\n",
      "Epoch [27/30] Train Loss: 0.0044 | Train Acc: 0.9992 | Val Loss: 0.1411 | Val Acc: 0.9805\n",
      "Epoch [28/30] Train Loss: 0.0043 | Train Acc: 0.9996 | Val Loss: 0.1377 | Val Acc: 0.9805\n",
      "Epoch [29/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.1443 | Val Acc: 0.9805\n",
      "Epoch [30/30] Train Loss: 0.0044 | Train Acc: 0.9989 | Val Loss: 0.1414 | Val Acc: 0.9805\n",
      "Training complete!\n",
      "\n",
      "===== Running: semi_synthetic | individual | simple =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train Loss: 0.2371 | Train Accuracy: 0.8769\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8891412994093594, 'recall': 0.958843704066634, 'f1-score': 0.9226779820839227, 'support': 2041.0}, 'treatment': {'precision': 0.8185745140388769, 'recall': 0.608346709470305, 'f1-score': 0.6979742173112339, 'support': 623.0}, 'accuracy': 0.8768768768768769, 'macro avg': {'precision': 0.8538579067241181, 'recall': 0.7835952067684695, 'f1-score': 0.8103260996975783, 'support': 2664.0}, 'weighted avg': {'precision': 0.8726386315092803, 'recall': 0.8768768768768769, 'f1-score': 0.8701290160728923, 'support': 2664.0}}\n",
      "Test Loss: 0.2309 | Test Accuracy: 0.8829\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8872987477638641, 'recall': 0.9706457925636007, 'f1-score': 0.9271028037383178, 'support': 511.0}, 'treatment': {'precision': 0.8598130841121495, 'recall': 0.5935483870967742, 'f1-score': 0.7022900763358778, 'support': 155.0}, 'accuracy': 0.8828828828828829, 'macro avg': {'precision': 0.8735559159380069, 'recall': 0.7820970898301874, 'f1-score': 0.8146964400370977, 'support': 666.0}, 'weighted avg': {'precision': 0.880901934151228, 'recall': 0.8828828828828829, 'f1-score': 0.874781523336849, 'support': 666.0}}\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2338 | Train Accuracy: 0.8825\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8970588235294118, 'recall': 0.9563939245467908, 'f1-score': 0.9257766184491345, 'support': 2041.0}, 'treatment': {'precision': 0.8176229508196722, 'recall': 0.6404494382022472, 'f1-score': 0.7182718271827183, 'support': 623.0}, 'accuracy': 0.8825075075075075, 'macro avg': {'precision': 0.857340887174542, 'recall': 0.7984216813745191, 'f1-score': 0.8220242228159265, 'support': 2664.0}, 'weighted avg': {'precision': 0.8784820409850546, 'recall': 0.8825075075075075, 'f1-score': 0.8772497847558248, 'support': 2664.0}}\n",
      "Test Loss: 0.2456 | Test Accuracy: 0.8769\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8994413407821229, 'recall': 0.9452054794520548, 'f1-score': 0.9217557251908397, 'support': 511.0}, 'treatment': {'precision': 0.7829457364341085, 'recall': 0.6516129032258065, 'f1-score': 0.7112676056338029, 'support': 155.0}, 'accuracy': 0.8768768768768769, 'macro avg': {'precision': 0.8411935386081157, 'recall': 0.7984091913389306, 'f1-score': 0.8165116654123212, 'support': 666.0}, 'weighted avg': {'precision': 0.8723290004308584, 'recall': 0.8768768768768769, 'f1-score': 0.8727682499185564, 'support': 666.0}}\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train Loss: 0.2296 | Train Accuracy: 0.8904\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.9017447199265382, 'recall': 0.9618021547502449, 'f1-score': 0.9308056872037914, 'support': 2042.0}, 'treatment': {'precision': 0.8395061728395061, 'recall': 0.6559485530546624, 'f1-score': 0.7364620938628159, 'support': 622.0}, 'accuracy': 0.8903903903903904, 'macro avg': {'precision': 0.8706254463830221, 'recall': 0.8088753539024536, 'f1-score': 0.8336338905333036, 'support': 2664.0}, 'weighted avg': {'precision': 0.8872130471457071, 'recall': 0.8903903903903904, 'f1-score': 0.8854296680378428, 'support': 2664.0}}\n",
      "Test Loss: 0.2623 | Test Accuracy: 0.8694\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8810810810810811, 'recall': 0.9588235294117647, 'f1-score': 0.9183098591549296, 'support': 510.0}, 'treatment': {'precision': 0.8108108108108109, 'recall': 0.5769230769230769, 'f1-score': 0.6741573033707865, 'support': 156.0}, 'accuracy': 0.8693693693693694, 'macro avg': {'precision': 0.845945945945946, 'recall': 0.7678733031674208, 'f1-score': 0.796233581262858, 'support': 666.0}, 'weighted avg': {'precision': 0.8646213781348916, 'recall': 0.8693693693693694, 'f1-score': 0.8611209722144997, 'support': 666.0}}\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2345 | Train Accuracy: 0.8836\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8954337899543379, 'recall': 0.9603330068560235, 'f1-score': 0.9267485822306238, 'support': 2042.0}, 'treatment': {'precision': 0.8291139240506329, 'recall': 0.6318327974276527, 'f1-score': 0.7171532846715328, 'support': 622.0}, 'accuracy': 0.8836336336336337, 'macro avg': {'precision': 0.8622738570024854, 'recall': 0.7960829021418381, 'f1-score': 0.8219509334510784, 'support': 2664.0}, 'weighted avg': {'precision': 0.8799491966389833, 'recall': 0.8836336336336337, 'f1-score': 0.87781154203477, 'support': 2664.0}}\n",
      "Test Loss: 0.2429 | Test Accuracy: 0.8919\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.9025735294117647, 'recall': 0.9627450980392157, 'f1-score': 0.9316888045540797, 'support': 510.0}, 'treatment': {'precision': 0.8442622950819673, 'recall': 0.6602564102564102, 'f1-score': 0.7410071942446043, 'support': 156.0}, 'accuracy': 0.8918918918918919, 'macro avg': {'precision': 0.8734179122468659, 'recall': 0.811500754147813, 'f1-score': 0.8363479993993419, 'support': 666.0}, 'weighted avg': {'precision': 0.8889150420912717, 'recall': 0.8918918918918919, 'f1-score': 0.887024643580689, 'support': 666.0}}\n",
      "\n",
      "--- Fold 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2356 | Train Accuracy: 0.8859\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8968036529680365, 'recall': 0.9618021547502449, 'f1-score': 0.9281663516068053, 'support': 2042.0}, 'treatment': {'precision': 0.8354430379746836, 'recall': 0.6366559485530546, 'f1-score': 0.7226277372262774, 'support': 622.0}, 'accuracy': 0.8858858858858859, 'macro avg': {'precision': 0.86612334547136, 'recall': 0.7992290516516498, 'f1-score': 0.8253970444165413, 'support': 2664.0}, 'weighted avg': {'precision': 0.8824769628306998, 'recall': 0.8858858858858859, 'f1-score': 0.8801764799308712, 'support': 2664.0}}\n",
      "Test Loss: 0.2397 | Test Accuracy: 0.8889\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.9052044609665427, 'recall': 0.9549019607843138, 'f1-score': 0.9293893129770993, 'support': 510.0}, 'treatment': {'precision': 0.8203125, 'recall': 0.6730769230769231, 'f1-score': 0.7394366197183099, 'support': 156.0}, 'accuracy': 0.8888888888888888, 'macro avg': {'precision': 0.8627584804832713, 'recall': 0.8139894419306184, 'f1-score': 0.8344129663477046, 'support': 666.0}, 'weighted avg': {'precision': 0.8853198574969021, 'recall': 0.8888888888888888, 'f1-score': 0.8848958893308965, 'support': 666.0}}\n",
      "\n",
      "===== Running: fixed | agnostic | nn =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch [1/30] Train Loss: 0.5785 | Train Acc: 0.7431 | Val Loss: 0.5656 | Val Acc: 0.7673\n",
      "Epoch [2/30] Train Loss: 0.5567 | Train Acc: 0.7660 | Val Loss: 6.9622 | Val Acc: 0.7673\n",
      "Epoch [3/30] Train Loss: 0.5568 | Train Acc: 0.7652 | Val Loss: 217.6278 | Val Acc: 0.7673\n",
      "Epoch [4/30] Train Loss: 0.5507 | Train Acc: 0.7671 | Val Loss: 35.0112 | Val Acc: 0.7673\n",
      "Epoch [5/30] Train Loss: 0.5531 | Train Acc: 0.7645 | Val Loss: 5.1318 | Val Acc: 0.7673\n",
      "Epoch [6/30] Train Loss: 0.5532 | Train Acc: 0.7652 | Val Loss: 20.4512 | Val Acc: 0.7673\n",
      "Epoch [7/30] Train Loss: 0.5476 | Train Acc: 0.7664 | Val Loss: 78.3795 | Val Acc: 0.7673\n",
      "Epoch [8/30] Train Loss: 0.5484 | Train Acc: 0.7656 | Val Loss: 9.4391 | Val Acc: 0.7673\n",
      "Epoch [9/30] Train Loss: 0.5514 | Train Acc: 0.7664 | Val Loss: 1.0358 | Val Acc: 0.7673\n",
      "Epoch [10/30] Train Loss: 0.5463 | Train Acc: 0.7652 | Val Loss: 2.8315 | Val Acc: 0.7673\n",
      "Epoch [11/30] Train Loss: 0.5497 | Train Acc: 0.7668 | Val Loss: 0.5595 | Val Acc: 0.7673\n",
      "Epoch [12/30] Train Loss: 0.5458 | Train Acc: 0.7660 | Val Loss: 9.0505 | Val Acc: 0.7673\n",
      "Epoch [13/30] Train Loss: 0.5468 | Train Acc: 0.7668 | Val Loss: 0.8459 | Val Acc: 0.2327\n",
      "Epoch [14/30] Train Loss: 0.5486 | Train Acc: 0.7660 | Val Loss: 0.7801 | Val Acc: 0.7673\n",
      "Epoch [15/30] Train Loss: 0.5462 | Train Acc: 0.7668 | Val Loss: 5.0695 | Val Acc: 0.7673\n",
      "Epoch [16/30] Train Loss: 0.5476 | Train Acc: 0.7656 | Val Loss: 2.6193 | Val Acc: 0.7673\n",
      "Epoch [17/30] Train Loss: 0.5480 | Train Acc: 0.7652 | Val Loss: 1.1848 | Val Acc: 0.7673\n",
      "Epoch [18/30] Train Loss: 0.5465 | Train Acc: 0.7675 | Val Loss: 15.3055 | Val Acc: 0.7673\n",
      "Epoch [19/30] Train Loss: 0.5460 | Train Acc: 0.7660 | Val Loss: 7.1710 | Val Acc: 0.7673\n",
      "Epoch [20/30] Train Loss: 0.5455 | Train Acc: 0.7656 | Val Loss: 1.0037 | Val Acc: 0.7673\n",
      "Epoch [21/30] Train Loss: 0.5430 | Train Acc: 0.7671 | Val Loss: 1.1436 | Val Acc: 0.7673\n",
      "Epoch [22/30] Train Loss: 0.5461 | Train Acc: 0.7664 | Val Loss: 0.6605 | Val Acc: 0.7673\n",
      "Epoch [23/30] Train Loss: 0.5455 | Train Acc: 0.7664 | Val Loss: 0.5449 | Val Acc: 0.7673\n",
      "Epoch [24/30] Train Loss: 0.5448 | Train Acc: 0.7671 | Val Loss: 2.5020 | Val Acc: 0.2327\n",
      "Epoch [25/30] Train Loss: 0.5458 | Train Acc: 0.7660 | Val Loss: 0.9006 | Val Acc: 0.7673\n",
      "Epoch [26/30] Train Loss: 0.5470 | Train Acc: 0.7664 | Val Loss: 0.7859 | Val Acc: 0.7673\n",
      "Epoch [27/30] Train Loss: 0.5434 | Train Acc: 0.7668 | Val Loss: 1.0685 | Val Acc: 0.7673\n",
      "Epoch [28/30] Train Loss: 0.5472 | Train Acc: 0.7652 | Val Loss: 1.1381 | Val Acc: 0.7673\n",
      "Epoch [29/30] Train Loss: 0.5440 | Train Acc: 0.7671 | Val Loss: 2.4016 | Val Acc: 0.2327\n",
      "Epoch [30/30] Train Loss: 0.5460 | Train Acc: 0.7645 | Val Loss: 1.4377 | Val Acc: 0.7673\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2 ---\n",
      "Epoch [1/30] Train Loss: 0.6011 | Train Acc: 0.7035 | Val Loss: 0.5448 | Val Acc: 0.7673\n",
      "Epoch [2/30] Train Loss: 0.5555 | Train Acc: 0.7664 | Val Loss: 6.2296 | Val Acc: 0.7673\n",
      "Epoch [3/30] Train Loss: 0.5512 | Train Acc: 0.7664 | Val Loss: 9.5357 | Val Acc: 0.7673\n",
      "Epoch [4/30] Train Loss: 0.5575 | Train Acc: 0.7652 | Val Loss: 87.1274 | Val Acc: 0.7673\n",
      "Epoch [5/30] Train Loss: 0.5523 | Train Acc: 0.7664 | Val Loss: 12.2738 | Val Acc: 0.7673\n",
      "Epoch [6/30] Train Loss: 0.5494 | Train Acc: 0.7668 | Val Loss: 21.0914 | Val Acc: 0.7673\n",
      "Epoch [7/30] Train Loss: 0.5505 | Train Acc: 0.7649 | Val Loss: 61.1019 | Val Acc: 0.7673\n",
      "Epoch [8/30] Train Loss: 0.5497 | Train Acc: 0.7660 | Val Loss: 43.1619 | Val Acc: 0.2327\n",
      "Epoch [9/30] Train Loss: 0.5506 | Train Acc: 0.7652 | Val Loss: 10.3838 | Val Acc: 0.7673\n",
      "Epoch [10/30] Train Loss: 0.5457 | Train Acc: 0.7679 | Val Loss: 0.5350 | Val Acc: 0.7673\n",
      "Epoch [11/30] Train Loss: 0.5494 | Train Acc: 0.7645 | Val Loss: 3.0636 | Val Acc: 0.7673\n",
      "Epoch [12/30] Train Loss: 0.5468 | Train Acc: 0.7664 | Val Loss: 6.6674 | Val Acc: 0.7673\n",
      "Epoch [13/30] Train Loss: 0.5499 | Train Acc: 0.7656 | Val Loss: 11.1596 | Val Acc: 0.7673\n",
      "Epoch [14/30] Train Loss: 0.5487 | Train Acc: 0.7641 | Val Loss: 1.3588 | Val Acc: 0.2327\n",
      "Epoch [15/30] Train Loss: 0.5483 | Train Acc: 0.7645 | Val Loss: 6.1625 | Val Acc: 0.7673\n",
      "Epoch [16/30] Train Loss: 0.5467 | Train Acc: 0.7660 | Val Loss: 3.3503 | Val Acc: 0.7673\n",
      "Epoch [17/30] Train Loss: 0.5450 | Train Acc: 0.7671 | Val Loss: 14.5482 | Val Acc: 0.7673\n",
      "Epoch [18/30] Train Loss: 0.5470 | Train Acc: 0.7660 | Val Loss: 3.3840 | Val Acc: 0.2327\n",
      "Epoch [19/30] Train Loss: 0.5491 | Train Acc: 0.7656 | Val Loss: 4.3084 | Val Acc: 0.7673\n",
      "Epoch [20/30] Train Loss: 0.5450 | Train Acc: 0.7664 | Val Loss: 0.9535 | Val Acc: 0.2327\n",
      "Epoch [21/30] Train Loss: 0.5450 | Train Acc: 0.7683 | Val Loss: 1.5914 | Val Acc: 0.7673\n",
      "Epoch [22/30] Train Loss: 0.5450 | Train Acc: 0.7652 | Val Loss: 1.4761 | Val Acc: 0.7673\n",
      "Epoch [23/30] Train Loss: 0.5463 | Train Acc: 0.7649 | Val Loss: 1.0913 | Val Acc: 0.7673\n",
      "Epoch [24/30] Train Loss: 0.5443 | Train Acc: 0.7660 | Val Loss: 2.5822 | Val Acc: 0.7673\n",
      "Epoch [25/30] Train Loss: 0.5462 | Train Acc: 0.7660 | Val Loss: 6.9488 | Val Acc: 0.7673\n",
      "Epoch [26/30] Train Loss: 0.5435 | Train Acc: 0.7668 | Val Loss: 1.0659 | Val Acc: 0.7673\n",
      "Epoch [27/30] Train Loss: 0.5471 | Train Acc: 0.7668 | Val Loss: 1.8629 | Val Acc: 0.7673\n",
      "Epoch [28/30] Train Loss: 0.5460 | Train Acc: 0.7671 | Val Loss: 2.4230 | Val Acc: 0.7673\n",
      "Epoch [29/30] Train Loss: 0.5434 | Train Acc: 0.7675 | Val Loss: 4.2467 | Val Acc: 0.7673\n",
      "Epoch [30/30] Train Loss: 0.5458 | Train Acc: 0.7652 | Val Loss: 1.6867 | Val Acc: 0.7673\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 3 ---\n",
      "Epoch [1/30] Train Loss: 0.5848 | Train Acc: 0.7359 | Val Loss: 0.5621 | Val Acc: 0.7658\n",
      "Epoch [2/30] Train Loss: 0.5625 | Train Acc: 0.7626 | Val Loss: 19.9982 | Val Acc: 0.2342\n",
      "Epoch [3/30] Train Loss: 0.5541 | Train Acc: 0.7664 | Val Loss: 16.5171 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.5522 | Train Acc: 0.7668 | Val Loss: 51.5087 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.5495 | Train Acc: 0.7656 | Val Loss: 15.8928 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.5449 | Train Acc: 0.7687 | Val Loss: 28.0809 | Val Acc: 0.7658\n",
      "Epoch [7/30] Train Loss: 0.5491 | Train Acc: 0.7660 | Val Loss: 5.8366 | Val Acc: 0.7658\n",
      "Epoch [8/30] Train Loss: 0.5520 | Train Acc: 0.7656 | Val Loss: 0.9242 | Val Acc: 0.7658\n",
      "Epoch [9/30] Train Loss: 0.5490 | Train Acc: 0.7675 | Val Loss: 184.0265 | Val Acc: 0.2342\n",
      "Epoch [10/30] Train Loss: 0.5448 | Train Acc: 0.7683 | Val Loss: 13.8648 | Val Acc: 0.2342\n",
      "Epoch [11/30] Train Loss: 0.5474 | Train Acc: 0.7660 | Val Loss: 4.1167 | Val Acc: 0.7658\n",
      "Epoch [12/30] Train Loss: 0.5474 | Train Acc: 0.7664 | Val Loss: 4.1201 | Val Acc: 0.7658\n",
      "Epoch [13/30] Train Loss: 0.5450 | Train Acc: 0.7679 | Val Loss: 11.7162 | Val Acc: 0.2342\n",
      "Epoch [14/30] Train Loss: 0.5481 | Train Acc: 0.7668 | Val Loss: 0.7336 | Val Acc: 0.7658\n",
      "Epoch [15/30] Train Loss: 0.5475 | Train Acc: 0.7649 | Val Loss: 19.1052 | Val Acc: 0.2342\n",
      "Epoch [16/30] Train Loss: 0.5450 | Train Acc: 0.7683 | Val Loss: 0.8787 | Val Acc: 0.7658\n",
      "Epoch [17/30] Train Loss: 0.5467 | Train Acc: 0.7671 | Val Loss: 3.5600 | Val Acc: 0.7658\n",
      "Epoch [18/30] Train Loss: 0.5449 | Train Acc: 0.7675 | Val Loss: 0.6638 | Val Acc: 0.7658\n",
      "Epoch [19/30] Train Loss: 0.5450 | Train Acc: 0.7664 | Val Loss: 10.7775 | Val Acc: 0.2342\n",
      "Epoch [20/30] Train Loss: 0.5449 | Train Acc: 0.7664 | Val Loss: 4.9575 | Val Acc: 0.2342\n",
      "Epoch [21/30] Train Loss: 0.5457 | Train Acc: 0.7664 | Val Loss: 1.1811 | Val Acc: 0.7658\n",
      "Epoch [22/30] Train Loss: 0.5419 | Train Acc: 0.7671 | Val Loss: 1.6072 | Val Acc: 0.7658\n",
      "Epoch [23/30] Train Loss: 0.5443 | Train Acc: 0.7664 | Val Loss: 2.3885 | Val Acc: 0.7658\n",
      "Epoch [24/30] Train Loss: 0.5473 | Train Acc: 0.7664 | Val Loss: 0.8445 | Val Acc: 0.2342\n",
      "Epoch [25/30] Train Loss: 0.5453 | Train Acc: 0.7664 | Val Loss: 0.8443 | Val Acc: 0.7658\n",
      "Epoch [26/30] Train Loss: 0.5472 | Train Acc: 0.7656 | Val Loss: 0.5914 | Val Acc: 0.7658\n",
      "Epoch [27/30] Train Loss: 0.5477 | Train Acc: 0.7652 | Val Loss: 0.5331 | Val Acc: 0.7658\n",
      "Epoch [28/30] Train Loss: 0.5422 | Train Acc: 0.7675 | Val Loss: 6.4566 | Val Acc: 0.2342\n",
      "Epoch [29/30] Train Loss: 0.5488 | Train Acc: 0.7649 | Val Loss: 0.5470 | Val Acc: 0.7658\n",
      "Epoch [30/30] Train Loss: 0.5431 | Train Acc: 0.7683 | Val Loss: 8.0433 | Val Acc: 0.7658\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 4 ---\n",
      "Epoch [1/30] Train Loss: 0.6344 | Train Acc: 0.6593 | Val Loss: 0.5518 | Val Acc: 0.7658\n",
      "Epoch [2/30] Train Loss: 0.5564 | Train Acc: 0.7637 | Val Loss: 4.3211 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.5541 | Train Acc: 0.7664 | Val Loss: 3.4578 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.5536 | Train Acc: 0.7664 | Val Loss: 14.7049 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.5527 | Train Acc: 0.7664 | Val Loss: 97.6532 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.5460 | Train Acc: 0.7679 | Val Loss: 22.3774 | Val Acc: 0.7658\n",
      "Epoch [7/30] Train Loss: 0.5493 | Train Acc: 0.7671 | Val Loss: 3.2532 | Val Acc: 0.7658\n",
      "Epoch [8/30] Train Loss: 0.5507 | Train Acc: 0.7668 | Val Loss: 3.1210 | Val Acc: 0.7658\n",
      "Epoch [9/30] Train Loss: 0.5474 | Train Acc: 0.7668 | Val Loss: 0.5581 | Val Acc: 0.7658\n",
      "Epoch [10/30] Train Loss: 0.5462 | Train Acc: 0.7671 | Val Loss: 28.6144 | Val Acc: 0.7658\n",
      "Epoch [11/30] Train Loss: 0.5495 | Train Acc: 0.7660 | Val Loss: 6.3044 | Val Acc: 0.2342\n",
      "Epoch [12/30] Train Loss: 0.5498 | Train Acc: 0.7660 | Val Loss: 11.7828 | Val Acc: 0.2342\n",
      "Epoch [13/30] Train Loss: 0.5491 | Train Acc: 0.7668 | Val Loss: 5.9435 | Val Acc: 0.7658\n",
      "Epoch [14/30] Train Loss: 0.5474 | Train Acc: 0.7675 | Val Loss: 7.1923 | Val Acc: 0.2342\n",
      "Epoch [15/30] Train Loss: 0.5514 | Train Acc: 0.7637 | Val Loss: 27.0562 | Val Acc: 0.2342\n",
      "Epoch [16/30] Train Loss: 0.5478 | Train Acc: 0.7656 | Val Loss: 1.1678 | Val Acc: 0.7658\n",
      "Epoch [17/30] Train Loss: 0.5467 | Train Acc: 0.7668 | Val Loss: 8.4574 | Val Acc: 0.2342\n",
      "Epoch [18/30] Train Loss: 0.5465 | Train Acc: 0.7656 | Val Loss: 0.5390 | Val Acc: 0.7658\n",
      "Epoch [19/30] Train Loss: 0.5430 | Train Acc: 0.7679 | Val Loss: 2.3056 | Val Acc: 0.7658\n",
      "Epoch [20/30] Train Loss: 0.5455 | Train Acc: 0.7664 | Val Loss: 2.6997 | Val Acc: 0.7658\n",
      "Epoch [21/30] Train Loss: 0.5466 | Train Acc: 0.7668 | Val Loss: 1.9420 | Val Acc: 0.2342\n",
      "Epoch [22/30] Train Loss: 0.5461 | Train Acc: 0.7664 | Val Loss: 0.7913 | Val Acc: 0.7658\n",
      "Epoch [23/30] Train Loss: 0.5452 | Train Acc: 0.7671 | Val Loss: 2.0991 | Val Acc: 0.7658\n",
      "Epoch [24/30] Train Loss: 0.5444 | Train Acc: 0.7668 | Val Loss: 0.6108 | Val Acc: 0.7658\n",
      "Epoch [25/30] Train Loss: 0.5437 | Train Acc: 0.7668 | Val Loss: 2.1908 | Val Acc: 0.7658\n",
      "Epoch [26/30] Train Loss: 0.5442 | Train Acc: 0.7656 | Val Loss: 0.7926 | Val Acc: 0.7658\n",
      "Epoch [27/30] Train Loss: 0.5422 | Train Acc: 0.7683 | Val Loss: 2.7937 | Val Acc: 0.7658\n",
      "Epoch [28/30] Train Loss: 0.5454 | Train Acc: 0.7664 | Val Loss: 4.2163 | Val Acc: 0.7658\n",
      "Epoch [29/30] Train Loss: 0.5449 | Train Acc: 0.7671 | Val Loss: 7.8051 | Val Acc: 0.7658\n",
      "Epoch [30/30] Train Loss: 0.5449 | Train Acc: 0.7671 | Val Loss: 12.1473 | Val Acc: 0.7658\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 5 ---\n",
      "Epoch [1/30] Train Loss: 0.5927 | Train Acc: 0.7233 | Val Loss: 0.5613 | Val Acc: 0.7658\n",
      "Epoch [2/30] Train Loss: 0.5498 | Train Acc: 0.7687 | Val Loss: 13.2915 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.5507 | Train Acc: 0.7664 | Val Loss: 127.7951 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.5554 | Train Acc: 0.7652 | Val Loss: 214.7433 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.5559 | Train Acc: 0.7652 | Val Loss: 263.3932 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.5524 | Train Acc: 0.7652 | Val Loss: 42.3846 | Val Acc: 0.7658\n",
      "Epoch [7/30] Train Loss: 0.5485 | Train Acc: 0.7675 | Val Loss: 38.8398 | Val Acc: 0.7658\n",
      "Epoch [8/30] Train Loss: 0.5504 | Train Acc: 0.7660 | Val Loss: 57.4607 | Val Acc: 0.7658\n",
      "Epoch [9/30] Train Loss: 0.5500 | Train Acc: 0.7675 | Val Loss: 10.4664 | Val Acc: 0.7658\n",
      "Epoch [10/30] Train Loss: 0.5482 | Train Acc: 0.7652 | Val Loss: 16.0860 | Val Acc: 0.7658\n",
      "Epoch [11/30] Train Loss: 0.5483 | Train Acc: 0.7671 | Val Loss: 2.6299 | Val Acc: 0.7658\n",
      "Epoch [12/30] Train Loss: 0.5466 | Train Acc: 0.7660 | Val Loss: 12.6425 | Val Acc: 0.7658\n",
      "Epoch [13/30] Train Loss: 0.5475 | Train Acc: 0.7668 | Val Loss: 7.7261 | Val Acc: 0.7658\n",
      "Epoch [14/30] Train Loss: 0.5472 | Train Acc: 0.7668 | Val Loss: 9.0427 | Val Acc: 0.7658\n",
      "Epoch [15/30] Train Loss: 0.5479 | Train Acc: 0.7664 | Val Loss: 1.9549 | Val Acc: 0.7658\n",
      "Epoch [16/30] Train Loss: 0.5485 | Train Acc: 0.7652 | Val Loss: 11.6474 | Val Acc: 0.7658\n",
      "Epoch [17/30] Train Loss: 0.5483 | Train Acc: 0.7656 | Val Loss: 4.9555 | Val Acc: 0.7658\n",
      "Epoch [18/30] Train Loss: 0.5454 | Train Acc: 0.7675 | Val Loss: 2.7424 | Val Acc: 0.7658\n",
      "Epoch [19/30] Train Loss: 0.5471 | Train Acc: 0.7649 | Val Loss: 9.1118 | Val Acc: 0.7658\n",
      "Epoch [20/30] Train Loss: 0.5464 | Train Acc: 0.7656 | Val Loss: 4.7630 | Val Acc: 0.7658\n",
      "Epoch [21/30] Train Loss: 0.5478 | Train Acc: 0.7664 | Val Loss: 3.7248 | Val Acc: 0.7658\n",
      "Epoch [22/30] Train Loss: 0.5458 | Train Acc: 0.7668 | Val Loss: 1.3764 | Val Acc: 0.7658\n",
      "Epoch [23/30] Train Loss: 0.5443 | Train Acc: 0.7664 | Val Loss: 1.9472 | Val Acc: 0.7658\n",
      "Epoch [24/30] Train Loss: 0.5444 | Train Acc: 0.7671 | Val Loss: 3.7367 | Val Acc: 0.7658\n",
      "Epoch [25/30] Train Loss: 0.5453 | Train Acc: 0.7664 | Val Loss: 1.4125 | Val Acc: 0.7658\n",
      "Epoch [26/30] Train Loss: 0.5452 | Train Acc: 0.7668 | Val Loss: 2.1703 | Val Acc: 0.7658\n",
      "Epoch [27/30] Train Loss: 0.5456 | Train Acc: 0.7664 | Val Loss: 3.1093 | Val Acc: 0.7658\n",
      "Epoch [28/30] Train Loss: 0.5456 | Train Acc: 0.7664 | Val Loss: 6.6136 | Val Acc: 0.7658\n",
      "Epoch [29/30] Train Loss: 0.5497 | Train Acc: 0.7641 | Val Loss: 3.4441 | Val Acc: 0.7658\n",
      "Epoch [30/30] Train Loss: 0.5480 | Train Acc: 0.7656 | Val Loss: 0.6803 | Val Acc: 0.7658\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running: fixed | agnostic | simple =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train Loss: 0.5439 | Train Accuracy: 0.7661\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7661411411411412, 'recall': 1.0, 'f1-score': 0.8675876726886291, 'support': 2041.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 623.0}, 'accuracy': 0.7661411411411412, 'macro avg': {'precision': 0.3830705705705706, 'recall': 0.5, 'f1-score': 0.43379383634431457, 'support': 2664.0}, 'weighted avg': {'precision': 0.58697224814905, 'recall': 0.7661411411411412, 'f1-score': 0.6646946095936532, 'support': 2664.0}}\n",
      "Test Loss: 0.5426 | Test Accuracy: 0.7673\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7672672672672672, 'recall': 1.0, 'f1-score': 0.8683092608326253, 'support': 511.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 155.0}, 'accuracy': 0.7672672672672672, 'macro avg': {'precision': 0.3836336336336336, 'recall': 0.5, 'f1-score': 0.43415463041631264, 'support': 666.0}, 'weighted avg': {'precision': 0.5886990594197802, 'recall': 0.7672672672672672, 'f1-score': 0.6662252737019092, 'support': 666.0}}\n",
      "\n",
      "--- Fold 2 ---\n",
      "Train Loss: 0.5439 | Train Accuracy: 0.7661\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7661411411411412, 'recall': 1.0, 'f1-score': 0.8675876726886291, 'support': 2041.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 623.0}, 'accuracy': 0.7661411411411412, 'macro avg': {'precision': 0.3830705705705706, 'recall': 0.5, 'f1-score': 0.43379383634431457, 'support': 2664.0}, 'weighted avg': {'precision': 0.58697224814905, 'recall': 0.7661411411411412, 'f1-score': 0.6646946095936532, 'support': 2664.0}}\n",
      "Test Loss: 0.5426 | Test Accuracy: 0.7673\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7672672672672672, 'recall': 1.0, 'f1-score': 0.8683092608326253, 'support': 511.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 155.0}, 'accuracy': 0.7672672672672672, 'macro avg': {'precision': 0.3836336336336336, 'recall': 0.5, 'f1-score': 0.43415463041631264, 'support': 666.0}, 'weighted avg': {'precision': 0.5886990594197802, 'recall': 0.7672672672672672, 'f1-score': 0.6662252737019092, 'support': 666.0}}\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train Loss: 0.5435 | Train Accuracy: 0.7665\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7665165165165165, 'recall': 1.0, 'f1-score': 0.8678283042923927, 'support': 2042.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 622.0}, 'accuracy': 0.7665165165165165, 'macro avg': {'precision': 0.38325825825825827, 'recall': 0.5, 'f1-score': 0.43391415214619633, 'support': 2664.0}, 'weighted avg': {'precision': 0.5875475700926152, 'recall': 0.7665165165165165, 'f1-score': 0.6652047287406403, 'support': 2664.0}}\n",
      "Test Loss: 0.5443 | Test Accuracy: 0.7658\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7657657657657657, 'recall': 1.0, 'f1-score': 0.8673469387755102, 'support': 510.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 156.0}, 'accuracy': 0.7657657657657657, 'macro avg': {'precision': 0.38288288288288286, 'recall': 0.5, 'f1-score': 0.4336734693877551, 'support': 666.0}, 'weighted avg': {'precision': 0.5863972080188297, 'recall': 0.7657657657657657, 'f1-score': 0.6641845927560213, 'support': 666.0}}\n",
      "\n",
      "--- Fold 4 ---\n",
      "Train Loss: 0.5435 | Train Accuracy: 0.7665\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7665165165165165, 'recall': 1.0, 'f1-score': 0.8678283042923927, 'support': 2042.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 622.0}, 'accuracy': 0.7665165165165165, 'macro avg': {'precision': 0.38325825825825827, 'recall': 0.5, 'f1-score': 0.43391415214619633, 'support': 2664.0}, 'weighted avg': {'precision': 0.5875475700926152, 'recall': 0.7665165165165165, 'f1-score': 0.6652047287406403, 'support': 2664.0}}\n",
      "Test Loss: 0.5443 | Test Accuracy: 0.7658\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7657657657657657, 'recall': 1.0, 'f1-score': 0.8673469387755102, 'support': 510.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 156.0}, 'accuracy': 0.7657657657657657, 'macro avg': {'precision': 0.38288288288288286, 'recall': 0.5, 'f1-score': 0.4336734693877551, 'support': 666.0}, 'weighted avg': {'precision': 0.5863972080188297, 'recall': 0.7657657657657657, 'f1-score': 0.6641845927560213, 'support': 666.0}}\n",
      "\n",
      "--- Fold 5 ---\n",
      "Train Loss: 0.5435 | Train Accuracy: 0.7665\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7665165165165165, 'recall': 1.0, 'f1-score': 0.8678283042923927, 'support': 2042.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 622.0}, 'accuracy': 0.7665165165165165, 'macro avg': {'precision': 0.38325825825825827, 'recall': 0.5, 'f1-score': 0.43391415214619633, 'support': 2664.0}, 'weighted avg': {'precision': 0.5875475700926152, 'recall': 0.7665165165165165, 'f1-score': 0.6652047287406403, 'support': 2664.0}}\n",
      "Test Loss: 0.5443 | Test Accuracy: 0.7658\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.7657657657657657, 'recall': 1.0, 'f1-score': 0.8673469387755102, 'support': 510.0}, 'treatment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 156.0}, 'accuracy': 0.7657657657657657, 'macro avg': {'precision': 0.38288288288288286, 'recall': 0.5, 'f1-score': 0.4336734693877551, 'support': 666.0}, 'weighted avg': {'precision': 0.5863972080188297, 'recall': 0.7657657657657657, 'f1-score': 0.6641845927560213, 'support': 666.0}}\n",
      "\n",
      "===== Running: fixed | population | nn =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch [1/30] Train Loss: 0.4362 | Train Acc: 0.7988 | Val Loss: 0.2961 | Val Acc: 0.8664\n",
      "Epoch [2/30] Train Loss: 0.2833 | Train Acc: 0.8704 | Val Loss: 0.2373 | Val Acc: 0.8859\n",
      "Epoch [3/30] Train Loss: 0.2162 | Train Acc: 0.9123 | Val Loss: 0.1758 | Val Acc: 0.9369\n",
      "Epoch [4/30] Train Loss: 0.1539 | Train Acc: 0.9440 | Val Loss: 0.1417 | Val Acc: 0.9459\n",
      "Epoch [5/30] Train Loss: 0.1148 | Train Acc: 0.9558 | Val Loss: 0.1091 | Val Acc: 0.9640\n",
      "Epoch [6/30] Train Loss: 0.0867 | Train Acc: 0.9691 | Val Loss: 0.0926 | Val Acc: 0.9670\n",
      "Epoch [7/30] Train Loss: 0.0738 | Train Acc: 0.9748 | Val Loss: 0.0857 | Val Acc: 0.9730\n",
      "Epoch [8/30] Train Loss: 0.0585 | Train Acc: 0.9848 | Val Loss: 0.0855 | Val Acc: 0.9745\n",
      "Epoch [9/30] Train Loss: 0.0424 | Train Acc: 0.9855 | Val Loss: 0.0687 | Val Acc: 0.9820\n",
      "Epoch [10/30] Train Loss: 0.0373 | Train Acc: 0.9893 | Val Loss: 0.0639 | Val Acc: 0.9850\n",
      "Epoch [11/30] Train Loss: 0.0290 | Train Acc: 0.9924 | Val Loss: 0.0597 | Val Acc: 0.9865\n",
      "Epoch [12/30] Train Loss: 0.0219 | Train Acc: 0.9950 | Val Loss: 0.0563 | Val Acc: 0.9880\n",
      "Epoch [13/30] Train Loss: 0.0156 | Train Acc: 0.9966 | Val Loss: 0.0547 | Val Acc: 0.9880\n",
      "Epoch [14/30] Train Loss: 0.0127 | Train Acc: 0.9977 | Val Loss: 0.0548 | Val Acc: 0.9895\n",
      "Epoch [15/30] Train Loss: 0.0117 | Train Acc: 0.9981 | Val Loss: 0.0481 | Val Acc: 0.9925\n",
      "Epoch [16/30] Train Loss: 0.0127 | Train Acc: 0.9973 | Val Loss: 0.0474 | Val Acc: 0.9925\n",
      "Epoch [17/30] Train Loss: 0.0075 | Train Acc: 0.9992 | Val Loss: 0.0542 | Val Acc: 0.9910\n",
      "Epoch [18/30] Train Loss: 0.0070 | Train Acc: 0.9989 | Val Loss: 0.0538 | Val Acc: 0.9910\n",
      "Epoch [19/30] Train Loss: 0.0091 | Train Acc: 0.9985 | Val Loss: 0.0555 | Val Acc: 0.9910\n",
      "Epoch [20/30] Train Loss: 0.0096 | Train Acc: 0.9985 | Val Loss: 0.0618 | Val Acc: 0.9910\n",
      "Epoch [21/30] Train Loss: 0.0084 | Train Acc: 0.9985 | Val Loss: 0.0576 | Val Acc: 0.9910\n",
      "Epoch [22/30] Train Loss: 0.0082 | Train Acc: 0.9973 | Val Loss: 0.0661 | Val Acc: 0.9880\n",
      "Epoch [23/30] Train Loss: 0.0059 | Train Acc: 0.9992 | Val Loss: 0.0520 | Val Acc: 0.9925\n",
      "Epoch [24/30] Train Loss: 0.0063 | Train Acc: 0.9985 | Val Loss: 0.0619 | Val Acc: 0.9910\n",
      "Epoch [25/30] Train Loss: 0.0061 | Train Acc: 0.9996 | Val Loss: 0.0567 | Val Acc: 0.9910\n",
      "Epoch [26/30] Train Loss: 0.0062 | Train Acc: 0.9989 | Val Loss: 0.0544 | Val Acc: 0.9925\n",
      "Epoch [27/30] Train Loss: 0.0047 | Train Acc: 0.9992 | Val Loss: 0.0549 | Val Acc: 0.9925\n",
      "Epoch [28/30] Train Loss: 0.0047 | Train Acc: 0.9992 | Val Loss: 0.0564 | Val Acc: 0.9925\n",
      "Epoch [29/30] Train Loss: 0.0055 | Train Acc: 0.9985 | Val Loss: 0.0594 | Val Acc: 0.9910\n",
      "Epoch [30/30] Train Loss: 0.0047 | Train Acc: 0.9989 | Val Loss: 0.0577 | Val Acc: 0.9925\n",
      "Training complete!\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch [1/30] Train Loss: 0.4168 | Train Acc: 0.8075 | Val Loss: 0.2698 | Val Acc: 0.8724\n",
      "Epoch [2/30] Train Loss: 0.2706 | Train Acc: 0.8754 | Val Loss: 0.1887 | Val Acc: 0.9144\n",
      "Epoch [3/30] Train Loss: 0.1990 | Train Acc: 0.9169 | Val Loss: 0.1150 | Val Acc: 0.9640\n",
      "Epoch [4/30] Train Loss: 0.1409 | Train Acc: 0.9516 | Val Loss: 0.0784 | Val Acc: 0.9685\n",
      "Epoch [5/30] Train Loss: 0.1157 | Train Acc: 0.9604 | Val Loss: 0.0561 | Val Acc: 0.9820\n",
      "Epoch [6/30] Train Loss: 0.0791 | Train Acc: 0.9733 | Val Loss: 0.0556 | Val Acc: 0.9805\n",
      "Epoch [7/30] Train Loss: 0.0616 | Train Acc: 0.9840 | Val Loss: 0.0401 | Val Acc: 0.9850\n",
      "Epoch [8/30] Train Loss: 0.0379 | Train Acc: 0.9893 | Val Loss: 0.0309 | Val Acc: 0.9910\n",
      "Epoch [9/30] Train Loss: 0.0321 | Train Acc: 0.9916 | Val Loss: 0.0219 | Val Acc: 0.9925\n",
      "Epoch [10/30] Train Loss: 0.0313 | Train Acc: 0.9901 | Val Loss: 0.0297 | Val Acc: 0.9895\n",
      "Epoch [11/30] Train Loss: 0.0199 | Train Acc: 0.9950 | Val Loss: 0.0192 | Val Acc: 0.9940\n",
      "Epoch [12/30] Train Loss: 0.0175 | Train Acc: 0.9950 | Val Loss: 0.0149 | Val Acc: 0.9970\n",
      "Epoch [13/30] Train Loss: 0.0140 | Train Acc: 0.9973 | Val Loss: 0.0146 | Val Acc: 0.9970\n",
      "Epoch [14/30] Train Loss: 0.0100 | Train Acc: 0.9985 | Val Loss: 0.0135 | Val Acc: 0.9970\n",
      "Epoch [15/30] Train Loss: 0.0126 | Train Acc: 0.9966 | Val Loss: 0.0172 | Val Acc: 0.9970\n",
      "Epoch [16/30] Train Loss: 0.0118 | Train Acc: 0.9958 | Val Loss: 0.0184 | Val Acc: 0.9970\n",
      "Epoch [17/30] Train Loss: 0.0086 | Train Acc: 0.9992 | Val Loss: 0.0165 | Val Acc: 0.9970\n",
      "Epoch [18/30] Train Loss: 0.0085 | Train Acc: 0.9977 | Val Loss: 0.0147 | Val Acc: 0.9970\n",
      "Epoch [19/30] Train Loss: 0.0076 | Train Acc: 0.9977 | Val Loss: 0.0144 | Val Acc: 0.9970\n",
      "Epoch [20/30] Train Loss: 0.0056 | Train Acc: 1.0000 | Val Loss: 0.0164 | Val Acc: 0.9970\n",
      "Epoch [21/30] Train Loss: 0.0081 | Train Acc: 0.9981 | Val Loss: 0.0168 | Val Acc: 0.9970\n",
      "Epoch [22/30] Train Loss: 0.0039 | Train Acc: 0.9996 | Val Loss: 0.0167 | Val Acc: 0.9970\n",
      "Epoch [23/30] Train Loss: 0.0037 | Train Acc: 1.0000 | Val Loss: 0.0168 | Val Acc: 0.9970\n",
      "Epoch [24/30] Train Loss: 0.0059 | Train Acc: 0.9989 | Val Loss: 0.0150 | Val Acc: 0.9970\n",
      "Epoch [25/30] Train Loss: 0.0059 | Train Acc: 0.9989 | Val Loss: 0.0163 | Val Acc: 0.9970\n",
      "Epoch [26/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0154 | Val Acc: 0.9970\n",
      "Epoch [27/30] Train Loss: 0.0064 | Train Acc: 0.9973 | Val Loss: 0.0153 | Val Acc: 0.9970\n",
      "Epoch [28/30] Train Loss: 0.0038 | Train Acc: 0.9992 | Val Loss: 0.0153 | Val Acc: 0.9970\n",
      "Epoch [29/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0165 | Val Acc: 0.9970\n",
      "Epoch [30/30] Train Loss: 0.0063 | Train Acc: 0.9973 | Val Loss: 0.0145 | Val Acc: 0.9970\n",
      "Training complete!\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch [1/30] Train Loss: 0.3938 | Train Acc: 0.8338 | Val Loss: 0.3004 | Val Acc: 0.8844\n",
      "Epoch [2/30] Train Loss: 0.2533 | Train Acc: 0.8815 | Val Loss: 0.2007 | Val Acc: 0.9129\n",
      "Epoch [3/30] Train Loss: 0.1732 | Train Acc: 0.9348 | Val Loss: 0.1308 | Val Acc: 0.9505\n",
      "Epoch [4/30] Train Loss: 0.1191 | Train Acc: 0.9581 | Val Loss: 0.1066 | Val Acc: 0.9595\n",
      "Epoch [5/30] Train Loss: 0.0918 | Train Acc: 0.9684 | Val Loss: 0.0919 | Val Acc: 0.9655\n",
      "Epoch [6/30] Train Loss: 0.0722 | Train Acc: 0.9771 | Val Loss: 0.0663 | Val Acc: 0.9760\n",
      "Epoch [7/30] Train Loss: 0.0550 | Train Acc: 0.9821 | Val Loss: 0.0594 | Val Acc: 0.9775\n",
      "Epoch [8/30] Train Loss: 0.0397 | Train Acc: 0.9882 | Val Loss: 0.0481 | Val Acc: 0.9805\n",
      "Epoch [9/30] Train Loss: 0.0394 | Train Acc: 0.9859 | Val Loss: 0.0496 | Val Acc: 0.9790\n",
      "Epoch [10/30] Train Loss: 0.0236 | Train Acc: 0.9935 | Val Loss: 0.0418 | Val Acc: 0.9880\n",
      "Epoch [11/30] Train Loss: 0.0232 | Train Acc: 0.9931 | Val Loss: 0.0473 | Val Acc: 0.9865\n",
      "Epoch [12/30] Train Loss: 0.0171 | Train Acc: 0.9954 | Val Loss: 0.0460 | Val Acc: 0.9865\n",
      "Epoch [13/30] Train Loss: 0.0156 | Train Acc: 0.9954 | Val Loss: 0.0434 | Val Acc: 0.9880\n",
      "Epoch [14/30] Train Loss: 0.0105 | Train Acc: 0.9981 | Val Loss: 0.0377 | Val Acc: 0.9910\n",
      "Epoch [15/30] Train Loss: 0.0118 | Train Acc: 0.9973 | Val Loss: 0.0388 | Val Acc: 0.9910\n",
      "Epoch [16/30] Train Loss: 0.0084 | Train Acc: 0.9981 | Val Loss: 0.0363 | Val Acc: 0.9925\n",
      "Epoch [17/30] Train Loss: 0.0092 | Train Acc: 0.9973 | Val Loss: 0.0452 | Val Acc: 0.9910\n",
      "Epoch [18/30] Train Loss: 0.0083 | Train Acc: 0.9981 | Val Loss: 0.0397 | Val Acc: 0.9910\n",
      "Epoch [19/30] Train Loss: 0.0056 | Train Acc: 0.9996 | Val Loss: 0.0449 | Val Acc: 0.9895\n",
      "Epoch [20/30] Train Loss: 0.0066 | Train Acc: 0.9985 | Val Loss: 0.0430 | Val Acc: 0.9910\n",
      "Epoch [21/30] Train Loss: 0.0046 | Train Acc: 0.9996 | Val Loss: 0.0468 | Val Acc: 0.9910\n",
      "Epoch [22/30] Train Loss: 0.0063 | Train Acc: 0.9985 | Val Loss: 0.0422 | Val Acc: 0.9910\n",
      "Epoch [23/30] Train Loss: 0.0047 | Train Acc: 0.9992 | Val Loss: 0.0438 | Val Acc: 0.9895\n",
      "Epoch [24/30] Train Loss: 0.0051 | Train Acc: 0.9981 | Val Loss: 0.0408 | Val Acc: 0.9880\n",
      "Epoch [25/30] Train Loss: 0.0037 | Train Acc: 0.9996 | Val Loss: 0.0450 | Val Acc: 0.9880\n",
      "Epoch [26/30] Train Loss: 0.0040 | Train Acc: 0.9989 | Val Loss: 0.0417 | Val Acc: 0.9880\n",
      "Epoch [27/30] Train Loss: 0.0045 | Train Acc: 0.9992 | Val Loss: 0.0438 | Val Acc: 0.9910\n",
      "Epoch [28/30] Train Loss: 0.0040 | Train Acc: 0.9992 | Val Loss: 0.0472 | Val Acc: 0.9910\n",
      "Epoch [29/30] Train Loss: 0.0038 | Train Acc: 0.9992 | Val Loss: 0.0476 | Val Acc: 0.9910\n",
      "Epoch [30/30] Train Loss: 0.0034 | Train Acc: 0.9996 | Val Loss: 0.0520 | Val Acc: 0.9895\n",
      "Training complete!\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch [1/30] Train Loss: 0.4750 | Train Acc: 0.7630 | Val Loss: 0.2838 | Val Acc: 0.8769\n",
      "Epoch [2/30] Train Loss: 0.2665 | Train Acc: 0.8880 | Val Loss: 0.2223 | Val Acc: 0.9144\n",
      "Epoch [3/30] Train Loss: 0.1988 | Train Acc: 0.9196 | Val Loss: 0.1817 | Val Acc: 0.9324\n",
      "Epoch [4/30] Train Loss: 0.1573 | Train Acc: 0.9413 | Val Loss: 0.1455 | Val Acc: 0.9459\n",
      "Epoch [5/30] Train Loss: 0.1179 | Train Acc: 0.9554 | Val Loss: 0.1156 | Val Acc: 0.9655\n",
      "Epoch [6/30] Train Loss: 0.0934 | Train Acc: 0.9649 | Val Loss: 0.0985 | Val Acc: 0.9730\n",
      "Epoch [7/30] Train Loss: 0.0740 | Train Acc: 0.9752 | Val Loss: 0.0885 | Val Acc: 0.9775\n",
      "Epoch [8/30] Train Loss: 0.0587 | Train Acc: 0.9775 | Val Loss: 0.0754 | Val Acc: 0.9775\n",
      "Epoch [9/30] Train Loss: 0.0435 | Train Acc: 0.9859 | Val Loss: 0.0749 | Val Acc: 0.9820\n",
      "Epoch [10/30] Train Loss: 0.0399 | Train Acc: 0.9878 | Val Loss: 0.0711 | Val Acc: 0.9835\n",
      "Epoch [11/30] Train Loss: 0.0309 | Train Acc: 0.9912 | Val Loss: 0.0670 | Val Acc: 0.9850\n",
      "Epoch [12/30] Train Loss: 0.0278 | Train Acc: 0.9931 | Val Loss: 0.0675 | Val Acc: 0.9850\n",
      "Epoch [13/30] Train Loss: 0.0249 | Train Acc: 0.9943 | Val Loss: 0.0714 | Val Acc: 0.9835\n",
      "Epoch [14/30] Train Loss: 0.0234 | Train Acc: 0.9935 | Val Loss: 0.0676 | Val Acc: 0.9865\n",
      "Epoch [15/30] Train Loss: 0.0155 | Train Acc: 0.9973 | Val Loss: 0.0689 | Val Acc: 0.9850\n",
      "Epoch [16/30] Train Loss: 0.0159 | Train Acc: 0.9966 | Val Loss: 0.0687 | Val Acc: 0.9865\n",
      "Epoch [17/30] Train Loss: 0.0120 | Train Acc: 0.9985 | Val Loss: 0.0678 | Val Acc: 0.9850\n",
      "Epoch [18/30] Train Loss: 0.0146 | Train Acc: 0.9962 | Val Loss: 0.0631 | Val Acc: 0.9865\n",
      "Epoch [19/30] Train Loss: 0.0104 | Train Acc: 0.9977 | Val Loss: 0.0639 | Val Acc: 0.9865\n",
      "Epoch [20/30] Train Loss: 0.0141 | Train Acc: 0.9962 | Val Loss: 0.0651 | Val Acc: 0.9865\n",
      "Epoch [21/30] Train Loss: 0.0074 | Train Acc: 0.9985 | Val Loss: 0.0615 | Val Acc: 0.9880\n",
      "Epoch [22/30] Train Loss: 0.0077 | Train Acc: 0.9977 | Val Loss: 0.0656 | Val Acc: 0.9880\n",
      "Epoch [23/30] Train Loss: 0.0067 | Train Acc: 0.9992 | Val Loss: 0.0649 | Val Acc: 0.9880\n",
      "Epoch [24/30] Train Loss: 0.0070 | Train Acc: 0.9989 | Val Loss: 0.0640 | Val Acc: 0.9880\n",
      "Epoch [25/30] Train Loss: 0.0055 | Train Acc: 0.9992 | Val Loss: 0.0679 | Val Acc: 0.9880\n",
      "Epoch [26/30] Train Loss: 0.0047 | Train Acc: 0.9985 | Val Loss: 0.0688 | Val Acc: 0.9880\n",
      "Epoch [27/30] Train Loss: 0.0066 | Train Acc: 0.9985 | Val Loss: 0.0692 | Val Acc: 0.9880\n",
      "Epoch [28/30] Train Loss: 0.0065 | Train Acc: 0.9992 | Val Loss: 0.0693 | Val Acc: 0.9880\n",
      "Epoch [29/30] Train Loss: 0.0042 | Train Acc: 1.0000 | Val Loss: 0.0701 | Val Acc: 0.9880\n",
      "Epoch [30/30] Train Loss: 0.0061 | Train Acc: 0.9981 | Val Loss: 0.0681 | Val Acc: 0.9880\n",
      "Training complete!\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch [1/30] Train Loss: 0.4056 | Train Acc: 0.8140 | Val Loss: 0.3033 | Val Acc: 0.8679\n",
      "Epoch [2/30] Train Loss: 0.2557 | Train Acc: 0.8834 | Val Loss: 0.2211 | Val Acc: 0.8964\n",
      "Epoch [3/30] Train Loss: 0.2030 | Train Acc: 0.9196 | Val Loss: 0.1689 | Val Acc: 0.9444\n",
      "Epoch [4/30] Train Loss: 0.1499 | Train Acc: 0.9428 | Val Loss: 0.1307 | Val Acc: 0.9429\n",
      "Epoch [5/30] Train Loss: 0.1116 | Train Acc: 0.9569 | Val Loss: 0.1055 | Val Acc: 0.9595\n",
      "Epoch [6/30] Train Loss: 0.0905 | Train Acc: 0.9684 | Val Loss: 0.1011 | Val Acc: 0.9625\n",
      "Epoch [7/30] Train Loss: 0.0630 | Train Acc: 0.9794 | Val Loss: 0.0699 | Val Acc: 0.9775\n",
      "Epoch [8/30] Train Loss: 0.0478 | Train Acc: 0.9859 | Val Loss: 0.0655 | Val Acc: 0.9745\n",
      "Epoch [9/30] Train Loss: 0.0411 | Train Acc: 0.9893 | Val Loss: 0.0685 | Val Acc: 0.9745\n",
      "Epoch [10/30] Train Loss: 0.0376 | Train Acc: 0.9893 | Val Loss: 0.0546 | Val Acc: 0.9820\n",
      "Epoch [11/30] Train Loss: 0.0230 | Train Acc: 0.9950 | Val Loss: 0.0523 | Val Acc: 0.9850\n",
      "Epoch [12/30] Train Loss: 0.0225 | Train Acc: 0.9943 | Val Loss: 0.0515 | Val Acc: 0.9865\n",
      "Epoch [13/30] Train Loss: 0.0159 | Train Acc: 0.9966 | Val Loss: 0.0545 | Val Acc: 0.9865\n",
      "Epoch [14/30] Train Loss: 0.0178 | Train Acc: 0.9962 | Val Loss: 0.0527 | Val Acc: 0.9850\n",
      "Epoch [15/30] Train Loss: 0.0163 | Train Acc: 0.9973 | Val Loss: 0.0490 | Val Acc: 0.9820\n",
      "Epoch [16/30] Train Loss: 0.0153 | Train Acc: 0.9973 | Val Loss: 0.0550 | Val Acc: 0.9850\n",
      "Epoch [17/30] Train Loss: 0.0105 | Train Acc: 0.9985 | Val Loss: 0.0548 | Val Acc: 0.9850\n",
      "Epoch [18/30] Train Loss: 0.0102 | Train Acc: 0.9981 | Val Loss: 0.0514 | Val Acc: 0.9865\n",
      "Epoch [19/30] Train Loss: 0.0078 | Train Acc: 0.9981 | Val Loss: 0.0542 | Val Acc: 0.9865\n",
      "Epoch [20/30] Train Loss: 0.0063 | Train Acc: 0.9992 | Val Loss: 0.0513 | Val Acc: 0.9865\n",
      "Epoch [21/30] Train Loss: 0.0058 | Train Acc: 0.9992 | Val Loss: 0.0556 | Val Acc: 0.9865\n",
      "Epoch [22/30] Train Loss: 0.0057 | Train Acc: 0.9992 | Val Loss: 0.0545 | Val Acc: 0.9865\n",
      "Epoch [23/30] Train Loss: 0.0075 | Train Acc: 0.9981 | Val Loss: 0.0568 | Val Acc: 0.9865\n",
      "Epoch [24/30] Train Loss: 0.0066 | Train Acc: 0.9985 | Val Loss: 0.0524 | Val Acc: 0.9895\n",
      "Epoch [25/30] Train Loss: 0.0067 | Train Acc: 0.9989 | Val Loss: 0.0580 | Val Acc: 0.9865\n",
      "Epoch [26/30] Train Loss: 0.0043 | Train Acc: 0.9996 | Val Loss: 0.0594 | Val Acc: 0.9865\n",
      "Epoch [27/30] Train Loss: 0.0037 | Train Acc: 0.9992 | Val Loss: 0.0573 | Val Acc: 0.9865\n",
      "Epoch [28/30] Train Loss: 0.0028 | Train Acc: 1.0000 | Val Loss: 0.0600 | Val Acc: 0.9865\n",
      "Epoch [29/30] Train Loss: 0.0035 | Train Acc: 0.9996 | Val Loss: 0.0584 | Val Acc: 0.9865\n",
      "Epoch [30/30] Train Loss: 0.0031 | Train Acc: 1.0000 | Val Loss: 0.0635 | Val Acc: 0.9835\n",
      "Training complete!\n",
      "\n",
      "===== Running: fixed | population | simple =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train Loss: 0.2934 | Train Accuracy: 0.8600\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8564102564102564, 'recall': 0.9818716315531603, 'f1-score': 0.9148596210910751, 'support': 2041.0}, 'treatment': {'precision': 0.8858024691358025, 'recall': 0.4606741573033708, 'f1-score': 0.6061246040126715, 'support': 623.0}, 'accuracy': 0.859984984984985, 'macro avg': {'precision': 0.8711063627730294, 'recall': 0.7212728944282656, 'f1-score': 0.7604921125518733, 'support': 2664.0}, 'weighted avg': {'precision': 0.8632838857375893, 'recall': 0.859984984984985, 'f1-score': 0.8426592023073493, 'support': 2664.0}}\n",
      "Test Loss: 0.2938 | Test Accuracy: 0.8529\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8470588235294118, 'recall': 0.9863013698630136, 'f1-score': 0.9113924050632911, 'support': 511.0}, 'treatment': {'precision': 0.9014084507042254, 'recall': 0.4129032258064516, 'f1-score': 0.5663716814159292, 'support': 155.0}, 'accuracy': 0.8528528528528528, 'macro avg': {'precision': 0.8742336371168186, 'recall': 0.6996022978347326, 'f1-score': 0.7388820432396102, 'support': 666.0}, 'weighted avg': {'precision': 0.8597077607848113, 'recall': 0.8528528528528528, 'f1-score': 0.8310947891994156, 'support': 666.0}}\n",
      "\n",
      "--- Fold 2 ---\n",
      "Train Loss: 0.2966 | Train Accuracy: 0.8675\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8619210977701544, 'recall': 0.984811366976972, 'f1-score': 0.9192773839469471, 'support': 2041.0}, 'treatment': {'precision': 0.9066265060240963, 'recall': 0.48314606741573035, 'f1-score': 0.6303664921465969, 'support': 623.0}, 'accuracy': 0.8674924924924925, 'macro avg': {'precision': 0.8842738018971253, 'recall': 0.7339787171963512, 'f1-score': 0.774821938046772, 'support': 2664.0}, 'weighted avg': {'precision': 0.8723758535292406, 'recall': 0.8674924924924925, 'f1-score': 0.8517130124786221, 'support': 2664.0}}\n",
      "Test Loss: 0.2819 | Test Accuracy: 0.8814\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8763066202090593, 'recall': 0.9843444227005871, 'f1-score': 0.9271889400921659, 'support': 511.0}, 'treatment': {'precision': 0.9130434782608695, 'recall': 0.5419354838709678, 'f1-score': 0.680161943319838, 'support': 155.0}, 'accuracy': 0.8813813813813813, 'macro avg': {'precision': 0.8946750492349644, 'recall': 0.7631399532857774, 'f1-score': 0.8036754417060019, 'support': 666.0}, 'weighted avg': {'precision': 0.8848564895754716, 'recall': 0.8813813813813813, 'f1-score': 0.8696976720745822, 'support': 666.0}}\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train Loss: 0.2881 | Train Accuracy: 0.8664\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8602564102564103, 'recall': 0.9857982370225269, 'f1-score': 0.9187585577361935, 'support': 2042.0}, 'treatment': {'precision': 0.9104938271604939, 'recall': 0.4742765273311897, 'f1-score': 0.6236786469344608, 'support': 622.0}, 'accuracy': 0.8663663663663663, 'macro avg': {'precision': 0.8853751187084521, 'recall': 0.7300373821768583, 'f1-score': 0.7712186023353271, 'support': 2664.0}, 'weighted avg': {'precision': 0.8719860173563878, 'recall': 0.8663663663663663, 'f1-score': 0.8498622722562094, 'support': 2664.0}}\n",
      "Test Loss: 0.3152 | Test Accuracy: 0.8453\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8431703204047217, 'recall': 0.9803921568627451, 'f1-score': 0.9066183136899365, 'support': 510.0}, 'treatment': {'precision': 0.863013698630137, 'recall': 0.40384615384615385, 'f1-score': 0.5502183406113537, 'support': 156.0}, 'accuracy': 0.8453453453453453, 'macro avg': {'precision': 0.8530920095174294, 'recall': 0.6921191553544495, 'f1-score': 0.728418327150645, 'support': 666.0}, 'weighted avg': {'precision': 0.8478183189079722, 'recall': 0.8453453453453453, 'f1-score': 0.823137238914773, 'support': 666.0}}\n",
      "\n",
      "--- Fold 4 ---\n",
      "Train Loss: 0.2931 | Train Accuracy: 0.8664\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8602564102564103, 'recall': 0.9857982370225269, 'f1-score': 0.9187585577361935, 'support': 2042.0}, 'treatment': {'precision': 0.9104938271604939, 'recall': 0.4742765273311897, 'f1-score': 0.6236786469344608, 'support': 622.0}, 'accuracy': 0.8663663663663663, 'macro avg': {'precision': 0.8853751187084521, 'recall': 0.7300373821768583, 'f1-score': 0.7712186023353271, 'support': 2664.0}, 'weighted avg': {'precision': 0.8719860173563878, 'recall': 0.8663663663663663, 'f1-score': 0.8498622722562094, 'support': 2664.0}}\n",
      "Test Loss: 0.2942 | Test Accuracy: 0.8634\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8605851979345955, 'recall': 0.9803921568627451, 'f1-score': 0.916590284142988, 'support': 510.0}, 'treatment': {'precision': 0.8823529411764706, 'recall': 0.4807692307692308, 'f1-score': 0.6224066390041494, 'support': 156.0}, 'accuracy': 0.8633633633633634, 'macro avg': {'precision': 0.871469069555533, 'recall': 0.7305806938159879, 'f1-score': 0.7694984615735687, 'support': 666.0}, 'weighted avg': {'precision': 0.8656839486038636, 'recall': 0.8633633633633634, 'f1-score': 0.8476824032996564, 'support': 666.0}}\n",
      "\n",
      "--- Fold 5 ---\n",
      "Train Loss: 0.2923 | Train Accuracy: 0.8634\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8582408198121264, 'recall': 0.9843290891283056, 'f1-score': 0.916970802919708, 'support': 2042.0}, 'treatment': {'precision': 0.9006211180124224, 'recall': 0.4662379421221865, 'f1-score': 0.614406779661017, 'support': 622.0}, 'accuracy': 0.8633633633633634, 'macro avg': {'precision': 0.8794309689122743, 'recall': 0.7252835156252461, 'f1-score': 0.7656887912903625, 'support': 2664.0}, 'weighted avg': {'precision': 0.8681359194670003, 'recall': 0.8633633633633634, 'f1-score': 0.8463271007924911, 'support': 2664.0}}\n",
      "Test Loss: 0.3029 | Test Accuracy: 0.8709\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8642611683848798, 'recall': 0.9862745098039216, 'f1-score': 0.9212454212454212, 'support': 510.0}, 'treatment': {'precision': 0.9166666666666666, 'recall': 0.4935897435897436, 'f1-score': 0.6416666666666667, 'support': 156.0}, 'accuracy': 0.8708708708708709, 'macro avg': {'precision': 0.8904639175257731, 'recall': 0.7399321266968326, 'f1-score': 0.781456043956044, 'support': 666.0}, 'weighted avg': {'precision': 0.8765363301445777, 'recall': 0.8708708708708709, 'f1-score': 0.8557585057585056, 'support': 666.0}}\n",
      "\n",
      "===== Running: fixed | individual | nn =====\n",
      "\n",
      "--- Fold 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Local\\Temp\\ipykernel_32640\\1094098609.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = 0\n",
      "C:\\Users\\91939\\AppData\\Local\\Temp\\ipykernel_32640\\1094098609.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Train Loss: 0.3752 | Train Acc: 0.8342 | Val Loss: 0.3443 | Val Acc: 0.8559\n",
      "Epoch [2/30] Train Loss: 0.2666 | Train Acc: 0.8841 | Val Loss: 9.5691 | Val Acc: 0.7673\n",
      "Epoch [3/30] Train Loss: 0.1907 | Train Acc: 0.9245 | Val Loss: 33.0594 | Val Acc: 0.7673\n",
      "Epoch [4/30] Train Loss: 0.1238 | Train Acc: 0.9604 | Val Loss: 0.3838 | Val Acc: 0.8844\n",
      "Epoch [5/30] Train Loss: 0.0797 | Train Acc: 0.9790 | Val Loss: 0.1292 | Val Acc: 0.9685\n",
      "Epoch [6/30] Train Loss: 0.0657 | Train Acc: 0.9813 | Val Loss: 0.1312 | Val Acc: 0.9685\n",
      "Epoch [7/30] Train Loss: 0.0486 | Train Acc: 0.9874 | Val Loss: 0.0934 | Val Acc: 0.9850\n",
      "Epoch [8/30] Train Loss: 0.0377 | Train Acc: 0.9901 | Val Loss: 0.0749 | Val Acc: 0.9850\n",
      "Epoch [9/30] Train Loss: 0.0334 | Train Acc: 0.9886 | Val Loss: 0.0659 | Val Acc: 0.9850\n",
      "Epoch [10/30] Train Loss: 0.0281 | Train Acc: 0.9924 | Val Loss: 0.1073 | Val Acc: 0.9805\n",
      "Epoch [11/30] Train Loss: 0.0211 | Train Acc: 0.9939 | Val Loss: 0.0717 | Val Acc: 0.9835\n",
      "Epoch [12/30] Train Loss: 0.0205 | Train Acc: 0.9954 | Val Loss: 0.0746 | Val Acc: 0.9880\n",
      "Epoch [13/30] Train Loss: 0.0143 | Train Acc: 0.9954 | Val Loss: 0.0783 | Val Acc: 0.9880\n",
      "Epoch [14/30] Train Loss: 0.0129 | Train Acc: 0.9970 | Val Loss: 0.0761 | Val Acc: 0.9895\n",
      "Epoch [15/30] Train Loss: 0.0108 | Train Acc: 0.9977 | Val Loss: 0.0715 | Val Acc: 0.9895\n",
      "Epoch [16/30] Train Loss: 0.0084 | Train Acc: 0.9977 | Val Loss: 0.0815 | Val Acc: 0.9895\n",
      "Epoch [17/30] Train Loss: 0.0091 | Train Acc: 0.9973 | Val Loss: 0.0706 | Val Acc: 0.9895\n",
      "Epoch [18/30] Train Loss: 0.0093 | Train Acc: 0.9977 | Val Loss: 0.0792 | Val Acc: 0.9910\n",
      "Epoch [19/30] Train Loss: 0.0098 | Train Acc: 0.9989 | Val Loss: 0.0895 | Val Acc: 0.9895\n",
      "Epoch [20/30] Train Loss: 0.0088 | Train Acc: 0.9981 | Val Loss: 0.0788 | Val Acc: 0.9895\n",
      "Epoch [21/30] Train Loss: 0.0067 | Train Acc: 0.9992 | Val Loss: 0.0921 | Val Acc: 0.9895\n",
      "Epoch [22/30] Train Loss: 0.0050 | Train Acc: 0.9996 | Val Loss: 0.0869 | Val Acc: 0.9895\n",
      "Epoch [23/30] Train Loss: 0.0056 | Train Acc: 0.9992 | Val Loss: 0.0836 | Val Acc: 0.9895\n",
      "Epoch [24/30] Train Loss: 0.0062 | Train Acc: 0.9989 | Val Loss: 0.0847 | Val Acc: 0.9895\n",
      "Epoch [25/30] Train Loss: 0.0042 | Train Acc: 0.9992 | Val Loss: 0.0777 | Val Acc: 0.9895\n",
      "Epoch [26/30] Train Loss: 0.0039 | Train Acc: 0.9996 | Val Loss: 0.0802 | Val Acc: 0.9895\n",
      "Epoch [27/30] Train Loss: 0.0053 | Train Acc: 0.9989 | Val Loss: 0.0753 | Val Acc: 0.9895\n",
      "Epoch [28/30] Train Loss: 0.0049 | Train Acc: 0.9992 | Val Loss: 0.0772 | Val Acc: 0.9895\n",
      "Epoch [29/30] Train Loss: 0.0040 | Train Acc: 0.9992 | Val Loss: 0.0820 | Val Acc: 0.9895\n",
      "Epoch [30/30] Train Loss: 0.0043 | Train Acc: 0.9996 | Val Loss: 0.0751 | Val Acc: 0.9895\n",
      "Training complete!\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch [1/30] Train Loss: 0.3702 | Train Acc: 0.8457 | Val Loss: 0.3644 | Val Acc: 0.8754\n",
      "Epoch [2/30] Train Loss: 0.2434 | Train Acc: 0.8956 | Val Loss: 3.4780 | Val Acc: 0.7673\n",
      "Epoch [3/30] Train Loss: 0.1717 | Train Acc: 0.9390 | Val Loss: 6.9246 | Val Acc: 0.7673\n",
      "Epoch [4/30] Train Loss: 0.1061 | Train Acc: 0.9657 | Val Loss: 18.8438 | Val Acc: 0.7673\n",
      "Epoch [5/30] Train Loss: 0.0813 | Train Acc: 0.9756 | Val Loss: 0.2001 | Val Acc: 0.9009\n",
      "Epoch [6/30] Train Loss: 0.0585 | Train Acc: 0.9817 | Val Loss: 0.0539 | Val Acc: 0.9925\n",
      "Epoch [7/30] Train Loss: 0.0419 | Train Acc: 0.9870 | Val Loss: 0.0291 | Val Acc: 0.9880\n",
      "Epoch [8/30] Train Loss: 0.0343 | Train Acc: 0.9912 | Val Loss: 0.0266 | Val Acc: 0.9925\n",
      "Epoch [9/30] Train Loss: 0.0236 | Train Acc: 0.9950 | Val Loss: 0.0386 | Val Acc: 0.9925\n",
      "Epoch [10/30] Train Loss: 0.0205 | Train Acc: 0.9931 | Val Loss: 0.0166 | Val Acc: 0.9970\n",
      "Epoch [11/30] Train Loss: 0.0159 | Train Acc: 0.9962 | Val Loss: 0.0158 | Val Acc: 0.9985\n",
      "Epoch [12/30] Train Loss: 0.0124 | Train Acc: 0.9966 | Val Loss: 0.0097 | Val Acc: 0.9985\n",
      "Epoch [13/30] Train Loss: 0.0092 | Train Acc: 0.9992 | Val Loss: 0.0132 | Val Acc: 0.9985\n",
      "Epoch [14/30] Train Loss: 0.0136 | Train Acc: 0.9962 | Val Loss: 0.0099 | Val Acc: 0.9985\n",
      "Epoch [15/30] Train Loss: 0.0076 | Train Acc: 0.9989 | Val Loss: 0.0138 | Val Acc: 0.9985\n",
      "Epoch [16/30] Train Loss: 0.0075 | Train Acc: 0.9981 | Val Loss: 0.0087 | Val Acc: 0.9985\n",
      "Epoch [17/30] Train Loss: 0.0077 | Train Acc: 0.9989 | Val Loss: 0.0172 | Val Acc: 0.9985\n",
      "Epoch [18/30] Train Loss: 0.0060 | Train Acc: 0.9985 | Val Loss: 0.0154 | Val Acc: 0.9985\n",
      "Epoch [19/30] Train Loss: 0.0075 | Train Acc: 0.9977 | Val Loss: 0.0093 | Val Acc: 0.9985\n",
      "Epoch [20/30] Train Loss: 0.0084 | Train Acc: 0.9985 | Val Loss: 0.0149 | Val Acc: 0.9985\n",
      "Epoch [21/30] Train Loss: 0.0062 | Train Acc: 0.9985 | Val Loss: 0.0088 | Val Acc: 0.9985\n",
      "Epoch [22/30] Train Loss: 0.0057 | Train Acc: 0.9989 | Val Loss: 0.0129 | Val Acc: 0.9985\n",
      "Epoch [23/30] Train Loss: 0.0049 | Train Acc: 0.9992 | Val Loss: 0.0069 | Val Acc: 0.9985\n",
      "Epoch [24/30] Train Loss: 0.0032 | Train Acc: 1.0000 | Val Loss: 0.0130 | Val Acc: 0.9985\n",
      "Epoch [25/30] Train Loss: 0.0044 | Train Acc: 0.9996 | Val Loss: 0.0091 | Val Acc: 0.9985\n",
      "Epoch [26/30] Train Loss: 0.0053 | Train Acc: 0.9989 | Val Loss: 0.0077 | Val Acc: 0.9985\n",
      "Epoch [27/30] Train Loss: 0.0035 | Train Acc: 0.9996 | Val Loss: 0.0091 | Val Acc: 0.9985\n",
      "Epoch [28/30] Train Loss: 0.0033 | Train Acc: 0.9996 | Val Loss: 0.0131 | Val Acc: 0.9985\n",
      "Epoch [29/30] Train Loss: 0.0028 | Train Acc: 1.0000 | Val Loss: 0.0143 | Val Acc: 0.9985\n",
      "Epoch [30/30] Train Loss: 0.0044 | Train Acc: 0.9985 | Val Loss: 0.0131 | Val Acc: 0.9985\n",
      "Training complete!\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch [1/30] Train Loss: 0.4198 | Train Acc: 0.8011 | Val Loss: 0.3279 | Val Acc: 0.8544\n",
      "Epoch [2/30] Train Loss: 0.2635 | Train Acc: 0.8853 | Val Loss: 1.1408 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.1942 | Train Acc: 0.9295 | Val Loss: 152.7825 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.1413 | Train Acc: 0.9520 | Val Loss: 125.9193 | Val Acc: 0.7658\n",
      "Epoch [5/30] Train Loss: 0.0950 | Train Acc: 0.9733 | Val Loss: 5.3916 | Val Acc: 0.7658\n",
      "Epoch [6/30] Train Loss: 0.0794 | Train Acc: 0.9741 | Val Loss: 0.0764 | Val Acc: 0.9760\n",
      "Epoch [7/30] Train Loss: 0.0547 | Train Acc: 0.9844 | Val Loss: 0.0683 | Val Acc: 0.9760\n",
      "Epoch [8/30] Train Loss: 0.0509 | Train Acc: 0.9844 | Val Loss: 0.0613 | Val Acc: 0.9775\n",
      "Epoch [9/30] Train Loss: 0.0350 | Train Acc: 0.9920 | Val Loss: 0.0456 | Val Acc: 0.9850\n",
      "Epoch [10/30] Train Loss: 0.0243 | Train Acc: 0.9943 | Val Loss: 0.0586 | Val Acc: 0.9955\n",
      "Epoch [11/30] Train Loss: 0.0204 | Train Acc: 0.9962 | Val Loss: 0.0220 | Val Acc: 0.9955\n",
      "Epoch [12/30] Train Loss: 0.0157 | Train Acc: 0.9962 | Val Loss: 0.0260 | Val Acc: 0.9955\n",
      "Epoch [13/30] Train Loss: 0.0147 | Train Acc: 0.9970 | Val Loss: 0.0214 | Val Acc: 0.9970\n",
      "Epoch [14/30] Train Loss: 0.0133 | Train Acc: 0.9977 | Val Loss: 0.0223 | Val Acc: 0.9955\n",
      "Epoch [15/30] Train Loss: 0.0110 | Train Acc: 0.9977 | Val Loss: 0.0223 | Val Acc: 0.9970\n",
      "Epoch [16/30] Train Loss: 0.0127 | Train Acc: 0.9970 | Val Loss: 0.0207 | Val Acc: 0.9970\n",
      "Epoch [17/30] Train Loss: 0.0090 | Train Acc: 0.9977 | Val Loss: 0.0198 | Val Acc: 0.9970\n",
      "Epoch [18/30] Train Loss: 0.0081 | Train Acc: 0.9992 | Val Loss: 0.0187 | Val Acc: 0.9970\n",
      "Epoch [19/30] Train Loss: 0.0065 | Train Acc: 0.9989 | Val Loss: 0.0201 | Val Acc: 0.9970\n",
      "Epoch [20/30] Train Loss: 0.0079 | Train Acc: 0.9981 | Val Loss: 0.0198 | Val Acc: 0.9970\n",
      "Epoch [21/30] Train Loss: 0.0080 | Train Acc: 0.9985 | Val Loss: 0.0222 | Val Acc: 0.9970\n",
      "Epoch [22/30] Train Loss: 0.0049 | Train Acc: 1.0000 | Val Loss: 0.0175 | Val Acc: 0.9970\n",
      "Epoch [23/30] Train Loss: 0.0046 | Train Acc: 0.9992 | Val Loss: 0.0203 | Val Acc: 0.9970\n",
      "Epoch [24/30] Train Loss: 0.0053 | Train Acc: 0.9989 | Val Loss: 0.0198 | Val Acc: 0.9970\n",
      "Epoch [25/30] Train Loss: 0.0050 | Train Acc: 0.9989 | Val Loss: 0.0200 | Val Acc: 0.9970\n",
      "Epoch [26/30] Train Loss: 0.0067 | Train Acc: 0.9989 | Val Loss: 0.0218 | Val Acc: 0.9970\n",
      "Epoch [27/30] Train Loss: 0.0048 | Train Acc: 0.9992 | Val Loss: 0.0200 | Val Acc: 0.9970\n",
      "Epoch [28/30] Train Loss: 0.0034 | Train Acc: 1.0000 | Val Loss: 0.0211 | Val Acc: 0.9970\n",
      "Epoch [29/30] Train Loss: 0.0044 | Train Acc: 0.9992 | Val Loss: 0.0202 | Val Acc: 0.9970\n",
      "Epoch [30/30] Train Loss: 0.0033 | Train Acc: 1.0000 | Val Loss: 0.0197 | Val Acc: 0.9970\n",
      "Training complete!\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch [1/30] Train Loss: 0.3794 | Train Acc: 0.8308 | Val Loss: 0.2999 | Val Acc: 0.8604\n",
      "Epoch [2/30] Train Loss: 0.2396 | Train Acc: 0.9024 | Val Loss: 16.0216 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.1734 | Train Acc: 0.9375 | Val Loss: 37.1633 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.1388 | Train Acc: 0.9493 | Val Loss: 0.5268 | Val Acc: 0.7763\n",
      "Epoch [5/30] Train Loss: 0.0850 | Train Acc: 0.9756 | Val Loss: 0.0981 | Val Acc: 0.9685\n",
      "Epoch [6/30] Train Loss: 0.0571 | Train Acc: 0.9844 | Val Loss: 0.0955 | Val Acc: 0.9655\n",
      "Epoch [7/30] Train Loss: 0.0524 | Train Acc: 0.9836 | Val Loss: 0.0742 | Val Acc: 0.9715\n",
      "Epoch [8/30] Train Loss: 0.0399 | Train Acc: 0.9886 | Val Loss: 0.0766 | Val Acc: 0.9775\n",
      "Epoch [9/30] Train Loss: 0.0265 | Train Acc: 0.9916 | Val Loss: 0.0875 | Val Acc: 0.9820\n",
      "Epoch [10/30] Train Loss: 0.0247 | Train Acc: 0.9928 | Val Loss: 0.0825 | Val Acc: 0.9790\n",
      "Epoch [11/30] Train Loss: 0.0183 | Train Acc: 0.9973 | Val Loss: 0.0819 | Val Acc: 0.9820\n",
      "Epoch [12/30] Train Loss: 0.0177 | Train Acc: 0.9954 | Val Loss: 0.0758 | Val Acc: 0.9820\n",
      "Epoch [13/30] Train Loss: 0.0131 | Train Acc: 0.9973 | Val Loss: 0.0773 | Val Acc: 0.9805\n",
      "Epoch [14/30] Train Loss: 0.0112 | Train Acc: 0.9973 | Val Loss: 0.0828 | Val Acc: 0.9820\n",
      "Epoch [15/30] Train Loss: 0.0090 | Train Acc: 0.9989 | Val Loss: 0.0759 | Val Acc: 0.9820\n",
      "Epoch [16/30] Train Loss: 0.0113 | Train Acc: 0.9981 | Val Loss: 0.0746 | Val Acc: 0.9820\n",
      "Epoch [17/30] Train Loss: 0.0093 | Train Acc: 0.9977 | Val Loss: 0.0696 | Val Acc: 0.9820\n",
      "Epoch [18/30] Train Loss: 0.0081 | Train Acc: 0.9992 | Val Loss: 0.0852 | Val Acc: 0.9820\n",
      "Epoch [19/30] Train Loss: 0.0056 | Train Acc: 0.9996 | Val Loss: 0.0773 | Val Acc: 0.9820\n",
      "Epoch [20/30] Train Loss: 0.0064 | Train Acc: 0.9992 | Val Loss: 0.0878 | Val Acc: 0.9820\n",
      "Epoch [21/30] Train Loss: 0.0059 | Train Acc: 0.9985 | Val Loss: 0.0866 | Val Acc: 0.9820\n",
      "Epoch [22/30] Train Loss: 0.0042 | Train Acc: 1.0000 | Val Loss: 0.0893 | Val Acc: 0.9820\n",
      "Epoch [23/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0919 | Val Acc: 0.9820\n",
      "Epoch [24/30] Train Loss: 0.0051 | Train Acc: 0.9992 | Val Loss: 0.0900 | Val Acc: 0.9820\n",
      "Epoch [25/30] Train Loss: 0.0049 | Train Acc: 0.9996 | Val Loss: 0.0851 | Val Acc: 0.9820\n",
      "Epoch [26/30] Train Loss: 0.0044 | Train Acc: 0.9992 | Val Loss: 0.0859 | Val Acc: 0.9820\n",
      "Epoch [27/30] Train Loss: 0.0041 | Train Acc: 0.9996 | Val Loss: 0.0964 | Val Acc: 0.9820\n",
      "Epoch [28/30] Train Loss: 0.0026 | Train Acc: 1.0000 | Val Loss: 0.0954 | Val Acc: 0.9820\n",
      "Epoch [29/30] Train Loss: 0.0050 | Train Acc: 0.9996 | Val Loss: 0.0903 | Val Acc: 0.9820\n",
      "Epoch [30/30] Train Loss: 0.0038 | Train Acc: 0.9996 | Val Loss: 0.0834 | Val Acc: 0.9820\n",
      "Training complete!\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch [1/30] Train Loss: 0.4079 | Train Acc: 0.8026 | Val Loss: 0.3047 | Val Acc: 0.8649\n",
      "Epoch [2/30] Train Loss: 0.2446 | Train Acc: 0.9009 | Val Loss: 7.1729 | Val Acc: 0.7658\n",
      "Epoch [3/30] Train Loss: 0.1643 | Train Acc: 0.9455 | Val Loss: 3.1293 | Val Acc: 0.7658\n",
      "Epoch [4/30] Train Loss: 0.1133 | Train Acc: 0.9691 | Val Loss: 0.1191 | Val Acc: 0.9670\n",
      "Epoch [5/30] Train Loss: 0.0776 | Train Acc: 0.9779 | Val Loss: 0.0833 | Val Acc: 0.9685\n",
      "Epoch [6/30] Train Loss: 0.0646 | Train Acc: 0.9802 | Val Loss: 0.0792 | Val Acc: 0.9760\n",
      "Epoch [7/30] Train Loss: 0.0388 | Train Acc: 0.9901 | Val Loss: 0.0819 | Val Acc: 0.9730\n",
      "Epoch [8/30] Train Loss: 0.0360 | Train Acc: 0.9905 | Val Loss: 0.0711 | Val Acc: 0.9775\n",
      "Epoch [9/30] Train Loss: 0.0291 | Train Acc: 0.9912 | Val Loss: 0.0576 | Val Acc: 0.9805\n",
      "Epoch [10/30] Train Loss: 0.0153 | Train Acc: 0.9973 | Val Loss: 0.0692 | Val Acc: 0.9805\n",
      "Epoch [11/30] Train Loss: 0.0170 | Train Acc: 0.9943 | Val Loss: 0.0626 | Val Acc: 0.9865\n",
      "Epoch [12/30] Train Loss: 0.0106 | Train Acc: 0.9981 | Val Loss: 0.0599 | Val Acc: 0.9865\n",
      "Epoch [13/30] Train Loss: 0.0105 | Train Acc: 0.9981 | Val Loss: 0.0737 | Val Acc: 0.9790\n",
      "Epoch [14/30] Train Loss: 0.0064 | Train Acc: 1.0000 | Val Loss: 0.0671 | Val Acc: 0.9820\n",
      "Epoch [15/30] Train Loss: 0.0068 | Train Acc: 0.9992 | Val Loss: 0.0637 | Val Acc: 0.9865\n",
      "Epoch [16/30] Train Loss: 0.0073 | Train Acc: 0.9989 | Val Loss: 0.0676 | Val Acc: 0.9865\n",
      "Epoch [17/30] Train Loss: 0.0074 | Train Acc: 0.9992 | Val Loss: 0.0587 | Val Acc: 0.9850\n",
      "Epoch [18/30] Train Loss: 0.0090 | Train Acc: 0.9977 | Val Loss: 0.0656 | Val Acc: 0.9805\n",
      "Epoch [19/30] Train Loss: 0.0074 | Train Acc: 0.9985 | Val Loss: 0.0742 | Val Acc: 0.9790\n",
      "Epoch [20/30] Train Loss: 0.0056 | Train Acc: 0.9992 | Val Loss: 0.0771 | Val Acc: 0.9805\n",
      "Epoch [21/30] Train Loss: 0.0056 | Train Acc: 0.9992 | Val Loss: 0.0658 | Val Acc: 0.9775\n",
      "Epoch [22/30] Train Loss: 0.0051 | Train Acc: 0.9996 | Val Loss: 0.0602 | Val Acc: 0.9850\n",
      "Epoch [23/30] Train Loss: 0.0045 | Train Acc: 0.9996 | Val Loss: 0.0648 | Val Acc: 0.9820\n",
      "Epoch [24/30] Train Loss: 0.0045 | Train Acc: 0.9996 | Val Loss: 0.0633 | Val Acc: 0.9790\n",
      "Epoch [25/30] Train Loss: 0.0039 | Train Acc: 1.0000 | Val Loss: 0.0696 | Val Acc: 0.9790\n",
      "Epoch [26/30] Train Loss: 0.0038 | Train Acc: 0.9996 | Val Loss: 0.0679 | Val Acc: 0.9835\n",
      "Epoch [27/30] Train Loss: 0.0040 | Train Acc: 0.9996 | Val Loss: 0.0678 | Val Acc: 0.9820\n",
      "Epoch [28/30] Train Loss: 0.0054 | Train Acc: 0.9989 | Val Loss: 0.0733 | Val Acc: 0.9805\n",
      "Epoch [29/30] Train Loss: 0.0032 | Train Acc: 1.0000 | Val Loss: 0.0703 | Val Acc: 0.9835\n",
      "Epoch [30/30] Train Loss: 0.0033 | Train Acc: 0.9992 | Val Loss: 0.0699 | Val Acc: 0.9835\n",
      "Training complete!\n",
      "\n",
      "===== Running: fixed | individual | simple =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train Loss: 0.2371 | Train Accuracy: 0.8769\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8891412994093594, 'recall': 0.958843704066634, 'f1-score': 0.9226779820839227, 'support': 2041.0}, 'treatment': {'precision': 0.8185745140388769, 'recall': 0.608346709470305, 'f1-score': 0.6979742173112339, 'support': 623.0}, 'accuracy': 0.8768768768768769, 'macro avg': {'precision': 0.8538579067241181, 'recall': 0.7835952067684695, 'f1-score': 0.8103260996975783, 'support': 2664.0}, 'weighted avg': {'precision': 0.8726386315092803, 'recall': 0.8768768768768769, 'f1-score': 0.8701290160728923, 'support': 2664.0}}\n",
      "Test Loss: 0.2309 | Test Accuracy: 0.8829\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8872987477638641, 'recall': 0.9706457925636007, 'f1-score': 0.9271028037383178, 'support': 511.0}, 'treatment': {'precision': 0.8598130841121495, 'recall': 0.5935483870967742, 'f1-score': 0.7022900763358778, 'support': 155.0}, 'accuracy': 0.8828828828828829, 'macro avg': {'precision': 0.8735559159380069, 'recall': 0.7820970898301874, 'f1-score': 0.8146964400370977, 'support': 666.0}, 'weighted avg': {'precision': 0.880901934151228, 'recall': 0.8828828828828829, 'f1-score': 0.874781523336849, 'support': 666.0}}\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2338 | Train Accuracy: 0.8825\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8970588235294118, 'recall': 0.9563939245467908, 'f1-score': 0.9257766184491345, 'support': 2041.0}, 'treatment': {'precision': 0.8176229508196722, 'recall': 0.6404494382022472, 'f1-score': 0.7182718271827183, 'support': 623.0}, 'accuracy': 0.8825075075075075, 'macro avg': {'precision': 0.857340887174542, 'recall': 0.7984216813745191, 'f1-score': 0.8220242228159265, 'support': 2664.0}, 'weighted avg': {'precision': 0.8784820409850546, 'recall': 0.8825075075075075, 'f1-score': 0.8772497847558248, 'support': 2664.0}}\n",
      "Test Loss: 0.2456 | Test Accuracy: 0.8769\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8994413407821229, 'recall': 0.9452054794520548, 'f1-score': 0.9217557251908397, 'support': 511.0}, 'treatment': {'precision': 0.7829457364341085, 'recall': 0.6516129032258065, 'f1-score': 0.7112676056338029, 'support': 155.0}, 'accuracy': 0.8768768768768769, 'macro avg': {'precision': 0.8411935386081157, 'recall': 0.7984091913389306, 'f1-score': 0.8165116654123212, 'support': 666.0}, 'weighted avg': {'precision': 0.8723290004308584, 'recall': 0.8768768768768769, 'f1-score': 0.8727682499185564, 'support': 666.0}}\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train Loss: 0.2296 | Train Accuracy: 0.8904\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.9017447199265382, 'recall': 0.9618021547502449, 'f1-score': 0.9308056872037914, 'support': 2042.0}, 'treatment': {'precision': 0.8395061728395061, 'recall': 0.6559485530546624, 'f1-score': 0.7364620938628159, 'support': 622.0}, 'accuracy': 0.8903903903903904, 'macro avg': {'precision': 0.8706254463830221, 'recall': 0.8088753539024536, 'f1-score': 0.8336338905333036, 'support': 2664.0}, 'weighted avg': {'precision': 0.8872130471457071, 'recall': 0.8903903903903904, 'f1-score': 0.8854296680378428, 'support': 2664.0}}\n",
      "Test Loss: 0.2623 | Test Accuracy: 0.8694\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8810810810810811, 'recall': 0.9588235294117647, 'f1-score': 0.9183098591549296, 'support': 510.0}, 'treatment': {'precision': 0.8108108108108109, 'recall': 0.5769230769230769, 'f1-score': 0.6741573033707865, 'support': 156.0}, 'accuracy': 0.8693693693693694, 'macro avg': {'precision': 0.845945945945946, 'recall': 0.7678733031674208, 'f1-score': 0.796233581262858, 'support': 666.0}, 'weighted avg': {'precision': 0.8646213781348916, 'recall': 0.8693693693693694, 'f1-score': 0.8611209722144997, 'support': 666.0}}\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2345 | Train Accuracy: 0.8836\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8954337899543379, 'recall': 0.9603330068560235, 'f1-score': 0.9267485822306238, 'support': 2042.0}, 'treatment': {'precision': 0.8291139240506329, 'recall': 0.6318327974276527, 'f1-score': 0.7171532846715328, 'support': 622.0}, 'accuracy': 0.8836336336336337, 'macro avg': {'precision': 0.8622738570024854, 'recall': 0.7960829021418381, 'f1-score': 0.8219509334510784, 'support': 2664.0}, 'weighted avg': {'precision': 0.8799491966389833, 'recall': 0.8836336336336337, 'f1-score': 0.87781154203477, 'support': 2664.0}}\n",
      "Test Loss: 0.2429 | Test Accuracy: 0.8919\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.9025735294117647, 'recall': 0.9627450980392157, 'f1-score': 0.9316888045540797, 'support': 510.0}, 'treatment': {'precision': 0.8442622950819673, 'recall': 0.6602564102564102, 'f1-score': 0.7410071942446043, 'support': 156.0}, 'accuracy': 0.8918918918918919, 'macro avg': {'precision': 0.8734179122468659, 'recall': 0.811500754147813, 'f1-score': 0.8363479993993419, 'support': 666.0}, 'weighted avg': {'precision': 0.8889150420912717, 'recall': 0.8918918918918919, 'f1-score': 0.887024643580689, 'support': 666.0}}\n",
      "\n",
      "--- Fold 5 ---\n",
      "Train Loss: 0.2356 | Train Accuracy: 0.8859\n",
      "\n",
      "===== Train Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.8968036529680365, 'recall': 0.9618021547502449, 'f1-score': 0.9281663516068053, 'support': 2042.0}, 'treatment': {'precision': 0.8354430379746836, 'recall': 0.6366559485530546, 'f1-score': 0.7226277372262774, 'support': 622.0}, 'accuracy': 0.8858858858858859, 'macro avg': {'precision': 0.86612334547136, 'recall': 0.7992290516516498, 'f1-score': 0.8253970444165413, 'support': 2664.0}, 'weighted avg': {'precision': 0.8824769628306998, 'recall': 0.8858858858858859, 'f1-score': 0.8801764799308712, 'support': 2664.0}}\n",
      "Test Loss: 0.2397 | Test Accuracy: 0.8889\n",
      "\n",
      "===== Test Classification Report =====\n",
      "\n",
      "{'no treatment': {'precision': 0.9052044609665427, 'recall': 0.9549019607843138, 'f1-score': 0.9293893129770993, 'support': 510.0}, 'treatment': {'precision': 0.8203125, 'recall': 0.6730769230769231, 'f1-score': 0.7394366197183099, 'support': 156.0}, 'accuracy': 0.8888888888888888, 'macro avg': {'precision': 0.8627584804832713, 'recall': 0.8139894419306184, 'f1-score': 0.8344129663477046, 'support': 666.0}, 'weighted avg': {'precision': 0.8853198574969021, 'recall': 0.8888888888888888, 'f1-score': 0.8848958893308965, 'support': 666.0}}\n",
      "\n",
      "âœ… All runs complete! You can now proceed to compute summaries and generate HTML reports.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"data.csv\")\n",
    "\n",
    "xp_features = [\n",
    "    'age',\n",
    "    'location', 'eol_preference',\n",
    "    'family_preference', 'biological_sex', 'gender_identity', 'political_leaning',\n",
    "    'marital_status', 'religion', 'religious_importance', 'annual_income',\n",
    "    'education', 'family_history_dementia', 'personal_history_dementia',\n",
    "    'dementia_worry'\n",
    "]\n",
    "\n",
    "# Xt Modular Components\n",
    "xt_parts = {\n",
    "    'medical': ['crisis_type'],\n",
    "    'patient_condition': ['crisis_chance','emotional_state', 'agitation_frequency', 'agitation_severity',\n",
    "    'family_visit_frequency', 'family_inconvenience', 'interaction_ability',\n",
    "    'functional_ability', 'behavior', 'affordability'],\n",
    "    'treatment': ['crisis_wean', 'crisis_tube'],\n",
    "    'prognosis': ['crisis_comfort', 'resuscitation_chance', 'leave_hospital', 'internal_damage', 'future_arrest']\n",
    "}\n",
    "\n",
    "\n",
    "columns_to_drop = list(set(df.columns) - set(xp_features) - set(sum(xt_parts.values(), [])) - {\"choice\"})\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Run the full experiment\n",
    "report = run_full_experiment(df, xp_features, xt_parts, num_folds=5, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af885227",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4136b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_report_to_json(report_collector, filename=\"final_report2.json\", folder=\"reports\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Convert any numpy data types to Python types to make them JSON serializable\n",
    "    def convert(o):\n",
    "        if isinstance(o, (np.integer, np.int64)): return int(o)\n",
    "        if isinstance(o, (np.floating, np.float64)): return float(o)\n",
    "        if isinstance(o, (np.ndarray,)): return o.tolist()\n",
    "        return o\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(report_collector, f, indent=4, default=convert)\n",
    "\n",
    "    print(f\"âœ… Saved final report to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be75d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved final report to reports\\final_report2.json\n"
     ]
    }
   ],
   "source": [
    "save_report_to_json(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4f493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def prepare_final_json(report_collector):\n",
    "    final_json = {}\n",
    "\n",
    "    for cpr_setting, cpr_data in report_collector.items():\n",
    "        final_json[cpr_setting] = {}\n",
    "\n",
    "        for scenario, scenario_data in cpr_data.items():\n",
    "            final_json[cpr_setting][scenario] = {}\n",
    "\n",
    "            for model_type, model_data in scenario_data.items():\n",
    "                folds_data = model_data[\"folds\"]\n",
    "\n",
    "                # Collect all histories across folds\n",
    "                train_losses_all = []\n",
    "                train_accs_all = []\n",
    "                val_losses_all = []\n",
    "                val_accs_all = []\n",
    "                test_accuracies = []\n",
    "\n",
    "                for fold_num, fold_metrics in folds_data.items():\n",
    "                    train_losses_all.append(fold_metrics[\"train_loss_history\"])\n",
    "                    train_accs_all.append(fold_metrics[\"train_acc_history\"])\n",
    "                    val_losses_all.append(fold_metrics[\"val_loss_history\"])\n",
    "                    val_accs_all.append(fold_metrics[\"val_acc_history\"])\n",
    "                    test_accuracies.append(fold_metrics[\"accuracy\"])\n",
    "\n",
    "                # Make sure all histories are same length\n",
    "                min_train_len = min(len(seq) for seq in train_losses_all)\n",
    "                min_val_len = min(len(seq) for seq in val_losses_all) if val_losses_all and all(val_losses_all) else 0\n",
    "\n",
    "                # Trim histories to the shortest run if necessary\n",
    "                train_losses_all = [seq[:min_train_len] for seq in train_losses_all]\n",
    "                train_accs_all = [seq[:min_train_len] for seq in train_accs_all]\n",
    "                val_losses_all = [seq[:min_val_len] for seq in val_losses_all] if min_val_len > 0 else []\n",
    "                val_accs_all = [seq[:min_val_len] for seq in val_accs_all] if min_val_len > 0 else []\n",
    "\n",
    "                # Calculate summaries\n",
    "                summary = {}\n",
    "\n",
    "                if test_accuracies:\n",
    "                    summary[\"avg_accuracy\"] = float(np.mean(test_accuracies))\n",
    "                    summary[\"std_accuracy\"] = float(np.std(test_accuracies))\n",
    "\n",
    "                else:\n",
    "                    summary[\"avg_accuracy\"] = None\n",
    "                    summary[\"std_accuracy\"] = None\n",
    "\n",
    "                # Training Loss\n",
    "                if train_losses_all:\n",
    "                    summary[\"train_loss_history\"] = [float(x) for x in np.mean(train_losses_all, axis=0)]\n",
    "                    summary[\"train_loss_std\"] = [float(x) for x in np.std(train_losses_all, axis=0)]\n",
    "\n",
    "                else:\n",
    "                    summary[\"train_loss_history\"] = []\n",
    "                    summary[\"train_loss_std\"] = []\n",
    "\n",
    "                # Training Accuracy\n",
    "                if train_accs_all:\n",
    "                    summary[\"train_acc_history\"] = [float(x) for x in np.mean(train_accs_all, axis=0)]\n",
    "                    summary[\"train_acc_std\"] = [float(x) for x in np.std(train_accs_all, axis=0)]\n",
    "                else:\n",
    "                    summary[\"train_acc_history\"] = []\n",
    "                    summary[\"train_acc_std\"] = []\n",
    "\n",
    "                # Validation Loss\n",
    "                if val_losses_all:\n",
    "                    summary[\"val_loss_history\"] = [float(x) for x in np.mean(val_losses_all, axis=0)]\n",
    "                    summary[\"val_loss_std\"] = [float(x) for x in np.std(val_losses_all, axis=0)]\n",
    "                else:\n",
    "                    summary[\"val_loss_history\"] = []\n",
    "                    summary[\"val_loss_std\"] = []\n",
    "\n",
    "                # Validation Accuracy\n",
    "                if val_accs_all:\n",
    "                    summary[\"val_acc_history\"] = [float(x) for x in np.mean(val_accs_all, axis=0)]\n",
    "                    summary[\"val_acc_std\"] = [float(x) for x in np.std(val_accs_all, axis=0)]\n",
    "                else:\n",
    "                    summary[\"val_acc_history\"] = []\n",
    "                    summary[\"val_acc_std\"] = []\n",
    "\n",
    "                # Store in final JSON structure\n",
    "                final_json[cpr_setting][scenario][model_type] = {\n",
    "                    \"summary\": summary,\n",
    "                    \"folds\": folds_data  # Direct copy, already in correct format\n",
    "                }\n",
    "\n",
    "    return final_json\n",
    "\n",
    "def save_json(data, filename=\"final_report2.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"\\nâœ… JSON successfully saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dee6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_json(obj):\n",
    "    \"\"\"\n",
    "    Recursively converts NumPy types to native Python types for JSON serialization.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_json(item) for item in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccee6f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… JSON successfully saved to final_report2.json\n"
     ]
    }
   ],
   "source": [
    "final_json = prepare_final_json(report)\n",
    "\n",
    "# After building your final_json\n",
    "final_json_clean = clean_json(final_json)\n",
    "\n",
    "# Save to file\n",
    "save_json(final_json_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2b0f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semi_synthetic': {'agnostic': {'nn': {'summary': {'avg_accuracy': 0.7663663663663662,\n",
       "     'std_accuracy': 0.0007355825053403044,\n",
       "     'train_loss_history': [0.5931899860137847,\n",
       "      0.5590149851833901,\n",
       "      0.5533666837506178,\n",
       "      0.5538954002101246,\n",
       "      0.5511747245381519,\n",
       "      0.5534228348150487,\n",
       "      0.5513192086684995,\n",
       "      0.5496226483728828,\n",
       "      0.5501733076281663,\n",
       "      0.5481136281315873,\n",
       "      0.5482521234489068,\n",
       "      0.5472193797913993,\n",
       "      0.547727445131395,\n",
       "      0.5477795506395944,\n",
       "      0.5466178885320337,\n",
       "      0.5456303715705871,\n",
       "      0.5471868721450248,\n",
       "      0.5484233846024769,\n",
       "      0.5463837052263865,\n",
       "      0.5460514376803143,\n",
       "      0.5453607912470655,\n",
       "      0.545860188617939,\n",
       "      0.545261319672189,\n",
       "      0.5446322548680189,\n",
       "      0.5461316347122193,\n",
       "      0.5465474307537079,\n",
       "      0.5467766894073021,\n",
       "      0.5461723881523783,\n",
       "      0.5449032857650664,\n",
       "      0.5455020653038491],\n",
       "     'train_loss_std': [0.017847068935598367,\n",
       "      0.00355217750899995,\n",
       "      0.0027862833322476175,\n",
       "      0.0013409794433956327,\n",
       "      0.0031983802471031806,\n",
       "      0.001956995465509137,\n",
       "      0.002488789162121401,\n",
       "      0.004833331759488012,\n",
       "      0.0009231785150777855,\n",
       "      0.0034874125654159984,\n",
       "      0.0007503490764943753,\n",
       "      0.0036131090635439637,\n",
       "      0.001365677643608241,\n",
       "      0.0017074147286503774,\n",
       "      0.0012305959776872679,\n",
       "      0.0006054165710825504,\n",
       "      0.0021026705470781042,\n",
       "      0.0019784421056228416,\n",
       "      0.0021383499442580857,\n",
       "      0.0009589836361490755,\n",
       "      0.001731321427611057,\n",
       "      0.0015950736332364156,\n",
       "      0.0011428717491520995,\n",
       "      0.0013039041252987844,\n",
       "      0.0015690531317476488,\n",
       "      0.0005338041277196384,\n",
       "      0.0015993367474066984,\n",
       "      0.0012708107001548389,\n",
       "      0.0020619882007233514,\n",
       "      0.0011841122304962256],\n",
       "     'train_acc_history': [0.7212652439024391,\n",
       "      0.7654725609756097,\n",
       "      0.7666158536585366,\n",
       "      0.7660823170731708,\n",
       "      0.766844512195122,\n",
       "      0.765701219512195,\n",
       "      0.7655487804878048,\n",
       "      0.766234756097561,\n",
       "      0.766310975609756,\n",
       "      0.7664634146341465,\n",
       "      0.7659298780487805,\n",
       "      0.7661585365853658,\n",
       "      0.7662347560975611,\n",
       "      0.7659298780487804,\n",
       "      0.7661585365853659,\n",
       "      0.7666158536585367,\n",
       "      0.7657774390243903,\n",
       "      0.7648628048780488,\n",
       "      0.7666158536585366,\n",
       "      0.766844512195122,\n",
       "      0.7663871951219512,\n",
       "      0.7663109756097561,\n",
       "      0.7669969512195123,\n",
       "      0.7666158536585366,\n",
       "      0.7663871951219512,\n",
       "      0.7659298780487805,\n",
       "      0.7660060975609756,\n",
       "      0.7660060975609756,\n",
       "      0.7672256097560975,\n",
       "      0.7663109756097561],\n",
       "     'train_acc_std': [0.028506097560975604,\n",
       "      0.000982019720024746,\n",
       "      0.0007847279070874218,\n",
       "      0.0003733978266437639,\n",
       "      0.0011149953382871762,\n",
       "      0.00044443230905831505,\n",
       "      0.000882304641980931,\n",
       "      0.0017313744956708337,\n",
       "      0.0009760860118037811,\n",
       "      0.0011659343399983435,\n",
       "      0.0007389755880207556,\n",
       "      0.0013761791223527403,\n",
       "      0.0007847279070873959,\n",
       "      0.0008487445675045463,\n",
       "      0.0003886447799994363,\n",
       "      0.0007467956532875822,\n",
       "      0.0007068306780103182,\n",
       "      0.0006376981909558675,\n",
       "      0.0014177648809251754,\n",
       "      0.0008823046419809674,\n",
       "      0.0009334945666094369,\n",
       "      0.0006556650355977791,\n",
       "      0.0005169458828601851,\n",
       "      0.0008555619024635668,\n",
       "      0.0007621951219512257,\n",
       "      0.00015243902439023848,\n",
       "      0.0009641090427342276,\n",
       "      0.0005389533393189958,\n",
       "      0.0007772895599989139,\n",
       "      0.0005600967399656458],\n",
       "     'val_loss_history': [0.5750421171838587,\n",
       "      11.153357651558789,\n",
       "      56.93476483686404,\n",
       "      55.837559292533186,\n",
       "      49.7941578485749,\n",
       "      72.84737648096952,\n",
       "      47.18832308162343,\n",
       "      21.539888094230133,\n",
       "      9.377929058941927,\n",
       "      28.233225434476672,\n",
       "      16.472757590900766,\n",
       "      14.859808985753494,\n",
       "      3.068155537410216,\n",
       "      3.5902488415891476,\n",
       "      9.550155986439098,\n",
       "      9.005066673322158,\n",
       "      11.077711031653665,\n",
       "      4.874557234482332,\n",
       "      8.57580277540467,\n",
       "      6.955603238669309,\n",
       "      3.9132573328234934,\n",
       "      2.675095399401404,\n",
       "      4.369599927555431,\n",
       "      1.0872766169634733,\n",
       "      3.6663333215496756,\n",
       "      5.83432329676368,\n",
       "      3.659078194878318,\n",
       "      5.8236096867106175,\n",
       "      2.990580326589671,\n",
       "      2.3925957983190362],\n",
       "     'val_loss_std': [0.028829985453066354,\n",
       "      9.15822535888532,\n",
       "      75.67776553273559,\n",
       "      23.481321990788945,\n",
       "      41.787051138598905,\n",
       "      65.89227876114442,\n",
       "      39.1516221824347,\n",
       "      32.12136774781682,\n",
       "      6.516626853301631,\n",
       "      23.51539048511426,\n",
       "      8.18737626409111,\n",
       "      9.915328951454798,\n",
       "      2.199799357467328,\n",
       "      3.0784159237849384,\n",
       "      6.458833550203309,\n",
       "      8.204584411633562,\n",
       "      5.364129672281487,\n",
       "      2.7237979572176583,\n",
       "      8.304889809009817,\n",
       "      2.8574077710628476,\n",
       "      2.536456974066935,\n",
       "      2.6418366296471856,\n",
       "      4.296925246628432,\n",
       "      0.46799362617451346,\n",
       "      3.583510105093025,\n",
       "      5.541992822923773,\n",
       "      2.394408787382783,\n",
       "      4.848911926480471,\n",
       "      3.1315709766203037,\n",
       "      3.4704168538502773],\n",
       "     'val_acc_history': [0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.553153153153153,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.553153153153153,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.6594594594594595,\n",
       "      0.66006006006006,\n",
       "      0.6594594594594595,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662],\n",
       "     'val_acc_std': [0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.2610102000357858,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.2610102000357858,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2133641558503688,\n",
       "      0.2129139717970933,\n",
       "      0.2133641558503688,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044]},\n",
       "    'folds': {'1': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5852900837979665,\n",
       "       0.5637752079382176,\n",
       "       0.5543525240770201,\n",
       "       0.5548306427350859,\n",
       "       0.5537899296458174,\n",
       "       0.5547721407762388,\n",
       "       0.5553872032863337,\n",
       "       0.5467951072425377,\n",
       "       0.551554245919716,\n",
       "       0.5469763533371251,\n",
       "       0.5487898428265642,\n",
       "       0.5441696869163979,\n",
       "       0.5478123550007983,\n",
       "       0.5470695619176074,\n",
       "       0.5477133117071012,\n",
       "       0.5451961246932425,\n",
       "       0.5476580996338914,\n",
       "       0.5483470966176289,\n",
       "       0.5498220302709719,\n",
       "       0.5473065070989656,\n",
       "       0.5487682034329671,\n",
       "       0.5456805221918153,\n",
       "       0.5439801027135152,\n",
       "       0.5455527254721013,\n",
       "       0.5439556081120561,\n",
       "       0.5456441597240728,\n",
       "       0.5497361835910053,\n",
       "       0.5455799567990187,\n",
       "       0.5454864276618492,\n",
       "       0.5465276357604236],\n",
       "      'train_acc_history': [0.7419969512195121,\n",
       "       0.7648628048780488,\n",
       "       0.7652439024390244,\n",
       "       0.7660060975609756,\n",
       "       0.7652439024390244,\n",
       "       0.7652439024390244,\n",
       "       0.7644817073170732,\n",
       "       0.7660060975609756,\n",
       "       0.765625,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7675304878048781,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.765625,\n",
       "       0.7660060975609756,\n",
       "       0.7652439024390244,\n",
       "       0.7637195121951219,\n",
       "       0.7648628048780488,\n",
       "       0.7667682926829268,\n",
       "       0.7648628048780488,\n",
       "       0.765625,\n",
       "       0.7675304878048781,\n",
       "       0.7663871951219512,\n",
       "       0.7675304878048781,\n",
       "       0.7660060975609756,\n",
       "       0.7652439024390244,\n",
       "       0.7667682926829268,\n",
       "       0.7663871951219512,\n",
       "       0.765625],\n",
       "      'val_loss_history': [0.6096809696067463,\n",
       "       4.124924746426669,\n",
       "       203.05091164328834,\n",
       "       96.12015394731002,\n",
       "       104.67976518110795,\n",
       "       129.14513327858666,\n",
       "       116.40910616787997,\n",
       "       7.7944443009116435,\n",
       "       3.883171569217335,\n",
       "       64.37756781144576,\n",
       "       27.008077187971637,\n",
       "       20.630355748263273,\n",
       "       0.9539010416377675,\n",
       "       3.0800086584958164,\n",
       "       14.193286462263627,\n",
       "       19.555414763363924,\n",
       "       15.389626502990723,\n",
       "       8.063531788912686,\n",
       "       10.384026050567627,\n",
       "       9.544142723083496,\n",
       "       2.5874659689989956,\n",
       "       1.6442612572149797,\n",
       "       3.1087884577837857,\n",
       "       0.7741443081335588,\n",
       "       0.6821232546459545,\n",
       "       3.139899730682373,\n",
       "       1.2942220297726719,\n",
       "       3.215886582027782,\n",
       "       0.6336403814229098,\n",
       "       9.327461676164107],\n",
       "      'val_acc_history': [0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7659409317062619,\n",
       "         'recall': 0.9983248747802435,\n",
       "         'f1-score': 0.8668284358551566,\n",
       "         'support': 60294.0},\n",
       "        '1': {'precision': 0.24060150375939848,\n",
       "         'recall': 0.0017366764354716162,\n",
       "         'f1-score': 0.003448461662805108,\n",
       "         'support': 18426.0},\n",
       "        'accuracy': 0.7650533536585366,\n",
       "        'macro avg': {'precision': 0.5032712177328302,\n",
       "         'recall': 0.5000307756078576,\n",
       "         'f1-score': 0.43513844875898083,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6429746677409557,\n",
       "         'recall': 0.7650533536585366,\n",
       "         'f1-score': 0.6647369800057121,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0,\n",
       "         'loss': 40.096256256103516},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.6214919315605629,\n",
       "       0.5573559098127412,\n",
       "       0.554483356272302,\n",
       "       0.556009425622661,\n",
       "       0.5549393819599617,\n",
       "       0.5535926360909532,\n",
       "       0.5481100678443909,\n",
       "       0.5505213584841752,\n",
       "       0.5489699564329008,\n",
       "       0.5523539127373114,\n",
       "       0.5471842012754301,\n",
       "       0.5488320632678706,\n",
       "       0.5487301524092512,\n",
       "       0.5499119126215214,\n",
       "       0.5457544086909876,\n",
       "       0.5466222603146623,\n",
       "       0.5472152901858818,\n",
       "       0.5467545077568148,\n",
       "       0.5438725817494277,\n",
       "       0.5463870923693587,\n",
       "       0.5443888124896259,\n",
       "       0.5434007535620433,\n",
       "       0.5472611704977547,\n",
       "       0.5455145763187874,\n",
       "       0.5485194994182121,\n",
       "       0.5471463697712596,\n",
       "       0.5471637227186342,\n",
       "       0.5459088843043257,\n",
       "       0.5457869615496659,\n",
       "       0.5470206606678847],\n",
       "      'train_acc_history': [0.6821646341463414,\n",
       "       0.7641006097560976,\n",
       "       0.7663871951219512,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.765625,\n",
       "       0.7648628048780488,\n",
       "       0.7652439024390244,\n",
       "       0.7648628048780488,\n",
       "       0.7671493902439024,\n",
       "       0.7671493902439024,\n",
       "       0.7667682926829268,\n",
       "       0.7648628048780488,\n",
       "       0.7644817073170732,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7652439024390244,\n",
       "       0.7682926829268293,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.7675304878048781,\n",
       "       0.7663871951219512,\n",
       "       0.7660060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.765625,\n",
       "       0.7648628048780488,\n",
       "       0.7660060975609756,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756],\n",
       "      'val_loss_history': [0.6089038577946749,\n",
       "       0.5832256105813113,\n",
       "       13.700236667286266,\n",
       "       28.15910243988037,\n",
       "       27.706299435008656,\n",
       "       18.12421564622359,\n",
       "       56.75862138921564,\n",
       "       85.12152377041903,\n",
       "       1.1608100479299372,\n",
       "       6.660794995047829,\n",
       "       10.188840129158713,\n",
       "       0.8856627886945551,\n",
       "       6.531055797230113,\n",
       "       1.4356301752003757,\n",
       "       7.465853452682495,\n",
       "       1.3716192137111316,\n",
       "       3.6340500007976186,\n",
       "       7.764819015156139,\n",
       "       4.295555569908836,\n",
       "       11.18391431461681,\n",
       "       5.007988214492798,\n",
       "       7.890773383053866,\n",
       "       0.9993743083693765,\n",
       "       1.2099447575482456,\n",
       "       1.437052851373499,\n",
       "       0.8038832274350253,\n",
       "       0.6397707679054954,\n",
       "       2.428125251423229,\n",
       "       1.2646190524101257,\n",
       "       0.5349809229373932],\n",
       "      'val_acc_history': [0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7659911376725536,\n",
       "         'recall': 0.994759535655058,\n",
       "         'f1-score': 0.8655137833762598,\n",
       "         'support': 60300.0},\n",
       "        '1': {'precision': 0.23114355231143552,\n",
       "         'recall': 0.00515743756786102,\n",
       "         'f1-score': 0.01008974563220222,\n",
       "         'support': 18420.0},\n",
       "        'accuracy': 0.7631986788617886,\n",
       "        'macro avg': {'precision': 0.4985673449919945,\n",
       "         'recall': 0.49995848661145953,\n",
       "         'f1-score': 0.43780176450423103,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6408400639638164,\n",
       "         'recall': 0.7631986788617886,\n",
       "         'f1-score': 0.6653497745443805,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.24939411878585815},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0,\n",
       "         'loss': 1.5108277797698975},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5694638875926413,\n",
       "       0.5542917665911884,\n",
       "       0.5478292107582092,\n",
       "       0.5534089164036077,\n",
       "       0.5522984099097368,\n",
       "       0.5496608258747473,\n",
       "       0.5517803866688799,\n",
       "       0.5467534624948734,\n",
       "       0.5508839517104916,\n",
       "       0.5508663203658127,\n",
       "       0.5475848644244962,\n",
       "       0.5534318445659265,\n",
       "       0.5452164040832985,\n",
       "       0.549690735049364,\n",
       "       0.5474591095273088,\n",
       "       0.545118741145948,\n",
       "       0.5464009104705438,\n",
       "       0.548570512271509,\n",
       "       0.5456833417822675,\n",
       "       0.5457752107120142,\n",
       "       0.5440002665287111,\n",
       "       0.5453516971774217,\n",
       "       0.5446693054059657,\n",
       "       0.5428462704507316,\n",
       "       0.5455569217844707,\n",
       "       0.5470264146967632,\n",
       "       0.5459561762286396,\n",
       "       0.5480337586344742,\n",
       "       0.5457680487051243,\n",
       "       0.54522751308069],\n",
       "      'train_acc_history': [0.7560975609756098,\n",
       "       0.7667682926829268,\n",
       "       0.7675304878048781,\n",
       "       0.7660060975609756,\n",
       "       0.7686737804878049,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7686737804878049,\n",
       "       0.7675304878048781,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7637195121951219,\n",
       "       0.7667682926829268,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7675304878048781,\n",
       "       0.765625,\n",
       "       0.7648628048780488,\n",
       "       0.7660060975609756,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7675304878048781,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7660060975609756,\n",
       "       0.7671493902439024,\n",
       "       0.765625,\n",
       "       0.7667682926829268,\n",
       "       0.7667682926829268],\n",
       "      'val_loss_history': [0.5421131578358737,\n",
       "       20.283267107876863,\n",
       "       1.1938897737047889,\n",
       "       44.93402355367487,\n",
       "       94.56058692932129,\n",
       "       38.738309383392334,\n",
       "       42.63563498583707,\n",
       "       13.063642198389227,\n",
       "       16.917026714845136,\n",
       "       7.05732507055456,\n",
       "       11.164256529374557,\n",
       "       5.6599321798844775,\n",
       "       4.457330172712153,\n",
       "       3.1915337172421543,\n",
       "       4.8805148005485535,\n",
       "       3.997199519114061,\n",
       "       10.46642192927274,\n",
       "       4.802145784551447,\n",
       "       2.092863779176365,\n",
       "       5.339989266612313,\n",
       "       3.321254329247908,\n",
       "       1.7847706133669072,\n",
       "       4.428643996065313,\n",
       "       1.9122614860534668,\n",
       "       6.124968702142889,\n",
       "       10.17007542740215,\n",
       "       6.438582680442116,\n",
       "       1.197105119174177,\n",
       "       0.602588262070309,\n",
       "       0.5316476117480885],\n",
       "      'val_acc_history': [0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7664891385863003,\n",
       "         'recall': 0.9993702353331123,\n",
       "         'f1-score': 0.8675735362879731,\n",
       "         'support': 60340.0},\n",
       "        '1': {'precision': 0.19148936170212766,\n",
       "         'recall': 0.0004896626768226333,\n",
       "         'f1-score': 0.0009768274814131437,\n",
       "         'support': 18380.0},\n",
       "        'accuracy': 0.7661458333333333,\n",
       "        'macro avg': {'precision': 0.478989250144214,\n",
       "         'recall': 0.49992994900496746,\n",
       "         'f1-score': 0.4342751818846931,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6322348715749805,\n",
       "         'recall': 0.7661458333333333,\n",
       "         'f1-score': 0.6652355344096121,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.26218220591545105},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0,\n",
       "         'loss': 1.4669440984725952},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.6038166204603707,\n",
       "       0.5624767963479205,\n",
       "       0.5552163618366893,\n",
       "       0.55286575163283,\n",
       "       0.5480237639531856,\n",
       "       0.5551062829610778,\n",
       "       0.5494264589577187,\n",
       "       0.558664702060746,\n",
       "       0.5496710720585614,\n",
       "       0.5480853362781245,\n",
       "       0.5485166936385922,\n",
       "       0.5461305372598695,\n",
       "       0.5477340504890535,\n",
       "       0.5457150630834626,\n",
       "       0.5475548716579995,\n",
       "       0.5460531282715682,\n",
       "       0.5505876773741187,\n",
       "       0.5464401659442157,\n",
       "       0.547722636926465,\n",
       "       0.5444016841853537,\n",
       "       0.5448647315909223,\n",
       "       0.5482789597860197,\n",
       "       0.5446825674394282,\n",
       "       0.5432664286799547,\n",
       "       0.5471722912497636,\n",
       "       0.5464079968812989,\n",
       "       0.5456068850145107,\n",
       "       0.5470269177018142,\n",
       "       0.5466247164621586,\n",
       "       0.5450749767989647],\n",
       "      'train_acc_history': [0.6935975609756098,\n",
       "       0.7652439024390244,\n",
       "       0.7667682926829268,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7637195121951219,\n",
       "       0.7671493902439024,\n",
       "       0.7648628048780488,\n",
       "       0.7648628048780488,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7667682926829268,\n",
       "       0.7675304878048781,\n",
       "       0.7648628048780488,\n",
       "       0.765625,\n",
       "       0.765625,\n",
       "       0.7682926829268293,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.7682926829268293,\n",
       "       0.7652439024390244,\n",
       "       0.7660060975609756,\n",
       "       0.7671493902439024,\n",
       "       0.7652439024390244,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756],\n",
       "      'val_loss_history': [0.5504597642204978,\n",
       "       7.05028206651861,\n",
       "       57.84191530401056,\n",
       "       43.85745837471702,\n",
       "       0.5633654431863264,\n",
       "       172.33788368918678,\n",
       "       8.529047185724432,\n",
       "       1.1602475047111511,\n",
       "       8.06379974972118,\n",
       "       15.366428418592973,\n",
       "       8.17107035896995,\n",
       "       19.89313168959184,\n",
       "       2.7131978381763804,\n",
       "       9.45959158377214,\n",
       "       19.511918804862283,\n",
       "       1.6919657858935269,\n",
       "       18.53894671526822,\n",
       "       1.3583150587298654,\n",
       "       24.033757990056817,\n",
       "       4.122492898594249,\n",
       "       0.5483670153401115,\n",
       "       0.574524773792787,\n",
       "       12.52563563260165,\n",
       "       0.5463171818039634,\n",
       "       9.543373151258988,\n",
       "       14.473711360584606,\n",
       "       6.152926835146817,\n",
       "       14.40421724319458,\n",
       "       3.593457915566184,\n",
       "       0.6422510824420236],\n",
       "      'val_acc_history': [0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.766181405259196,\n",
       "         'recall': 0.9955734606585099,\n",
       "         'f1-score': 0.8659432567864739,\n",
       "         'support': 60318.0},\n",
       "        '1': {'precision': 0.22157434402332363,\n",
       "         'recall': 0.0041299858711009675,\n",
       "         'f1-score': 0.008108829021072286,\n",
       "         'support': 18402.0},\n",
       "        'accuracy': 0.7638084349593496,\n",
       "        'macro avg': {'precision': 0.4938778746412598,\n",
       "         'recall': 0.4998517232648054,\n",
       "         'f1-score': 0.43702604290377306,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6388712027584017,\n",
       "         'recall': 0.7638084349593496,\n",
       "         'f1-score': 0.6654113825519856,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.08206772059202194},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0,\n",
       "         'loss': 2.540964365005493},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5858874066573817,\n",
       "       0.5571752452268833,\n",
       "       0.5549519658088684,\n",
       "       0.552362264656439,\n",
       "       0.5468221372220574,\n",
       "       0.5539822883722259,\n",
       "       0.5518919265851742,\n",
       "       0.5453786115820815,\n",
       "       0.5497873120191621,\n",
       "       0.5422862179395629,\n",
       "       0.5491850150794517,\n",
       "       0.5435327669469322,\n",
       "       0.5491442636745733,\n",
       "       0.546510480526017,\n",
       "       0.5446077410767718,\n",
       "       0.5451616034275149,\n",
       "       0.544072383060688,\n",
       "       0.5520046404222163,\n",
       "       0.5448179354028004,\n",
       "       0.546386694035879,\n",
       "       0.544781942193101,\n",
       "       0.5465890103723945,\n",
       "       0.5457134523042818,\n",
       "       0.5459812734185195,\n",
       "       0.5454538529965935,\n",
       "       0.546512212695145,\n",
       "       0.5454204794837207,\n",
       "       0.5443124233222589,\n",
       "       0.540850274446534,\n",
       "       0.5436595402112822],\n",
       "      'train_acc_history': [0.7324695121951219,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7667682926829268,\n",
       "       0.7671493902439024,\n",
       "       0.7652439024390244,\n",
       "       0.7652439024390244,\n",
       "       0.7675304878048781,\n",
       "       0.7663871951219512,\n",
       "       0.7682926829268293,\n",
       "       0.765625,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.7660060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.7648628048780488,\n",
       "       0.7682926829268293,\n",
       "       0.7663871951219512,\n",
       "       0.7675304878048781,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7663871951219512,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.7686737804878049,\n",
       "       0.7671493902439024],\n",
       "      'val_loss_history': [0.5640528364615007,\n",
       "       23.725088726390492,\n",
       "       8.886870796030218,\n",
       "       66.11705814708363,\n",
       "       21.460772254250266,\n",
       "       5.891340407458219,\n",
       "       11.609205679460006,\n",
       "       0.5595826967196031,\n",
       "       16.86483721299605,\n",
       "       47.704010876742274,\n",
       "       25.831543749028985,\n",
       "       27.22996252233332,\n",
       "       0.6852928372946653,\n",
       "       0.7844800732352517,\n",
       "       1.6992064118385315,\n",
       "       18.409134084528144,\n",
       "       7.3595100099390205,\n",
       "       2.3839745250615207,\n",
       "       2.072810487313704,\n",
       "       4.587476990439675,\n",
       "       8.101211136037653,\n",
       "       1.481146969578483,\n",
       "       0.7855572429570284,\n",
       "       0.9937153512781317,\n",
       "       0.5441486483270471,\n",
       "       0.5840467377142473,\n",
       "       3.7698886611244897,\n",
       "       7.87271423773332,\n",
       "       8.858596021478826,\n",
       "       0.9266376983035695],\n",
       "      'val_acc_history': [0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7665275055367462,\n",
       "         'recall': 0.9979782914905957,\n",
       "         'f1-score': 0.8670731619526172,\n",
       "         'support': 60345.0},\n",
       "        '1': {'precision': 0.2077922077922078,\n",
       "         'recall': 0.0017414965986394559,\n",
       "         'f1-score': 0.003454045010524043,\n",
       "         'support': 18375.0},\n",
       "        'accuracy': 0.7654344512195121,\n",
       "        'macro avg': {'precision': 0.48715985666447703,\n",
       "         'recall': 0.49985989404461756,\n",
       "         'f1-score': 0.4352636034815706,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6361062517758735,\n",
       "         'recall': 0.7654344512195121,\n",
       "         'f1-score': 0.6654852392670104,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.020621653646230698},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0,\n",
       "         'loss': 3.8917040824890137},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}}}},\n",
       "   'simple': {'summary': {'avg_accuracy': 0.7663663663663662,\n",
       "     'std_accuracy': 0.0007355825053403044,\n",
       "     'train_loss_history': [0.5436296673986601],\n",
       "     'train_loss_std': [0.00021841220568158848],\n",
       "     'train_acc_history': [0.7663663663663663],\n",
       "     'train_acc_std': [0.00018389562633506248],\n",
       "     'val_loss_history': [],\n",
       "     'val_loss_std': [],\n",
       "     'val_acc_history': [],\n",
       "     'val_acc_std': []},\n",
       "    'folds': {'1': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5438971666274179],\n",
       "      'train_acc_history': [0.7661411411411412],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7661411411411412,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8675876726886291,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.7661411411411412,\n",
       "        'macro avg': {'precision': 0.3830705705705706,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43379383634431457,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.58697224814905,\n",
       "         'recall': 0.7661411411411412,\n",
       "         'f1-score': 0.6646946095936532,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.543897166627418],\n",
       "      'train_acc_history': [0.7661411411411412],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7661411411411412,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8675876726886291,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.7661411411411412,\n",
       "        'macro avg': {'precision': 0.3830705705705706,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43379383634431457,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.58697224814905,\n",
       "         'recall': 0.7661411411411412,\n",
       "         'f1-score': 0.6646946095936532,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5434513345794882],\n",
       "      'train_acc_history': [0.7665165165165165],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7665165165165165,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8678283042923927,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.7665165165165165,\n",
       "        'macro avg': {'precision': 0.38325825825825827,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43391415214619633,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.5875475700926152,\n",
       "         'recall': 0.7665165165165165,\n",
       "         'f1-score': 0.6652047287406403,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5434513345794882],\n",
       "      'train_acc_history': [0.7665165165165165],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7665165165165165,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8678283042923927,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.7665165165165165,\n",
       "        'macro avg': {'precision': 0.38325825825825827,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43391415214619633,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.5875475700926152,\n",
       "         'recall': 0.7665165165165165,\n",
       "         'f1-score': 0.6652047287406403,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5434513345794882],\n",
       "      'train_acc_history': [0.7665165165165165],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7665165165165165,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8678283042923927,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.7665165165165165,\n",
       "        'macro avg': {'precision': 0.38325825825825827,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43391415214619633,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.5875475700926152,\n",
       "         'recall': 0.7665165165165165,\n",
       "         'f1-score': 0.6652047287406403,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}}}}},\n",
       "  'population': {'nn': {'summary': {'avg_accuracy': 0.9897897897897897,\n",
       "     'std_accuracy': 0.004073471461336471,\n",
       "     'train_loss_history': [0.4082963215868648,\n",
       "      0.2560620414047707,\n",
       "      0.18459973960387996,\n",
       "      0.1369223566862141,\n",
       "      0.09921069072514047,\n",
       "      0.07796841301023959,\n",
       "      0.06155939766819157,\n",
       "      0.04731785215028539,\n",
       "      0.03992564918291642,\n",
       "      0.02882688076848664,\n",
       "      0.02482560660417487,\n",
       "      0.019248286445011815,\n",
       "      0.01699491177412977,\n",
       "      0.015220815009187632,\n",
       "      0.013649343192668223,\n",
       "      0.011540881293348786,\n",
       "      0.010923936437508771,\n",
       "      0.010154168497453014,\n",
       "      0.010870798372254683,\n",
       "      0.008978258708787218,\n",
       "      0.00839349468091003,\n",
       "      0.006875275708053533,\n",
       "      0.005836979795826554,\n",
       "      0.0054214612702929935,\n",
       "      0.004960558766487805,\n",
       "      0.005582022723719114,\n",
       "      0.004513093613928593,\n",
       "      0.004219547529398214,\n",
       "      0.003852061270139884,\n",
       "      0.004152553997899792],\n",
       "     'train_loss_std': [0.018344008324249733,\n",
       "      0.014380487896903632,\n",
       "      0.011317509328528133,\n",
       "      0.012902940586785037,\n",
       "      0.009706649495063028,\n",
       "      0.005395391032695678,\n",
       "      0.006381803112474333,\n",
       "      0.006301638800666842,\n",
       "      0.005767885452918462,\n",
       "      0.002780406769559608,\n",
       "      0.003518915006423894,\n",
       "      0.005135684370733908,\n",
       "      0.004090753403324236,\n",
       "      0.0024039219672986776,\n",
       "      0.0017869646669418236,\n",
       "      0.0022080993227225113,\n",
       "      0.0017050384661614958,\n",
       "      0.0030411850489306825,\n",
       "      0.0025078159879028043,\n",
       "      0.0037462169126771055,\n",
       "      0.0024575201353979285,\n",
       "      0.002341815110695689,\n",
       "      0.0016663439467420537,\n",
       "      0.0007327852581614683,\n",
       "      0.0010610541422600432,\n",
       "      0.0007542418588651854,\n",
       "      0.001374687692170442,\n",
       "      0.0012306981396421354,\n",
       "      0.00042787808256671976,\n",
       "      0.0009578449560568753],\n",
       "     'train_acc_history': [0.8152439024390243,\n",
       "      0.8881859756097562,\n",
       "      0.9272103658536585,\n",
       "      0.9469512195121952,\n",
       "      0.9643292682926828,\n",
       "      0.9733993902439024,\n",
       "      0.9785060975609756,\n",
       "      0.9858231707317074,\n",
       "      0.9875762195121951,\n",
       "      0.991996951219512,\n",
       "      0.9933689024390244,\n",
       "      0.9950457317073171,\n",
       "      0.9963414634146341,\n",
       "      0.9957317073170732,\n",
       "      0.996951219512195,\n",
       "      0.9974847560975609,\n",
       "      0.9975609756097562,\n",
       "      0.9972560975609757,\n",
       "      0.9975609756097562,\n",
       "      0.998170731707317,\n",
       "      0.9980945121951219,\n",
       "      0.9983993902439025,\n",
       "      0.9990853658536587,\n",
       "      0.9991615853658538,\n",
       "      0.9989329268292682,\n",
       "      0.998704268292683,\n",
       "      0.9992378048780488,\n",
       "      0.9994664634146341,\n",
       "      0.999314024390244,\n",
       "      0.9991615853658538],\n",
       "     'train_acc_std': [0.01376896574755299,\n",
       "      0.0064880670564117175,\n",
       "      0.005913769738192342,\n",
       "      0.005721034757559495,\n",
       "      0.00522312246921091,\n",
       "      0.0032995214667550766,\n",
       "      0.0033362909494267217,\n",
       "      0.0018886450752078818,\n",
       "      0.001937235522429712,\n",
       "      0.0022992535257619438,\n",
       "      0.0017145155303661685,\n",
       "      0.002800483699828335,\n",
       "      0.0014581651272643902,\n",
       "      0.0009458592717981,\n",
       "      0.0009018414303505567,\n",
       "      0.000982019720024777,\n",
       "      0.0005703746016423488,\n",
       "      0.0012844740507890993,\n",
       "      0.0008555619024635469,\n",
       "      0.0014935913065751087,\n",
       "      0.00122900270553332,\n",
       "      0.0015878556902438657,\n",
       "      0.0005703746016423874,\n",
       "      0.00044443230905831505,\n",
       "      0.00044443230905831505,\n",
       "      0.0003886447799994711,\n",
       "      0.0007993969879345807,\n",
       "      0.0004573170731707155,\n",
       "      0.00015243902439023848,\n",
       "      0.0003733978266437639],\n",
       "     'val_loss_history': [0.28042386797341434,\n",
       "      0.2046368506821719,\n",
       "      0.14241704050112852,\n",
       "      0.10933778482404621,\n",
       "      0.08534601354463535,\n",
       "      0.07168191474947064,\n",
       "      0.06012406453320925,\n",
       "      0.05658557757057927,\n",
       "      0.055546072510663756,\n",
       "      0.04997193487996066,\n",
       "      0.04456834743138064,\n",
       "      0.04179468822284517,\n",
       "      0.0389702696903524,\n",
       "      0.04077970492865213,\n",
       "      0.04227667629761113,\n",
       "      0.04357922899811952,\n",
       "      0.03948404613395476,\n",
       "      0.04312799958385189,\n",
       "      0.042238762110619894,\n",
       "      0.04084704514455304,\n",
       "      0.03865972669763406,\n",
       "      0.039745993996885685,\n",
       "      0.03807276072171094,\n",
       "      0.03944722489580851,\n",
       "      0.03927962237979624,\n",
       "      0.03855984058675611,\n",
       "      0.03983394206152298,\n",
       "      0.03963651823049242,\n",
       "      0.04153551518975292,\n",
       "      0.04126997031033335],\n",
       "     'val_loss_std': [0.015156815577371753,\n",
       "      0.01494722943493304,\n",
       "      0.016217370925318712,\n",
       "      0.019279181090347052,\n",
       "      0.015813872724091738,\n",
       "      0.01779059380066552,\n",
       "      0.013135412323430436,\n",
       "      0.01695847789651854,\n",
       "      0.013459068849864222,\n",
       "      0.015018585234127559,\n",
       "      0.017196396463479115,\n",
       "      0.015484461213966546,\n",
       "      0.015018314063324644,\n",
       "      0.01569351702185413,\n",
       "      0.016824677103101874,\n",
       "      0.018081857759098763,\n",
       "      0.015753016945989174,\n",
       "      0.018672658910606245,\n",
       "      0.020228113199724645,\n",
       "      0.01714912165026723,\n",
       "      0.01646196997148726,\n",
       "      0.01760225250730437,\n",
       "      0.01629023957552823,\n",
       "      0.017707531410954158,\n",
       "      0.017156035872595555,\n",
       "      0.016218296302335823,\n",
       "      0.01807721823589729,\n",
       "      0.016636684245854013,\n",
       "      0.017838968127907202,\n",
       "      0.016930342350480587],\n",
       "     'val_acc_history': [0.8747747747747748,\n",
       "      0.9120120120120119,\n",
       "      0.942942942942943,\n",
       "      0.9582582582582582,\n",
       "      0.963063063063063,\n",
       "      0.9693693693693695,\n",
       "      0.9747747747747747,\n",
       "      0.9807807807807809,\n",
       "      0.9801801801801802,\n",
       "      0.9834834834834835,\n",
       "      0.9846846846846848,\n",
       "      0.9873873873873874,\n",
       "      0.987987987987988,\n",
       "      0.987987987987988,\n",
       "      0.987987987987988,\n",
       "      0.9867867867867869,\n",
       "      0.9876876876876877,\n",
       "      0.9888888888888887,\n",
       "      0.9888888888888889,\n",
       "      0.9882882882882884,\n",
       "      0.9897897897897897,\n",
       "      0.9888888888888889,\n",
       "      0.9888888888888887,\n",
       "      0.9888888888888889,\n",
       "      0.9891891891891891,\n",
       "      0.9891891891891891,\n",
       "      0.987987987987988,\n",
       "      0.9894894894894894,\n",
       "      0.9900900900900901,\n",
       "      0.9897897897897897],\n",
       "     'val_acc_std': [0.01750006280400416,\n",
       "      0.005975900523162889,\n",
       "      0.004748164654907461,\n",
       "      0.0073803037402069074,\n",
       "      0.007979177330682543,\n",
       "      0.01109893009764344,\n",
       "      0.0058075314131572735,\n",
       "      0.004879302345126687,\n",
       "      0.004690840646190198,\n",
       "      0.0071695714031911805,\n",
       "      0.007318953517690056,\n",
       "      0.0056660547339679014,\n",
       "      0.005776391610111488,\n",
       "      0.005287332390888561,\n",
       "      0.004934437455001486,\n",
       "      0.005060750614520343,\n",
       "      0.005807531413157245,\n",
       "      0.005422063088668972,\n",
       "      0.00480480480480479,\n",
       "      0.005807531413157245,\n",
       "      0.0038457202627224076,\n",
       "      0.004514503416928784,\n",
       "      0.004988963281031867,\n",
       "      0.0055859084798012834,\n",
       "      0.004970854461635683,\n",
       "      0.005149077537382049,\n",
       "      0.005287332390888586,\n",
       "      0.004028951310810408,\n",
       "      0.004204204204204179,\n",
       "      0.004073471461336471]},\n",
       "    'folds': {'1': {'accuracy': 0.990990990990991,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4292322906052194,\n",
       "       0.2632934203962,\n",
       "       0.18517158507573894,\n",
       "       0.14787136327202727,\n",
       "       0.10348840130538475,\n",
       "       0.08183367350479452,\n",
       "       0.06546528798687022,\n",
       "       0.0495596755432283,\n",
       "       0.04080821613485857,\n",
       "       0.028435087467475636,\n",
       "       0.02756139833661841,\n",
       "       0.020054137765816073,\n",
       "       0.017201891237097543,\n",
       "       0.01815339486773421,\n",
       "       0.012494994974808722,\n",
       "       0.011303369438548276,\n",
       "       0.010597611663908494,\n",
       "       0.008592309657393432,\n",
       "       0.009279923272182846,\n",
       "       0.012743738142023908,\n",
       "       0.008604822859236198,\n",
       "       0.005559354395855491,\n",
       "       0.005594175947807365,\n",
       "       0.006569400364419491,\n",
       "       0.005744385944747525,\n",
       "       0.005505825844366194,\n",
       "       0.0035315132174618177,\n",
       "       0.005998557835260815,\n",
       "       0.004105528862011142,\n",
       "       0.003637827167616839],\n",
       "      'train_acc_history': [0.796875,\n",
       "       0.8841463414634146,\n",
       "       0.9230182926829268,\n",
       "       0.9401676829268293,\n",
       "       0.9576981707317073,\n",
       "       0.9676067073170732,\n",
       "       0.9790396341463414,\n",
       "       0.9858993902439024,\n",
       "       0.9889481707317073,\n",
       "       0.9919969512195121,\n",
       "       0.9916158536585366,\n",
       "       0.995045731707317,\n",
       "       0.9954268292682927,\n",
       "       0.995045731707317,\n",
       "       0.9984756097560976,\n",
       "       0.9977134146341463,\n",
       "       0.9980945121951219,\n",
       "       0.9973323170731707,\n",
       "       0.9980945121951219,\n",
       "       0.9961890243902439,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9988567073170732,\n",
       "       0.9988567073170732,\n",
       "       1.0,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488],\n",
       "      'val_loss_history': [0.280498207970099,\n",
       "       0.21474547616460107,\n",
       "       0.15228541669520465,\n",
       "       0.11976012147285721,\n",
       "       0.09278275529769334,\n",
       "       0.07393703186376528,\n",
       "       0.0606645109470595,\n",
       "       0.051870282612402334,\n",
       "       0.06393798762424426,\n",
       "       0.05601897395469926,\n",
       "       0.04444346602328799,\n",
       "       0.04953129877420989,\n",
       "       0.049502299897457386,\n",
       "       0.04488039385019378,\n",
       "       0.04615805581190877,\n",
       "       0.05418914500412277,\n",
       "       0.04577213399831883,\n",
       "       0.049124850097789684,\n",
       "       0.04768051626160741,\n",
       "       0.05141498547428372,\n",
       "       0.04756185206563466,\n",
       "       0.04782243719356219,\n",
       "       0.04290512424301018,\n",
       "       0.04240317370171066,\n",
       "       0.04378474333895032,\n",
       "       0.04297221227104522,\n",
       "       0.04544948533822952,\n",
       "       0.044843195884657856,\n",
       "       0.044837655769449404,\n",
       "       0.0441871296365703],\n",
       "      'val_acc_history': [0.8648648648648649,\n",
       "       0.9039039039039038,\n",
       "       0.9414414414414415,\n",
       "       0.948948948948949,\n",
       "       0.954954954954955,\n",
       "       0.9654654654654654,\n",
       "       0.972972972972973,\n",
       "       0.9864864864864865,\n",
       "       0.9774774774774775,\n",
       "       0.984984984984985,\n",
       "       0.987987987987988,\n",
       "       0.9864864864864865,\n",
       "       0.9894894894894894,\n",
       "       0.987987987987988,\n",
       "       0.9864864864864865,\n",
       "       0.987987987987988,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.982339012037098,\n",
       "         'recall': 0.9904007162159921,\n",
       "         'f1-score': 0.9863533918384532,\n",
       "         'support': 60317.0},\n",
       "        '1': {'precision': 0.9676680813044449,\n",
       "         'recall': 0.9416399500081508,\n",
       "         'f1-score': 0.9544766048855718,\n",
       "         'support': 18403.0},\n",
       "        'accuracy': 0.9790015243902439,\n",
       "        'macro avg': {'precision': 0.9750035466707714,\n",
       "         'recall': 0.9660203331120715,\n",
       "         'f1-score': 0.9704149983620125,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9789092719675729,\n",
       "         'recall': 0.9790015243902439,\n",
       "         'f1-score': 0.9789013020227281,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9941291585127201,\n",
       "         'recall': 0.9941291585127201,\n",
       "         'f1-score': 0.9941291585127201,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.012455063872039318},\n",
       "        'treatment': {'precision': 0.9806451612903225,\n",
       "         'recall': 0.9806451612903225,\n",
       "         'f1-score': 0.9806451612903225,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.1498107761144638},\n",
       "        'accuracy': 0.990990990990991,\n",
       "        'macro avg': {'precision': 0.9873871599015214,\n",
       "         'recall': 0.9873871599015214,\n",
       "         'f1-score': 0.9873871599015214,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.990990990990991,\n",
       "         'recall': 0.990990990990991,\n",
       "         'f1-score': 0.990990990990991,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.996996996996997,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.405608000551782,\n",
       "       0.2616189336631356,\n",
       "       0.1916318241415954,\n",
       "       0.14625240208172216,\n",
       "       0.09975084889589286,\n",
       "       0.07750467488133325,\n",
       "       0.0630981770412224,\n",
       "       0.05251338489626239,\n",
       "       0.044608838015758406,\n",
       "       0.02982668725137667,\n",
       "       0.021882270962545056,\n",
       "       0.011578654688669414,\n",
       "       0.011495414996383393,\n",
       "       0.01188105474248892,\n",
       "       0.011467957173519564,\n",
       "       0.009536826665081629,\n",
       "       0.008791403867667767,\n",
       "       0.010875005394265783,\n",
       "       0.013712907408750275,\n",
       "       0.0064141373575960356,\n",
       "       0.004612208185036008,\n",
       "       0.004139896186931831,\n",
       "       0.005361315788610316,\n",
       "       0.005094441335375716,\n",
       "       0.003498348354918473,\n",
       "       0.004564377704987348,\n",
       "       0.00721143614557549,\n",
       "       0.0034410532467385254,\n",
       "       0.00317633705406745,\n",
       "       0.003302144764059382],\n",
       "      'train_acc_history': [0.823170731707317,\n",
       "       0.8841463414634146,\n",
       "       0.9230182926829268,\n",
       "       0.9474085365853658,\n",
       "       0.9668445121951219,\n",
       "       0.9740853658536586,\n",
       "       0.9782774390243902,\n",
       "       0.9832317073170732,\n",
       "       0.9855182926829268,\n",
       "       0.9919969512195121,\n",
       "       0.995045731707317,\n",
       "       1.0,\n",
       "       0.9977134146341463,\n",
       "       0.9961890243902439,\n",
       "       0.9965701219512195,\n",
       "       0.9988567073170732,\n",
       "       0.9977134146341463,\n",
       "       0.9969512195121951,\n",
       "       0.9961890243902439,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9977134146341463,\n",
       "       1.0,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488],\n",
       "      'val_loss_history': [0.2682728293267163,\n",
       "       0.18475048040801828,\n",
       "       0.12204852683300321,\n",
       "       0.07617136937650767,\n",
       "       0.05717404559254646,\n",
       "       0.04070706936446103,\n",
       "       0.03668911670419303,\n",
       "       0.028956382620063694,\n",
       "       0.030571247154677458,\n",
       "       0.024158511924642054,\n",
       "       0.013682119929316368,\n",
       "       0.011845134632577274,\n",
       "       0.009579352283088321,\n",
       "       0.010181561980227178,\n",
       "       0.010739985001485118,\n",
       "       0.008607042276046493,\n",
       "       0.010131470901383595,\n",
       "       0.010564929234202613,\n",
       "       0.005131737381981855,\n",
       "       0.007056989159371535,\n",
       "       0.006663385506147857,\n",
       "       0.005757904309906404,\n",
       "       0.007142504370263355,\n",
       "       0.005647240569074215,\n",
       "       0.006889329379191622,\n",
       "       0.00726968176340134,\n",
       "       0.004052888400937346,\n",
       "       0.006934298181229017,\n",
       "       0.007660847109615464,\n",
       "       0.008705257137824612],\n",
       "      'val_acc_history': [0.8903903903903904,\n",
       "       0.9129129129129129,\n",
       "       0.93993993993994,\n",
       "       0.96996996996997,\n",
       "       0.9774774774774775,\n",
       "       0.990990990990991,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.9864864864864865,\n",
       "       0.993993993993994,\n",
       "       0.9954954954954955,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.9954954954954955,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.983597117567701,\n",
       "         'recall': 0.9915415871962849,\n",
       "         'f1-score': 0.9875533751249205,\n",
       "         'support': 60295.0},\n",
       "        '1': {'precision': 0.9715687367599509,\n",
       "         'recall': 0.9458887381275441,\n",
       "         'f1-score': 0.958556774743558,\n",
       "         'support': 18425.0},\n",
       "        'accuracy': 0.9808561991869919,\n",
       "        'macro avg': {'precision': 0.9775829271638259,\n",
       "         'recall': 0.9687151626619146,\n",
       "         'f1-score': 0.9730550749342393,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9807817858047081,\n",
       "         'recall': 0.9808561991869919,\n",
       "         'f1-score': 0.9807665056886069,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 1.0,\n",
       "         'recall': 0.9960861056751468,\n",
       "         'f1-score': 0.9980392156862745,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.011755562387406826},\n",
       "        'treatment': {'precision': 0.9872611464968153,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.9935897435897436,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.0007250531925819814},\n",
       "        'accuracy': 0.996996996996997,\n",
       "        'macro avg': {'precision': 0.9936305732484076,\n",
       "         'recall': 0.9980430528375734,\n",
       "         'f1-score': 0.9958144796380091,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9970352518123218,\n",
       "         'recall': 0.996996996996997,\n",
       "         'f1-score': 0.9970036778860308,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.987987987987988,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.41704369899703236,\n",
       "       0.27318924265663797,\n",
       "       0.19944418130851374,\n",
       "       0.1397602569039275,\n",
       "       0.11124100190837209,\n",
       "       0.08353296004054023,\n",
       "       0.06254089133041661,\n",
       "       0.0517567980625644,\n",
       "       0.04379634890796208,\n",
       "       0.02973246419938599,\n",
       "       0.02923834937193045,\n",
       "       0.022985441314919693,\n",
       "       0.024069822881734225,\n",
       "       0.016333249443006223,\n",
       "       0.016509289580692605,\n",
       "       0.014357526252818545,\n",
       "       0.013607782930726321,\n",
       "       0.015744337181719702,\n",
       "       0.014094897755989579,\n",
       "       0.013523963234591775,\n",
       "       0.011041360680104756,\n",
       "       0.011057737558830256,\n",
       "       0.009066627781111293,\n",
       "       0.005943941111426528,\n",
       "       0.00652541150055008,\n",
       "       0.006679792675507687,\n",
       "       0.004194755927750432,\n",
       "       0.005286927048185068,\n",
       "       0.004346833090161587,\n",
       "       0.006000753033554136],\n",
       "      'train_acc_history': [0.8125,\n",
       "       0.8814786585365854,\n",
       "       0.9222560975609756,\n",
       "       0.9458841463414634,\n",
       "       0.9611280487804879,\n",
       "       0.973704268292683,\n",
       "       0.9744664634146342,\n",
       "       0.9847560975609756,\n",
       "       0.9855182926829268,\n",
       "       0.9939024390243902,\n",
       "       0.9919969512195121,\n",
       "       0.9927591463414634,\n",
       "       0.9939024390243902,\n",
       "       0.9961890243902439,\n",
       "       0.9965701219512195,\n",
       "       0.9965701219512195,\n",
       "       0.9973323170731707,\n",
       "       0.995045731707317,\n",
       "       0.9969512195121951,\n",
       "       0.9965701219512195,\n",
       "       0.9965701219512195,\n",
       "       0.9954268292682927,\n",
       "       0.9980945121951219,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976],\n",
       "      'val_loss_history': [0.30735716359181836,\n",
       "       0.2264718460765752,\n",
       "       0.16826992414214395,\n",
       "       0.13400689139962196,\n",
       "       0.10536718656393615,\n",
       "       0.09569601545279677,\n",
       "       0.07653407117521221,\n",
       "       0.07779677470468661,\n",
       "       0.05954044718633999,\n",
       "       0.06460643684576181,\n",
       "       0.05479577277914028,\n",
       "       0.047718511248769406,\n",
       "       0.044018104492517356,\n",
       "       0.047836290852336046,\n",
       "       0.05075810832733458,\n",
       "       0.05405940673187037,\n",
       "       0.04356181114615703,\n",
       "       0.06563700987978584,\n",
       "       0.06537250874415887,\n",
       "       0.047139029844071374,\n",
       "       0.048283896551848476,\n",
       "       0.05198739409520799,\n",
       "       0.051142861590382054,\n",
       "       0.05205678219367242,\n",
       "       0.04983857954555953,\n",
       "       0.051705425886955876,\n",
       "       0.0511700922136449,\n",
       "       0.04799178680828349,\n",
       "       0.05761690675783691,\n",
       "       0.05715369335005314],\n",
       "      'val_acc_history': [0.8768768768768769,\n",
       "       0.9069069069069069,\n",
       "       0.9384384384384384,\n",
       "       0.9519519519519519,\n",
       "       0.9594594594594594,\n",
       "       0.9594594594594594,\n",
       "       0.9714714714714715,\n",
       "       0.9774774774774775,\n",
       "       0.9774774774774775,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.9894894894894894,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.987987987987988,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9822505628872418,\n",
       "         'recall': 0.9906187327001806,\n",
       "         'f1-score': 0.9864169004786268,\n",
       "         'support': 60333.0},\n",
       "        '1': {'precision': 0.968332121076484,\n",
       "         'recall': 0.9412628487518355,\n",
       "         'f1-score': 0.9546056260341974,\n",
       "         'support': 18387.0},\n",
       "        'accuracy': 0.9790904471544716,\n",
       "        'macro avg': {'precision': 0.975291341981863,\n",
       "         'recall': 0.965940790726008,\n",
       "         'f1-score': 0.9705112632564121,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9789995670847214,\n",
       "         'recall': 0.9790904471544716,\n",
       "         'f1-score': 0.9789865917488286,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 1.0,\n",
       "         'recall': 0.984313725490196,\n",
       "         'f1-score': 0.9920948616600791,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.059998612850904465},\n",
       "        'treatment': {'precision': 0.9512195121951219,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.975,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.008119875565171242},\n",
       "        'accuracy': 0.987987987987988,\n",
       "        'macro avg': {'precision': 0.975609756097561,\n",
       "         'recall': 0.9921568627450981,\n",
       "         'f1-score': 0.9835474308300396,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.988573939793452,\n",
       "         'recall': 0.987987987987988,\n",
       "         'f1-score': 0.9880906598297902,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.987987987987988,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.37484775592641134,\n",
       "       0.23085707389726873,\n",
       "       0.16568365147927913,\n",
       "       0.11213042923226589,\n",
       "       0.08168040306829824,\n",
       "       0.06804311102846773,\n",
       "       0.049279836074608126,\n",
       "       0.035202873087056524,\n",
       "       0.02872183478278358,\n",
       "       0.02385495344131458,\n",
       "       0.019769036495013208,\n",
       "       0.01563717163077033,\n",
       "       0.015170660557043626,\n",
       "       0.012911918732087786,\n",
       "       0.012997639328582077,\n",
       "       0.008782813791185617,\n",
       "       0.011963235198735917,\n",
       "       0.008395160045815496,\n",
       "       0.00817046726190644,\n",
       "       0.0036309288911203423,\n",
       "       0.006802966146695814,\n",
       "       0.007403391796709379,\n",
       "       0.00449576889159067,\n",
       "       0.004543825675926466,\n",
       "       0.004649719189307312,\n",
       "       0.005033300076497764,\n",
       "       0.0035637370211344848,\n",
       "       0.0036978318906820767,\n",
       "       0.004087023924689785,\n",
       "       0.004093615819484268],\n",
       "      'train_acc_history': [0.836890243902439,\n",
       "       0.8990091463414634,\n",
       "       0.9375,\n",
       "       0.9573170731707317,\n",
       "       0.9729420731707317,\n",
       "       0.9778963414634146,\n",
       "       0.984375,\n",
       "       0.9889481707317073,\n",
       "       0.9904725609756098,\n",
       "       0.9942835365853658,\n",
       "       0.9958079268292683,\n",
       "       0.9954268292682927,\n",
       "       0.9973323170731707,\n",
       "       0.9969512195121951,\n",
       "       0.9958079268292683,\n",
       "       0.9980945121951219,\n",
       "       0.9965701219512195,\n",
       "       0.9980945121951219,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9980945121951219,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488],\n",
       "      'val_loss_history': [0.26390627974813635,\n",
       "       0.19269254126332022,\n",
       "       0.13761550662192432,\n",
       "       0.1128126247362657,\n",
       "       0.08610655062577942,\n",
       "       0.0701455743136731,\n",
       "       0.05998298750174316,\n",
       "       0.05377208509228446,\n",
       "       0.05421223804693331,\n",
       "       0.04259923226411708,\n",
       "       0.044860074571757155,\n",
       "       0.043846121047284796,\n",
       "       0.041866688766855405,\n",
       "       0.046050346858630124,\n",
       "       0.043204306974075735,\n",
       "       0.04370289475297217,\n",
       "       0.04052761405050247,\n",
       "       0.03702828447355635,\n",
       "       0.040683220021574845,\n",
       "       0.045182408331046725,\n",
       "       0.0395371530586007,\n",
       "       0.040043243042997674,\n",
       "       0.037836182142861864,\n",
       "       0.04180584964342415,\n",
       "       0.039557319827674124,\n",
       "       0.040449521350885996,\n",
       "       0.04639115260215476,\n",
       "       0.04506141261134127,\n",
       "       0.04290150050391359,\n",
       "       0.045219487280436704],\n",
       "      'val_acc_history': [0.8948948948948949,\n",
       "       0.9204204204204204,\n",
       "       0.9429429429429429,\n",
       "       0.9594594594594594,\n",
       "       0.9654654654654654,\n",
       "       0.9669669669669669,\n",
       "       0.978978978978979,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.9894894894894894,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9834834834834835,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9834834834834835,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9846894353818606,\n",
       "         'recall': 0.9937827442138073,\n",
       "         'f1-score': 0.9892151927980263,\n",
       "         'support': 60316.0},\n",
       "        '1': {'precision': 0.9789880652210455,\n",
       "         'recall': 0.9493588350358617,\n",
       "         'f1-score': 0.9639458221842157,\n",
       "         'support': 18404.0},\n",
       "        'accuracy': 0.9833968495934959,\n",
       "        'macro avg': {'precision': 0.9818387503014531,\n",
       "         'recall': 0.9715707896248345,\n",
       "         'f1-score': 0.9765805074911209,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9833565083437554,\n",
       "         'recall': 0.9833968495934959,\n",
       "         'f1-score': 0.9833074502068605,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.990234375,\n",
       "         'recall': 0.9941176470588236,\n",
       "         'f1-score': 0.9921722113502935,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.01968659646809101},\n",
       "        'treatment': {'precision': 0.9805194805194806,\n",
       "         'recall': 0.967948717948718,\n",
       "         'f1-score': 0.9741935483870968,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.13961337506771088},\n",
       "        'accuracy': 0.987987987987988,\n",
       "        'macro avg': {'precision': 0.9853769277597403,\n",
       "         'recall': 0.9810331825037708,\n",
       "         'f1-score': 0.9831828798686952,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9879588141306892,\n",
       "         'recall': 0.987987987987988,\n",
       "         'f1-score': 0.9879609929985538,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.984984984984985,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4147498618538787,\n",
       "       0.25135153641061087,\n",
       "       0.18106745601427265,\n",
       "       0.13859733194112778,\n",
       "       0.09989279844775432,\n",
       "       0.07892764559606226,\n",
       "       0.0674127959078405,\n",
       "       0.047556529162315335,\n",
       "       0.04169300807321944,\n",
       "       0.03228521148288032,\n",
       "       0.025676977854767222,\n",
       "       0.02598602682488357,\n",
       "       0.01703676919839004,\n",
       "       0.01682445726062103,\n",
       "       0.014776834905738147,\n",
       "       0.013723870319109864,\n",
       "       0.009659648526505363,\n",
       "       0.007164030208070649,\n",
       "       0.00909579616244428,\n",
       "       0.008578525918604033,\n",
       "       0.010906115533477376,\n",
       "       0.00621599860194071,\n",
       "       0.004667010570013124,\n",
       "       0.004955697864316768,\n",
       "       0.004384928842915631,\n",
       "       0.006126817317236579,\n",
       "       0.004064025757720739,\n",
       "       0.002673367626124584,\n",
       "       0.003544583419769457,\n",
       "       0.0037284292047843337],\n",
       "      'train_acc_history': [0.8067835365853658,\n",
       "       0.8921493902439024,\n",
       "       0.9302591463414634,\n",
       "       0.9439786585365854,\n",
       "       0.9630335365853658,\n",
       "       0.973704268292683,\n",
       "       0.9763719512195121,\n",
       "       0.9862804878048781,\n",
       "       0.9874237804878049,\n",
       "       0.9878048780487805,\n",
       "       0.9923780487804879,\n",
       "       0.9919969512195121,\n",
       "       0.9973323170731707,\n",
       "       0.9942835365853658,\n",
       "       0.9973323170731707,\n",
       "       0.9961890243902439,\n",
       "       0.9980945121951219,\n",
       "       0.9988567073170732,\n",
       "       0.9980945121951219,\n",
       "       0.9988567073170732,\n",
       "       0.9969512195121951,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       0.9980945121951219,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244],\n",
       "      'val_loss_history': [0.2820848592303016,\n",
       "       0.20452390949834476,\n",
       "       0.1318658282133666,\n",
       "       0.10393791713497856,\n",
       "       0.08529952964322134,\n",
       "       0.07792388275265694,\n",
       "       0.06674963633783838,\n",
       "       0.07053236282345923,\n",
       "       0.06946844254112379,\n",
       "       0.0624765194105831,\n",
       "       0.0650603038534014,\n",
       "       0.056032375411384484,\n",
       "       0.04988490301184356,\n",
       "       0.05494993110187352,\n",
       "       0.06052292537325146,\n",
       "       0.0573376562255858,\n",
       "       0.05742720057341186,\n",
       "       0.05328492423392494,\n",
       "       0.05232582814377648,\n",
       "       0.05344181291399185,\n",
       "       0.051252346305938605,\n",
       "       0.05311899134275419,\n",
       "       0.051337131262037226,\n",
       "       0.055323078371161086,\n",
       "       0.05632813980760561,\n",
       "       0.05040236166149208,\n",
       "       0.052106091752648354,\n",
       "       0.053351897666950455,\n",
       "       0.05466066580794921,\n",
       "       0.05108428414678201],\n",
       "      'val_acc_history': [0.8468468468468469,\n",
       "       0.9159159159159159,\n",
       "       0.9519519519519519,\n",
       "       0.960960960960961,\n",
       "       0.9579579579579579,\n",
       "       0.963963963963964,\n",
       "       0.9669669669669669,\n",
       "       0.972972972972973,\n",
       "       0.9744744744744744,\n",
       "       0.9714714714714715,\n",
       "       0.972972972972973,\n",
       "       0.9804804804804805,\n",
       "       0.978978978978979,\n",
       "       0.9804804804804805,\n",
       "       0.9819819819819819,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.984984984984985,\n",
       "       0.9804804804804805,\n",
       "       0.9864864864864865,\n",
       "       0.9834834834834835,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9851382146041812,\n",
       "         'recall': 0.9888612630532073,\n",
       "         'f1-score': 0.9869962279134405,\n",
       "         'support': 60330.0},\n",
       "        '1': {'precision': 0.9629996696399075,\n",
       "         'recall': 0.9510603588907015,\n",
       "         'f1-score': 0.9569927774130007,\n",
       "         'support': 18390.0},\n",
       "        'accuracy': 0.980030487804878,\n",
       "        'macro avg': {'precision': 0.9740689421220443,\n",
       "         'recall': 0.9699608109719544,\n",
       "         'f1-score': 0.9719945026632206,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9799663670191584,\n",
       "         'recall': 0.980030487804878,\n",
       "         'f1-score': 0.979987037686013,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9863813229571985,\n",
       "         'recall': 0.9941176470588236,\n",
       "         'f1-score': 0.990234375,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.01488159317523241},\n",
       "        'treatment': {'precision': 0.9802631578947368,\n",
       "         'recall': 0.9551282051282052,\n",
       "         'f1-score': 0.9675324675324676,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.18172334134578705},\n",
       "        'accuracy': 0.984984984984985,\n",
       "        'macro avg': {'precision': 0.9833222404259676,\n",
       "         'recall': 0.9746229260935144,\n",
       "         'f1-score': 0.9788834212662338,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9849482392488742,\n",
       "         'recall': 0.984984984984985,\n",
       "         'f1-score': 0.9849168110886861,\n",
       "         'support': 666.0}}}}}},\n",
       "   'simple': {'summary': {'avg_accuracy': 0.8627627627627626,\n",
       "     'std_accuracy': 0.012761879496021485,\n",
       "     'train_loss_history': [0.2926997235126339],\n",
       "     'train_loss_std': [0.0027455994404608293],\n",
       "     'train_acc_history': [0.8647147147147146],\n",
       "     'train_acc_std': [0.002733805023428383],\n",
       "     'val_loss_history': [],\n",
       "     'val_loss_std': [],\n",
       "     'val_acc_history': [],\n",
       "     'val_acc_std': []},\n",
       "    'folds': {'1': {'accuracy': 0.8528528528528528,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.2934015871061245],\n",
       "      'train_acc_history': [0.859984984984985],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8564102564102564,\n",
       "         'recall': 0.9818716315531603,\n",
       "         'f1-score': 0.9148596210910751,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.8858024691358025,\n",
       "         'recall': 0.4606741573033708,\n",
       "         'f1-score': 0.6061246040126715,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.859984984984985,\n",
       "        'macro avg': {'precision': 0.8711063627730294,\n",
       "         'recall': 0.7212728944282656,\n",
       "         'f1-score': 0.7604921125518733,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8632838857375893,\n",
       "         'recall': 0.859984984984985,\n",
       "         'f1-score': 0.8426592023073493,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8470588235294118,\n",
       "         'recall': 0.9863013698630136,\n",
       "         'f1-score': 0.9113924050632911,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.9014084507042254,\n",
       "         'recall': 0.4129032258064516,\n",
       "         'f1-score': 0.5663716814159292,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8528528528528528,\n",
       "        'macro avg': {'precision': 0.8742336371168186,\n",
       "         'recall': 0.6996022978347326,\n",
       "         'f1-score': 0.7388820432396102,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8597077607848113,\n",
       "         'recall': 0.8528528528528528,\n",
       "         'f1-score': 0.8310947891994156,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.8813813813813813,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.29664639758151445],\n",
       "      'train_acc_history': [0.8674924924924925],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8619210977701544,\n",
       "         'recall': 0.984811366976972,\n",
       "         'f1-score': 0.9192773839469471,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.9066265060240963,\n",
       "         'recall': 0.48314606741573035,\n",
       "         'f1-score': 0.6303664921465969,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.8674924924924925,\n",
       "        'macro avg': {'precision': 0.8842738018971253,\n",
       "         'recall': 0.7339787171963512,\n",
       "         'f1-score': 0.774821938046772,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8723758535292406,\n",
       "         'recall': 0.8674924924924925,\n",
       "         'f1-score': 0.8517130124786221,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8763066202090593,\n",
       "         'recall': 0.9843444227005871,\n",
       "         'f1-score': 0.9271889400921659,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.9130434782608695,\n",
       "         'recall': 0.5419354838709678,\n",
       "         'f1-score': 0.680161943319838,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8813813813813813,\n",
       "        'macro avg': {'precision': 0.8946750492349644,\n",
       "         'recall': 0.7631399532857774,\n",
       "         'f1-score': 0.8036754417060019,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8848564895754716,\n",
       "         'recall': 0.8813813813813813,\n",
       "         'f1-score': 0.8696976720745822,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.8453453453453453,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.2880784124521485],\n",
       "      'train_acc_history': [0.8663663663663663],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8602564102564103,\n",
       "         'recall': 0.9857982370225269,\n",
       "         'f1-score': 0.9187585577361935,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.9104938271604939,\n",
       "         'recall': 0.4742765273311897,\n",
       "         'f1-score': 0.6236786469344608,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8663663663663663,\n",
       "        'macro avg': {'precision': 0.8853751187084521,\n",
       "         'recall': 0.7300373821768583,\n",
       "         'f1-score': 0.7712186023353271,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8719860173563878,\n",
       "         'recall': 0.8663663663663663,\n",
       "         'f1-score': 0.8498622722562094,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8431703204047217,\n",
       "         'recall': 0.9803921568627451,\n",
       "         'f1-score': 0.9066183136899365,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.863013698630137,\n",
       "         'recall': 0.40384615384615385,\n",
       "         'f1-score': 0.5502183406113537,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8453453453453453,\n",
       "        'macro avg': {'precision': 0.8530920095174294,\n",
       "         'recall': 0.6921191553544495,\n",
       "         'f1-score': 0.728418327150645,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8478183189079722,\n",
       "         'recall': 0.8453453453453453,\n",
       "         'f1-score': 0.823137238914773,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.8633633633633634,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.2930506963920122],\n",
       "      'train_acc_history': [0.8663663663663663],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8602564102564103,\n",
       "         'recall': 0.9857982370225269,\n",
       "         'f1-score': 0.9187585577361935,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.9104938271604939,\n",
       "         'recall': 0.4742765273311897,\n",
       "         'f1-score': 0.6236786469344608,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8663663663663663,\n",
       "        'macro avg': {'precision': 0.8853751187084521,\n",
       "         'recall': 0.7300373821768583,\n",
       "         'f1-score': 0.7712186023353271,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8719860173563878,\n",
       "         'recall': 0.8663663663663663,\n",
       "         'f1-score': 0.8498622722562094,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8605851979345955,\n",
       "         'recall': 0.9803921568627451,\n",
       "         'f1-score': 0.916590284142988,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8823529411764706,\n",
       "         'recall': 0.4807692307692308,\n",
       "         'f1-score': 0.6224066390041494,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8633633633633634,\n",
       "        'macro avg': {'precision': 0.871469069555533,\n",
       "         'recall': 0.7305806938159879,\n",
       "         'f1-score': 0.7694984615735687,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8656839486038636,\n",
       "         'recall': 0.8633633633633634,\n",
       "         'f1-score': 0.8476824032996564,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.8708708708708709,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.29232152403136974],\n",
       "      'train_acc_history': [0.8633633633633634],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8582408198121264,\n",
       "         'recall': 0.9843290891283056,\n",
       "         'f1-score': 0.916970802919708,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.9006211180124224,\n",
       "         'recall': 0.4662379421221865,\n",
       "         'f1-score': 0.614406779661017,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8633633633633634,\n",
       "        'macro avg': {'precision': 0.8794309689122743,\n",
       "         'recall': 0.7252835156252461,\n",
       "         'f1-score': 0.7656887912903625,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8681359194670003,\n",
       "         'recall': 0.8633633633633634,\n",
       "         'f1-score': 0.8463271007924911,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8642611683848798,\n",
       "         'recall': 0.9862745098039216,\n",
       "         'f1-score': 0.9212454212454212,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.9166666666666666,\n",
       "         'recall': 0.4935897435897436,\n",
       "         'f1-score': 0.6416666666666667,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8708708708708709,\n",
       "        'macro avg': {'precision': 0.8904639175257731,\n",
       "         'recall': 0.7399321266968326,\n",
       "         'f1-score': 0.781456043956044,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8765363301445777,\n",
       "         'recall': 0.8708708708708709,\n",
       "         'f1-score': 0.8557585057585056,\n",
       "         'support': 666.0}}}}}}},\n",
       "  'individual': {'nn': {'summary': {'avg_accuracy': 0.9906906906906906,\n",
       "     'std_accuracy': 0.007004146420234583,\n",
       "     'train_loss_history': [0.37629359342703006,\n",
       "      0.24952387413600596,\n",
       "      0.177708448615016,\n",
       "      0.1235357758838956,\n",
       "      0.08946032610426588,\n",
       "      0.06737345920921098,\n",
       "      0.04969977785992186,\n",
       "      0.04117698073387146,\n",
       "      0.03234069339658429,\n",
       "      0.030601008986009325,\n",
       "      0.022734785980613133,\n",
       "      0.017183656011503644,\n",
       "      0.01278283861352176,\n",
       "      0.01266379672466073,\n",
       "      0.011275340031787026,\n",
       "      0.00832759986245414,\n",
       "      0.008863630745469071,\n",
       "      0.008764099644315316,\n",
       "      0.008111082248264788,\n",
       "      0.008287810604371948,\n",
       "      0.0065555020718176555,\n",
       "      0.005866253925146671,\n",
       "      0.006111887827935833,\n",
       "      0.00456302260593852,\n",
       "      0.0046167310872455925,\n",
       "      0.00480556683785214,\n",
       "      0.004233516431016075,\n",
       "      0.00483702557394281,\n",
       "      0.00449074713127097,\n",
       "      0.003821189952134041],\n",
       "     'train_loss_std': [0.023789585419762758,\n",
       "      0.007658011883554877,\n",
       "      0.009497137659654004,\n",
       "      0.009810073493179526,\n",
       "      0.009774261803486235,\n",
       "      0.009136018688779819,\n",
       "      0.006036958146825921,\n",
       "      0.005406768749191984,\n",
       "      0.00603360994774922,\n",
       "      0.007916257488268687,\n",
       "      0.004480763416837739,\n",
       "      0.0031226975892951485,\n",
       "      0.002132491983487577,\n",
       "      0.002917072831551456,\n",
       "      0.0010375510537547383,\n",
       "      0.002018178817341176,\n",
       "      0.0018438345023974378,\n",
       "      0.0020269004446734394,\n",
       "      0.0022450396501443636,\n",
       "      0.0008936380904159029,\n",
       "      0.0016705434706828983,\n",
       "      0.0011942726747394615,\n",
       "      0.002057515928384378,\n",
       "      0.0012314758922271002,\n",
       "      0.0009501909544180674,\n",
       "      0.0008985636009941276,\n",
       "      0.0008859819628753193,\n",
       "      0.001412092247206122,\n",
       "      0.0009161821303793011,\n",
       "      0.0009287715179952905],\n",
       "     'train_acc_history': [0.8346798780487805,\n",
       "      0.8960365853658537,\n",
       "      0.936814024390244,\n",
       "      0.9605945121951219,\n",
       "      0.9692073170731706,\n",
       "      0.978125,\n",
       "      0.9849085365853657,\n",
       "      0.9868140243902438,\n",
       "      0.9915396341463415,\n",
       "      0.9910060975609756,\n",
       "      0.9939786585365853,\n",
       "      0.9955792682926831,\n",
       "      0.9977896341463415,\n",
       "      0.9974085365853658,\n",
       "      0.9977134146341463,\n",
       "      0.9988567073170731,\n",
       "      0.9984756097560975,\n",
       "      0.9979420731707316,\n",
       "      0.9982469512195122,\n",
       "      0.9983231707317074,\n",
       "      0.998780487804878,\n",
       "      0.9987042682926829,\n",
       "      0.9986280487804878,\n",
       "      0.9991615853658538,\n",
       "      0.9996189024390244,\n",
       "      0.999314024390244,\n",
       "      0.9993140243902439,\n",
       "      0.999390243902439,\n",
       "      0.9992378048780488,\n",
       "      0.999390243902439],\n",
       "     'train_acc_std': [0.016967018102825436,\n",
       "      0.0032373110642675723,\n",
       "      0.007328180086415798,\n",
       "      0.003942840596456599,\n",
       "      0.006624523654768533,\n",
       "      0.002898346546010896,\n",
       "      0.002785924829648946,\n",
       "      0.0027963317572933474,\n",
       "      0.002677464644855216,\n",
       "      0.0039058316997923875,\n",
       "      0.001676829268292703,\n",
       "      0.001145830516644286,\n",
       "      0.0009146341463414827,\n",
       "      0.0010338917657203015,\n",
       "      0.0003408640209603217,\n",
       "      0.0005389533393190271,\n",
       "      0.0015987939758691188,\n",
       "      0.001093955799878613,\n",
       "      0.001039495556172716,\n",
       "      0.0007068306780103684,\n",
       "      0.0010054044175512942,\n",
       "      0.0005169458828601655,\n",
       "      0.001093955799878613,\n",
       "      0.0006985633681334874,\n",
       "      0.00024102726068355696,\n",
       "      0.00028518730082117445,\n",
       "      0.00044443230905831505,\n",
       "      0.0005169458828601393,\n",
       "      0.0005389533393189958,\n",
       "      0.0004573170731707154],\n",
       "     'val_loss_history': [0.32612764456055376,\n",
       "      4.238862267949364,\n",
       "      24.021781789037313,\n",
       "      1.792386304553259,\n",
       "      0.10550744694403627,\n",
       "      0.0863353368402882,\n",
       "      0.0842717257497663,\n",
       "      0.07198174813846972,\n",
       "      0.08136460582979702,\n",
       "      0.07588131447238002,\n",
       "      0.057031722248277884,\n",
       "      0.05902587905716659,\n",
       "      0.06049071424005722,\n",
       "      0.05665970654967665,\n",
       "      0.06246358581437644,\n",
       "      0.06235429309626025,\n",
       "      0.059771935347023165,\n",
       "      0.0610021787812002,\n",
       "      0.06404940014578063,\n",
       "      0.06395367698701607,\n",
       "      0.06382417403987016,\n",
       "      0.06358829987161285,\n",
       "      0.0633905301092785,\n",
       "      0.06492923056040042,\n",
       "      0.06788528459993276,\n",
       "      0.06602358073578216,\n",
       "      0.06826459093029949,\n",
       "      0.06668322241457644,\n",
       "      0.0686567484395875,\n",
       "      0.06717155327602417],\n",
       "     'val_loss_std': [0.025538108417036257,\n",
       "      4.662286519991476,\n",
       "      31.69147933410786,\n",
       "      2.1160473602997576,\n",
       "      0.034211113774889584,\n",
       "      0.024919758386092495,\n",
       "      0.03479833007871599,\n",
       "      0.0316543992677122,\n",
       "      0.021038179293523765,\n",
       "      0.04429933942837747,\n",
       "      0.03835327098114855,\n",
       "      0.0429763468679868,\n",
       "      0.04170407296512265,\n",
       "      0.040605589402270506,\n",
       "      0.044115096126581176,\n",
       "      0.04532853437105954,\n",
       "      0.044783518940826474,\n",
       "      0.04028716710576478,\n",
       "      0.042770151105758734,\n",
       "      0.04300538870077269,\n",
       "      0.043356989026611656,\n",
       "      0.044859492947552995,\n",
       "      0.045198720255226206,\n",
       "      0.048106694357669504,\n",
       "      0.04574671756983174,\n",
       "      0.046656091620283356,\n",
       "      0.047264720639878174,\n",
       "      0.045985163647151064,\n",
       "      0.04884364204703752,\n",
       "      0.047106374058052666],\n",
       "     'val_acc_history': [0.863963963963964,\n",
       "      0.6714714714714713,\n",
       "      0.6339339339339338,\n",
       "      0.7753753753753754,\n",
       "      0.9675675675675676,\n",
       "      0.975975975975976,\n",
       "      0.9756756756756756,\n",
       "      0.9831831831831831,\n",
       "      0.9804804804804805,\n",
       "      0.9825825825825826,\n",
       "      0.9891891891891893,\n",
       "      0.9897897897897898,\n",
       "      0.9894894894894894,\n",
       "      0.9894894894894894,\n",
       "      0.9897897897897898,\n",
       "      0.9897897897897898,\n",
       "      0.9903903903903905,\n",
       "      0.9900900900900901,\n",
       "      0.9900900900900901,\n",
       "      0.9900900900900901,\n",
       "      0.9903903903903905,\n",
       "      0.9903903903903905,\n",
       "      0.9903903903903905,\n",
       "      0.9903903903903905,\n",
       "      0.9903903903903905,\n",
       "      0.9903903903903905,\n",
       "      0.9903903903903905,\n",
       "      0.9900900900900901,\n",
       "      0.9903903903903905,\n",
       "      0.9906906906906906],\n",
       "     'val_acc_std': [0.01689706293890114,\n",
       "      0.23528370424776046,\n",
       "      0.30641286493676095,\n",
       "      0.2808050285815574,\n",
       "      0.016132583806082044,\n",
       "      0.006647430516870458,\n",
       "      0.00887792522468138,\n",
       "      0.007257084668825565,\n",
       "      0.003003003003003002,\n",
       "      0.011576177337775613,\n",
       "      0.007796249240688972,\n",
       "      0.007561368355564047,\n",
       "      0.00684790045104586,\n",
       "      0.00684790045104586,\n",
       "      0.007068229607171055,\n",
       "      0.007068229607171055,\n",
       "      0.006887294258966783,\n",
       "      0.007144370722744929,\n",
       "      0.007144370722744929,\n",
       "      0.006821511526486803,\n",
       "      0.006887294258966783,\n",
       "      0.006887294258966783,\n",
       "      0.006887294258966783,\n",
       "      0.006887294258966783,\n",
       "      0.006887294258966783,\n",
       "      0.006887294258966783,\n",
       "      0.006887294258966783,\n",
       "      0.006755088215736917,\n",
       "      0.006887294258966783,\n",
       "      0.007004146420234583]},\n",
       "    'folds': {'1': {'accuracy': 0.993993993993994,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.34631768305127214,\n",
       "       0.24995060446785716,\n",
       "       0.17153006410453378,\n",
       "       0.12362104290869178,\n",
       "       0.09255963186847001,\n",
       "       0.07412866675635664,\n",
       "       0.04722329551672063,\n",
       "       0.04370978931192218,\n",
       "       0.03200767641296474,\n",
       "       0.04184007683269134,\n",
       "       0.02823371673002839,\n",
       "       0.021071682774984255,\n",
       "       0.01587547569694679,\n",
       "       0.016367501681443394,\n",
       "       0.012352925850214755,\n",
       "       0.006980720203278995,\n",
       "       0.01209734171265509,\n",
       "       0.01167793196864517,\n",
       "       0.009162191284547856,\n",
       "       0.007702974933369949,\n",
       "       0.00687505452178146,\n",
       "       0.008041692917553208,\n",
       "       0.009181329151388348,\n",
       "       0.005398923964182869,\n",
       "       0.004421448888734165,\n",
       "       0.004559595558065467,\n",
       "       0.004611424198828456,\n",
       "       0.005365089994639431,\n",
       "       0.0045279195069380836,\n",
       "       0.005109629819521726],\n",
       "      'train_acc_history': [0.8521341463414634,\n",
       "       0.8929115853658537,\n",
       "       0.9405487804878049,\n",
       "       0.9603658536585366,\n",
       "       0.9668445121951219,\n",
       "       0.9759908536585366,\n",
       "       0.9858993902439024,\n",
       "       0.9851371951219512,\n",
       "       0.993140243902439,\n",
       "       0.9855182926829268,\n",
       "       0.9916158536585366,\n",
       "       0.9939024390243902,\n",
       "       0.9969512195121951,\n",
       "       0.9958079268292683,\n",
       "       0.9973323170731707,\n",
       "       0.9992378048780488,\n",
       "       0.9954268292682927,\n",
       "       0.9980945121951219,\n",
       "       0.9980945121951219,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       0.9980945121951219,\n",
       "       0.9969512195121951,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732],\n",
       "      'val_loss_history': [0.3114173832264813,\n",
       "       0.3094887232238596,\n",
       "       1.3305439786477522,\n",
       "       0.12903216091746633,\n",
       "       0.09894164651632309,\n",
       "       0.09090142197568309,\n",
       "       0.08674515318125486,\n",
       "       0.08536229231818156,\n",
       "       0.07061034197580408,\n",
       "       0.1119166101244363,\n",
       "       0.06569994071667845,\n",
       "       0.056892467493360695,\n",
       "       0.05317704316059297,\n",
       "       0.054070677751124924,\n",
       "       0.05151074580234391,\n",
       "       0.05400382435817102,\n",
       "       0.052603403135435656,\n",
       "       0.04973046967378733,\n",
       "       0.05070654495449906,\n",
       "       0.05644217907683924,\n",
       "       0.05186775844777003,\n",
       "       0.054871614220742645,\n",
       "       0.05517076877930032,\n",
       "       0.05309590902073647,\n",
       "       0.05850658274721354,\n",
       "       0.05557261351813478,\n",
       "       0.05496207332048057,\n",
       "       0.05863166363666427,\n",
       "       0.05767734203783965,\n",
       "       0.05441614100180397],\n",
       "      'val_acc_history': [0.8603603603603603,\n",
       "       0.8648648648648649,\n",
       "       0.3003003003003003,\n",
       "       0.948948948948949,\n",
       "       0.96996996996997,\n",
       "       0.975975975975976,\n",
       "       0.9744744744744744,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.9654654654654654,\n",
       "       0.984984984984985,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.993993993993994],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9832292332661191,\n",
       "         'recall': 0.9948248407643312,\n",
       "         'f1-score': 0.9889930495436444,\n",
       "         'support': 60288.0},\n",
       "        '1': {'precision': 0.9823937701032673,\n",
       "         'recall': 0.9444986979166666,\n",
       "         'f1-score': 0.9630736038503028,\n",
       "         'support': 18432.0},\n",
       "        'accuracy': 0.9830411585365854,\n",
       "        'macro avg': {'precision': 0.9828115016846932,\n",
       "         'recall': 0.969661769340499,\n",
       "         'f1-score': 0.9760333266969736,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9830336126231098,\n",
       "         'recall': 0.9830411585365854,\n",
       "         'f1-score': 0.9829241061617888,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9941520467836257,\n",
       "         'recall': 0.9980430528375733,\n",
       "         'f1-score': 0.99609375,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.01153844129294157},\n",
       "        'treatment': {'precision': 0.9934640522875817,\n",
       "         'recall': 0.9806451612903225,\n",
       "         'f1-score': 0.987012987012987,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.16358286142349243},\n",
       "        'accuracy': 0.993993993993994,\n",
       "        'macro avg': {'precision': 0.9938080495356036,\n",
       "         'recall': 0.9893441070639479,\n",
       "         'f1-score': 0.9915533685064934,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9939919279444563,\n",
       "         'recall': 0.993993993993994,\n",
       "         'f1-score': 0.9939803592147343,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 1.0,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.3914879167952189,\n",
       "       0.2571381385006556,\n",
       "       0.1926608952443774,\n",
       "       0.1311251582895837,\n",
       "       0.09613373630293985,\n",
       "       0.06799816317492868,\n",
       "       0.056783329922615026,\n",
       "       0.04660800487802523,\n",
       "       0.04186962517659839,\n",
       "       0.03170205335836948,\n",
       "       0.025914720339109985,\n",
       "       0.01828444027909782,\n",
       "       0.013831844286447981,\n",
       "       0.016047219518662954,\n",
       "       0.011109513741713472,\n",
       "       0.01084468250231045,\n",
       "       0.009734336025558593,\n",
       "       0.00999994192374643,\n",
       "       0.011806689608819419,\n",
       "       0.007937809600063213,\n",
       "       0.005444352867127192,\n",
       "       0.00535710535307483,\n",
       "       0.007809895661477818,\n",
       "       0.006326991797811011,\n",
       "       0.005275180956587286,\n",
       "       0.0063806785009347085,\n",
       "       0.005565663231763898,\n",
       "       0.003700456760891872,\n",
       "       0.004963558164379764,\n",
       "       0.0040680388650266314],\n",
       "      'train_acc_history': [0.834984756097561,\n",
       "       0.8917682926829268,\n",
       "       0.9275914634146342,\n",
       "       0.9596036585365854,\n",
       "       0.9664634146341463,\n",
       "       0.9790396341463414,\n",
       "       0.9817073170731707,\n",
       "       0.9855182926829268,\n",
       "       0.9874237804878049,\n",
       "       0.9923780487804879,\n",
       "       0.9927591463414634,\n",
       "       0.9958079268292683,\n",
       "       0.9969512195121951,\n",
       "       0.9965701219512195,\n",
       "       0.9980945121951219,\n",
       "       0.9980945121951219,\n",
       "       0.9992378048780488,\n",
       "       0.9961890243902439,\n",
       "       0.9969512195121951,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9980945121951219,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9984756097560976,\n",
       "       1.0,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244],\n",
       "      'val_loss_history': [0.3716800646348433,\n",
       "       0.6997912905432961,\n",
       "       0.2156218168410388,\n",
       "       0.1153151931410486,\n",
       "       0.067855739119378,\n",
       "       0.05144669708203186,\n",
       "       0.05696318332444538,\n",
       "       0.036261160095984284,\n",
       "       0.08526198667558757,\n",
       "       0.019548555328087372,\n",
       "       0.008701660670340061,\n",
       "       0.0051454000058583915,\n",
       "       0.005136931844224984,\n",
       "       0.004638187396763401,\n",
       "       0.003951286764773117,\n",
       "       0.0032274169127710843,\n",
       "       0.002153024426661432,\n",
       "       0.0028912076536058025,\n",
       "       0.007458921754732728,\n",
       "       0.0012876052007248456,\n",
       "       0.001047017486681315,\n",
       "       0.001048376114340499,\n",
       "       0.0013929819741117005,\n",
       "       0.000756830268073827,\n",
       "       0.0008535039378330112,\n",
       "       0.000750046993330629,\n",
       "       0.0007146922130645676,\n",
       "       0.0005893995958930728,\n",
       "       0.000699293373724107,\n",
       "       0.0005792725886832076],\n",
       "      'val_acc_history': [0.8933933933933934,\n",
       "       0.6276276276276276,\n",
       "       0.9414414414414415,\n",
       "       0.96996996996997,\n",
       "       0.9864864864864865,\n",
       "       0.9804804804804805,\n",
       "       0.9819819819819819,\n",
       "       0.9834834834834835,\n",
       "       0.978978978978979,\n",
       "       0.996996996996997,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       1.0],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9837131411308859,\n",
       "         'recall': 0.9932200580190634,\n",
       "         'f1-score': 0.9884437405656875,\n",
       "         'support': 60325.0},\n",
       "        '1': {'precision': 0.9770379519425106,\n",
       "         'recall': 0.9460723022560479,\n",
       "         'f1-score': 0.9613058248405004,\n",
       "         'support': 18395.0},\n",
       "        'accuracy': 0.982202743902439,\n",
       "        'macro avg': {'precision': 0.9803755465366983,\n",
       "         'recall': 0.9696461801375557,\n",
       "         'f1-score': 0.9748747827030939,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9821533074784449,\n",
       "         'recall': 0.982202743902439,\n",
       "         'f1-score': 0.9821022522556669,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 1.0,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 1.0,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.0005830255104228854},\n",
       "        'treatment': {'precision': 1.0,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 1.0,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.000496634456794709},\n",
       "        'accuracy': 1.0,\n",
       "        'macro avg': {'precision': 1.0,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 1.0,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 1.0,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 1.0,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.993993993993994,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.35488562031490045,\n",
       "       0.23491371114079546,\n",
       "       0.16820611314075748,\n",
       "       0.11441604865760338,\n",
       "       0.07676242882522141,\n",
       "       0.057240269010568536,\n",
       "       0.05331083321262423,\n",
       "       0.03994494092809718,\n",
       "       0.03377042669893765,\n",
       "       0.03567795668978516,\n",
       "       0.0211607112447027,\n",
       "       0.017360129700292173,\n",
       "       0.010904034872244044,\n",
       "       0.010315344447451757,\n",
       "       0.010736204712174652,\n",
       "       0.007576812805448909,\n",
       "       0.00760290797845256,\n",
       "       0.0076043427603819025,\n",
       "       0.00757261025066283,\n",
       "       0.009401661572444094,\n",
       "       0.009417129297195593,\n",
       "       0.006176381095074027,\n",
       "       0.004959143895553652,\n",
       "       0.00453146738170578,\n",
       "       0.0038838171011132258,\n",
       "       0.004042109612250564,\n",
       "       0.0035303251550919037,\n",
       "       0.0073498599614561875,\n",
       "       0.005811582597699471,\n",
       "       0.0025999539896630025],\n",
       "      'train_acc_history': [0.8509908536585366,\n",
       "       0.9005335365853658,\n",
       "       0.9432164634146342,\n",
       "       0.9641768292682927,\n",
       "       0.9782774390243902,\n",
       "       0.9817073170731707,\n",
       "       0.9828506097560976,\n",
       "       0.9851371951219512,\n",
       "       0.9897103658536586,\n",
       "       0.9874237804878049,\n",
       "       0.9954268292682927,\n",
       "       0.9965701219512195,\n",
       "       0.9992378048780488,\n",
       "       0.9980945121951219,\n",
       "       0.9977134146341463,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9973323170731707,\n",
       "       0.9973323170731707,\n",
       "       0.9969512195121951,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       1.0],\n",
       "      'val_loss_history': [0.2951633876020258,\n",
       "       10.529913219538601,\n",
       "       80.39215980876575,\n",
       "       3.4145253029736606,\n",
       "       0.07035390482368795,\n",
       "       0.06951865926384926,\n",
       "       0.04581650799478997,\n",
       "       0.033499986229633745,\n",
       "       0.04992758406495506,\n",
       "       0.03376461734825915,\n",
       "       0.020506350048394364,\n",
       "       0.024370921511118384,\n",
       "       0.03252953778825362,\n",
       "       0.024993677507154644,\n",
       "       0.03660106247248636,\n",
       "       0.029340949973133815,\n",
       "       0.02510551351059059,\n",
       "       0.04251533852022311,\n",
       "       0.03730657932729545,\n",
       "       0.04262013956692747,\n",
       "       0.04295194313057105,\n",
       "       0.036832569505680694,\n",
       "       0.03424076708192429,\n",
       "       0.03498927330259572,\n",
       "       0.04471681391111237,\n",
       "       0.04100365583276884,\n",
       "       0.04892973582239144,\n",
       "       0.04485684187834109,\n",
       "       0.04322188196767291,\n",
       "       0.047630177382168105],\n",
       "      'val_acc_history': [0.8408408408408409,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.9819819819819819,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.9954954954954955,\n",
       "       0.9804804804804805,\n",
       "       0.993993993993994,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994,\n",
       "       0.9924924924924925,\n",
       "       0.993993993993994,\n",
       "       0.993993993993994],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9844838038438222,\n",
       "         'recall': 0.9953781164582125,\n",
       "         'f1-score': 0.9899009868366859,\n",
       "         'support': 60365.0},\n",
       "        '1': {'precision': 0.9842257024933567,\n",
       "         'recall': 0.9484064287660038,\n",
       "         'f1-score': 0.9659841296265468,\n",
       "         'support': 18355.0},\n",
       "        'accuracy': 0.98442581300813,\n",
       "        'macro avg': {'precision': 0.9843547531685894,\n",
       "         'recall': 0.9718922726121082,\n",
       "         'f1-score': 0.9779425582316164,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9844236228188249,\n",
       "         'recall': 0.98442581300813,\n",
       "         'f1-score': 0.9843243365052314,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.996078431372549,\n",
       "         'recall': 0.996078431372549,\n",
       "         'f1-score': 0.996078431372549,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.018080944195389748},\n",
       "        'treatment': {'precision': 0.9871794871794872,\n",
       "         'recall': 0.9871794871794872,\n",
       "         'f1-score': 0.9871794871794872,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.08774354308843613},\n",
       "        'accuracy': 0.993993993993994,\n",
       "        'macro avg': {'precision': 0.9916289592760181,\n",
       "         'recall': 0.9916289592760181,\n",
       "         'f1-score': 0.9916289592760181,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.993993993993994,\n",
       "         'recall': 0.993993993993994,\n",
       "         'f1-score': 0.993993993993994,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.984984984984985,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4115224880416219,\n",
       "       0.25293644390455106,\n",
       "       0.17104868136528062,\n",
       "       0.111232030500726,\n",
       "       0.07956452931209308,\n",
       "       0.057242725089919275,\n",
       "       0.03930159244777226,\n",
       "       0.03125066664524195,\n",
       "       0.022989647724188683,\n",
       "       0.019848667369110555,\n",
       "       0.015162487332595558,\n",
       "       0.011514227542026741,\n",
       "       0.009921871540250211,\n",
       "       0.009729287070335775,\n",
       "       0.009704509943087653,\n",
       "       0.0057133334382187305,\n",
       "       0.007716461868456951,\n",
       "       0.00573469265988778,\n",
       "       0.005202578724271096,\n",
       "       0.009264071885121577,\n",
       "       0.006589455419310891,\n",
       "       0.004665469351826553,\n",
       "       0.0035973249099830666,\n",
       "       0.002792434867687251,\n",
       "       0.0034345851802244417,\n",
       "       0.003909235874122781,\n",
       "       0.0030208506916717787,\n",
       "       0.003516208403175924,\n",
       "       0.0030502371245813443,\n",
       "       0.002939132811706059],\n",
       "      'train_acc_history': [0.805640243902439,\n",
       "       0.897484756097561,\n",
       "       0.944359756097561,\n",
       "       0.9649390243902439,\n",
       "       0.9748475609756098,\n",
       "       0.9801829268292683,\n",
       "       0.9897103658536586,\n",
       "       0.9923780487804879,\n",
       "       0.995045731707317,\n",
       "       0.9958079268292683,\n",
       "       0.9961890243902439,\n",
       "       0.9969512195121951,\n",
       "       0.9984756097560976,\n",
       "       0.9980945121951219,\n",
       "       0.9980945121951219,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9977134146341463,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9996189024390244],\n",
       "      'val_loss_history': [0.3285089826042002,\n",
       "       9.324731956828725,\n",
       "       37.97664416920055,\n",
       "       5.171586860309947,\n",
       "       0.14686440100724046,\n",
       "       0.09452556378462097,\n",
       "       0.08574150756678799,\n",
       "       0.09179752091453834,\n",
       "       0.08708242453973401,\n",
       "       0.07863971129567786,\n",
       "       0.07631595180877908,\n",
       "       0.0818237951237031,\n",
       "       0.08723064052703028,\n",
       "       0.07971607949796387,\n",
       "       0.08805364339654757,\n",
       "       0.09573855308223177,\n",
       "       0.09354276143924588,\n",
       "       0.09063565942712805,\n",
       "       0.0986470767212185,\n",
       "       0.09207975308791819,\n",
       "       0.09919062879635021,\n",
       "       0.09592759566889568,\n",
       "       0.09700009858087552,\n",
       "       0.09784588337060995,\n",
       "       0.10374943929756145,\n",
       "       0.0962292224417483,\n",
       "       0.09558497486498461,\n",
       "       0.09166611547962847,\n",
       "       0.09743233102331446,\n",
       "       0.09185290279459547],\n",
       "      'val_acc_history': [0.8603603603603603,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.9564564564564565,\n",
       "       0.975975975975976,\n",
       "       0.9774774774774775,\n",
       "       0.9804804804804805,\n",
       "       0.9819819819819819,\n",
       "       0.978978978978979,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9862693447481067,\n",
       "         'recall': 0.9929226128321151,\n",
       "         'f1-score': 0.9895847959495511,\n",
       "         'support': 60333.0},\n",
       "        '1': {'precision': 0.9762513904338154,\n",
       "         'recall': 0.9546418665361397,\n",
       "         'f1-score': 0.9653257073720681,\n",
       "         'support': 18387.0},\n",
       "        'accuracy': 0.9839811991869919,\n",
       "        'macro avg': {'precision': 0.981260367590961,\n",
       "         'recall': 0.9737822396841274,\n",
       "         'f1-score': 0.9774552516608096,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9839294041234006,\n",
       "         'recall': 0.9839811991869919,\n",
       "         'f1-score': 0.983918486731129,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9863813229571985,\n",
       "         'recall': 0.9941176470588236,\n",
       "         'f1-score': 0.990234375,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.05144967883825302},\n",
       "        'treatment': {'precision': 0.9802631578947368,\n",
       "         'recall': 0.9551282051282052,\n",
       "         'f1-score': 0.9675324675324676,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.24595972895622253},\n",
       "        'accuracy': 0.984984984984985,\n",
       "        'macro avg': {'precision': 0.9833222404259676,\n",
       "         'recall': 0.9746229260935144,\n",
       "         'f1-score': 0.9788834212662338,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9849482392488742,\n",
       "         'recall': 0.984984984984985,\n",
       "         'f1-score': 0.9849168110886861,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.9804804804804805,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.37725425893213693,\n",
       "       0.25268047266617055,\n",
       "       0.18509648922013072,\n",
       "       0.1372845990628731,\n",
       "       0.10228130421260508,\n",
       "       0.08025747201428181,\n",
       "       0.051879838199877154,\n",
       "       0.04437150190607077,\n",
       "       0.031066090970231993,\n",
       "       0.0239362906800901,\n",
       "       0.023202294256629015,\n",
       "       0.017687799761117232,\n",
       "       0.013380966671719783,\n",
       "       0.010859630905409775,\n",
       "       0.0124735459117446,\n",
       "       0.01052245036301362,\n",
       "       0.0071671061422221545,\n",
       "       0.008803588908915295,\n",
       "       0.006811341373022736,\n",
       "       0.007132535030860908,\n",
       "       0.0044515182536731404,\n",
       "       0.0050906209082047385,\n",
       "       0.0050117455212762806,\n",
       "       0.003765295018305684,\n",
       "       0.006068623309568842,\n",
       "       0.005136214643887176,\n",
       "       0.00443931887772434,\n",
       "       0.004253512749550637,\n",
       "       0.004100438262756187,\n",
       "       0.004389194274752787],\n",
       "      'train_acc_history': [0.8296493902439024,\n",
       "       0.897484756097561,\n",
       "       0.9283536585365854,\n",
       "       0.9538871951219512,\n",
       "       0.9596036585365854,\n",
       "       0.973704268292683,\n",
       "       0.984375,\n",
       "       0.9858993902439024,\n",
       "       0.9923780487804879,\n",
       "       0.9939024390243902,\n",
       "       0.9939024390243902,\n",
       "       0.9946646341463414,\n",
       "       0.9973323170731707,\n",
       "       0.9984756097560976,\n",
       "       0.9973323170731707,\n",
       "       0.9984756097560976,\n",
       "       1.0,\n",
       "       0.9973323170731707,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9984756097560976,\n",
       "       1.0,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732],\n",
       "      'val_loss_history': [0.3238684047352184,\n",
       "       0.3303861496123401,\n",
       "       0.19393917173147202,\n",
       "       0.1314720054241744,\n",
       "       0.14352154325355182,\n",
       "       0.12528434209525585,\n",
       "       0.14609227668155322,\n",
       "       0.11298778113401071,\n",
       "       0.11394069189290432,\n",
       "       0.13553707826543937,\n",
       "       0.11393470799719746,\n",
       "       0.12689681115179238,\n",
       "       0.12437941788018426,\n",
       "       0.11987991059537638,\n",
       "       0.13220119063573127,\n",
       "       0.12946072115499357,\n",
       "       0.12545497422318228,\n",
       "       0.1192382186312567,\n",
       "       0.12612787797115743,\n",
       "       0.12733870800267058,\n",
       "       0.1240635223379782,\n",
       "       0.12926134384840474,\n",
       "       0.12914803413018075,\n",
       "       0.13795825683998622,\n",
       "       0.13160008310594343,\n",
       "       0.13656236489292828,\n",
       "       0.14113147843057627,\n",
       "       0.13767209148235535,\n",
       "       0.1442528937953863,\n",
       "       0.1413792726128701],\n",
       "      'val_acc_history': [0.8648648648648649,\n",
       "       0.8648648648648649,\n",
       "       0.9279279279279279,\n",
       "       0.9579579579579579,\n",
       "       0.9429429429429429,\n",
       "       0.963963963963964,\n",
       "       0.9594594594594594,\n",
       "       0.972972972972973,\n",
       "       0.975975975975976,\n",
       "       0.9774774774774775,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9832650385182757,\n",
       "         'recall': 0.9938041515497904,\n",
       "         'f1-score': 0.9885065047415817,\n",
       "         'support': 60363.0},\n",
       "        '1': {'precision': 0.9788819875776398,\n",
       "         'recall': 0.9443808901236586,\n",
       "         'f1-score': 0.9613219840851748,\n",
       "         'support': 18357.0},\n",
       "        'accuracy': 0.9822789634146342,\n",
       "        'macro avg': {'precision': 0.9810735130479578,\n",
       "         'recall': 0.9690925208367245,\n",
       "         'f1-score': 0.9749142444133783,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9822429391011359,\n",
       "         'recall': 0.9822789634146342,\n",
       "         'f1-score': 0.9821672485717435,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9788053949903661,\n",
       "         'recall': 0.996078431372549,\n",
       "         'f1-score': 0.9873663751214772,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.017248108983039856},\n",
       "        'treatment': {'precision': 0.9863945578231292,\n",
       "         'recall': 0.9294871794871795,\n",
       "         'f1-score': 0.9570957095709571,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.5814996957778931},\n",
       "        'accuracy': 0.9804804804804805,\n",
       "        'macro avg': {'precision': 0.9825999764067477,\n",
       "         'recall': 0.9627828054298643,\n",
       "         'f1-score': 0.9722310423462172,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9805830367349773,\n",
       "         'recall': 0.9804804804804805,\n",
       "         'f1-score': 0.9802759489564905,\n",
       "         'support': 666.0}}}}}},\n",
       "   'simple': {'summary': {'avg_accuracy': 0.881981981981982,\n",
       "     'std_accuracy': 0.008146942922672984,\n",
       "     'train_loss_history': [0.2341198645968631],\n",
       "     'train_loss_std': [0.002527242601366719],\n",
       "     'train_acc_history': [0.883858858858859],\n",
       "     'train_acc_std': [0.0044134950320417555],\n",
       "     'val_loss_history': [],\n",
       "     'val_loss_std': [],\n",
       "     'val_acc_history': [],\n",
       "     'val_acc_std': []},\n",
       "    'folds': {'1': {'accuracy': 0.8828828828828829,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.23714838932722734],\n",
       "      'train_acc_history': [0.8768768768768769],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8891412994093594,\n",
       "         'recall': 0.958843704066634,\n",
       "         'f1-score': 0.9226779820839227,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.8185745140388769,\n",
       "         'recall': 0.608346709470305,\n",
       "         'f1-score': 0.6979742173112339,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.8768768768768769,\n",
       "        'macro avg': {'precision': 0.8538579067241181,\n",
       "         'recall': 0.7835952067684695,\n",
       "         'f1-score': 0.8103260996975783,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8726386315092803,\n",
       "         'recall': 0.8768768768768769,\n",
       "         'f1-score': 0.8701290160728923,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8872987477638641,\n",
       "         'recall': 0.9706457925636007,\n",
       "         'f1-score': 0.9271028037383178,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.8598130841121495,\n",
       "         'recall': 0.5935483870967742,\n",
       "         'f1-score': 0.7022900763358778,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8828828828828829,\n",
       "        'macro avg': {'precision': 0.8735559159380069,\n",
       "         'recall': 0.7820970898301874,\n",
       "         'f1-score': 0.8146964400370977,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.880901934151228,\n",
       "         'recall': 0.8828828828828829,\n",
       "         'f1-score': 0.874781523336849,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.8768768768768769,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.23379102071562283],\n",
       "      'train_acc_history': [0.8825075075075075],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8970588235294118,\n",
       "         'recall': 0.9563939245467908,\n",
       "         'f1-score': 0.9257766184491345,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.8176229508196722,\n",
       "         'recall': 0.6404494382022472,\n",
       "         'f1-score': 0.7182718271827183,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.8825075075075075,\n",
       "        'macro avg': {'precision': 0.857340887174542,\n",
       "         'recall': 0.7984216813745191,\n",
       "         'f1-score': 0.8220242228159265,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8784820409850546,\n",
       "         'recall': 0.8825075075075075,\n",
       "         'f1-score': 0.8772497847558248,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8994413407821229,\n",
       "         'recall': 0.9452054794520548,\n",
       "         'f1-score': 0.9217557251908397,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.7829457364341085,\n",
       "         'recall': 0.6516129032258065,\n",
       "         'f1-score': 0.7112676056338029,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8768768768768769,\n",
       "        'macro avg': {'precision': 0.8411935386081157,\n",
       "         'recall': 0.7984091913389306,\n",
       "         'f1-score': 0.8165116654123212,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8723290004308584,\n",
       "         'recall': 0.8768768768768769,\n",
       "         'f1-score': 0.8727682499185564,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.8693693693693694,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.22960848290765817],\n",
       "      'train_acc_history': [0.8903903903903904],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.9017447199265382,\n",
       "         'recall': 0.9618021547502449,\n",
       "         'f1-score': 0.9308056872037914,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.8395061728395061,\n",
       "         'recall': 0.6559485530546624,\n",
       "         'f1-score': 0.7364620938628159,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8903903903903904,\n",
       "        'macro avg': {'precision': 0.8706254463830221,\n",
       "         'recall': 0.8088753539024536,\n",
       "         'f1-score': 0.8336338905333036,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8872130471457071,\n",
       "         'recall': 0.8903903903903904,\n",
       "         'f1-score': 0.8854296680378428,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8810810810810811,\n",
       "         'recall': 0.9588235294117647,\n",
       "         'f1-score': 0.9183098591549296,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8108108108108109,\n",
       "         'recall': 0.5769230769230769,\n",
       "         'f1-score': 0.6741573033707865,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8693693693693694,\n",
       "        'macro avg': {'precision': 0.845945945945946,\n",
       "         'recall': 0.7678733031674208,\n",
       "         'f1-score': 0.796233581262858,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8646213781348916,\n",
       "         'recall': 0.8693693693693694,\n",
       "         'f1-score': 0.8611209722144997,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.8918918918918919,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.23445083208208617],\n",
       "      'train_acc_history': [0.8836336336336337],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8954337899543379,\n",
       "         'recall': 0.9603330068560235,\n",
       "         'f1-score': 0.9267485822306238,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.8291139240506329,\n",
       "         'recall': 0.6318327974276527,\n",
       "         'f1-score': 0.7171532846715328,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8836336336336337,\n",
       "        'macro avg': {'precision': 0.8622738570024854,\n",
       "         'recall': 0.7960829021418381,\n",
       "         'f1-score': 0.8219509334510784,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8799491966389833,\n",
       "         'recall': 0.8836336336336337,\n",
       "         'f1-score': 0.87781154203477,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9025735294117647,\n",
       "         'recall': 0.9627450980392157,\n",
       "         'f1-score': 0.9316888045540797,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8442622950819673,\n",
       "         'recall': 0.6602564102564102,\n",
       "         'f1-score': 0.7410071942446043,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8918918918918919,\n",
       "        'macro avg': {'precision': 0.8734179122468659,\n",
       "         'recall': 0.811500754147813,\n",
       "         'f1-score': 0.8363479993993419,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8889150420912717,\n",
       "         'recall': 0.8918918918918919,\n",
       "         'f1-score': 0.887024643580689,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.8888888888888888,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.235600597951721],\n",
       "      'train_acc_history': [0.8858858858858859],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8968036529680365,\n",
       "         'recall': 0.9618021547502449,\n",
       "         'f1-score': 0.9281663516068053,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.8354430379746836,\n",
       "         'recall': 0.6366559485530546,\n",
       "         'f1-score': 0.7226277372262774,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8858858858858859,\n",
       "        'macro avg': {'precision': 0.86612334547136,\n",
       "         'recall': 0.7992290516516498,\n",
       "         'f1-score': 0.8253970444165413,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8824769628306998,\n",
       "         'recall': 0.8858858858858859,\n",
       "         'f1-score': 0.8801764799308712,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9052044609665427,\n",
       "         'recall': 0.9549019607843138,\n",
       "         'f1-score': 0.9293893129770993,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8203125,\n",
       "         'recall': 0.6730769230769231,\n",
       "         'f1-score': 0.7394366197183099,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8888888888888888,\n",
       "        'macro avg': {'precision': 0.8627584804832713,\n",
       "         'recall': 0.8139894419306184,\n",
       "         'f1-score': 0.8344129663477046,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8853198574969021,\n",
       "         'recall': 0.8888888888888888,\n",
       "         'f1-score': 0.8848958893308965,\n",
       "         'support': 666.0}}}}}}}},\n",
       " 'fixed': {'agnostic': {'nn': {'summary': {'avg_accuracy': 0.7663663663663662,\n",
       "     'std_accuracy': 0.0007355825053403044,\n",
       "     'train_loss_history': [0.5982840786619885,\n",
       "      0.5561831112315014,\n",
       "      0.5533925810965096,\n",
       "      0.5538795439208426,\n",
       "      0.552699142549096,\n",
       "      0.5491921215522579,\n",
       "      0.5489829611487506,\n",
       "      0.550246408363668,\n",
       "      0.5496943883779573,\n",
       "      0.5462471884925192,\n",
       "      0.5488559496111985,\n",
       "      0.5472664055300923,\n",
       "      0.5476768458761819,\n",
       "      0.5479967182729303,\n",
       "      0.5482426450019929,\n",
       "      0.5471070378291898,\n",
       "      0.5469488434675263,\n",
       "      0.5460670093210732,\n",
       "      0.546048475474846,\n",
       "      0.5454737214053549,\n",
       "      0.545591233125547,\n",
       "      0.5449664396483722,\n",
       "      0.5451020041616952,\n",
       "      0.5450171316542276,\n",
       "      0.5452708859269212,\n",
       "      0.5453794999820432,\n",
       "      0.545223962679142,\n",
       "      0.5452848542027358,\n",
       "      0.5461616075620419,\n",
       "      0.5455692624173514],\n",
       "     'train_loss_std': [0.01957021251770383,\n",
       "      0.004018785898181684,\n",
       "      0.002238153685116329,\n",
       "      0.0023970644104368646,\n",
       "      0.0020485705908193028,\n",
       "      0.0033111386182378528,\n",
       "      0.0009391461627260621,\n",
       "      0.0011796744056644006,\n",
       "      0.001376042295614182,\n",
       "      0.0011142676578756502,\n",
       "      0.0008778196051351697,\n",
       "      0.0013453015397376087,\n",
       "      0.00173463979928769,\n",
       "      0.0006002438878482178,\n",
       "      0.001708539468474514,\n",
       "      0.0012202860853823677,\n",
       "      0.0011760790869032216,\n",
       "      0.0007635243957141881,\n",
       "      0.002035176857413994,\n",
       "      0.0005480518765384381,\n",
       "      0.0016186980481593418,\n",
       "      0.001577808571655271,\n",
       "      0.0007615119196913256,\n",
       "      0.0011383520157388929,\n",
       "      0.0008620062290576872,\n",
       "      0.0014735819995764763,\n",
       "      0.002104970789080092,\n",
       "      0.0016561406133755372,\n",
       "      0.002568837611465283,\n",
       "      0.0016016463477922056],\n",
       "     'train_acc_history': [0.7130335365853658,\n",
       "      0.7654725609756097,\n",
       "      0.7661585365853659,\n",
       "      0.7661585365853658,\n",
       "      0.7656249999999999,\n",
       "      0.7667682926829269,\n",
       "      0.7663871951219512,\n",
       "      0.7660060975609756,\n",
       "      0.7666920731707317,\n",
       "      0.7667682926829269,\n",
       "      0.7660823170731708,\n",
       "      0.7661585365853659,\n",
       "      0.7667682926829268,\n",
       "      0.766234756097561,\n",
       "      0.7652439024390244,\n",
       "      0.7661585365853658,\n",
       "      0.7663871951219512,\n",
       "      0.766844512195122,\n",
       "      0.7661585365853659,\n",
       "      0.7660823170731706,\n",
       "      0.7669969512195122,\n",
       "      0.7663871951219512,\n",
       "      0.766234756097561,\n",
       "      0.7666920731707317,\n",
       "      0.766310975609756,\n",
       "      0.766234756097561,\n",
       "      0.7666920731707318,\n",
       "      0.7665396341463415,\n",
       "      0.7661585365853659,\n",
       "      0.7661585365853658],\n",
       "     'train_acc_std': [0.030044413015077903,\n",
       "      0.0021368667330604,\n",
       "      0.00045731707317071547,\n",
       "      0.0007847279070873959,\n",
       "      0.0007230817820506708,\n",
       "      0.0013845961985201939,\n",
       "      0.0009334945666094369,\n",
       "      0.0004174714615130691,\n",
       "      0.0008487445675045901,\n",
       "      0.0012979715217931703,\n",
       "      0.0009146341463414308,\n",
       "      0.00018669891332188194,\n",
       "      0.0007230817820507059,\n",
       "      0.001170906364004344,\n",
       "      0.001155926134763944,\n",
       "      0.0010939557998786115,\n",
       "      0.0007993969879345383,\n",
       "      0.0008487445675046002,\n",
       "      0.0010111660946205512,\n",
       "      0.0003733978266437639,\n",
       "      0.0007068306780103589,\n",
       "      0.0006376981909558276,\n",
       "      0.0007467956532875278,\n",
       "      0.00044443230905831505,\n",
       "      0.00028518730082117445,\n",
       "      0.0005169458828601393,\n",
       "      0.0009760860118037897,\n",
       "      0.000784727907087424,\n",
       "      0.0013971267362669966,\n",
       "      0.0013761791223527132],\n",
       "     'val_loss_history': [0.5571288368918679,\n",
       "      10.160537600517273,\n",
       "      74.9867031140761,\n",
       "      80.61911083568226,\n",
       "      78.8689570687034,\n",
       "      26.877096713672984,\n",
       "      37.48222644329071,\n",
       "      22.82138554778966,\n",
       "      41.29414440664378,\n",
       "      12.386321559277448,\n",
       "      3.334840259768746,\n",
       "      8.852668105472219,\n",
       "      7.478271813826128,\n",
       "      3.821507955139334,\n",
       "      11.86966586221348,\n",
       "      3.932693860747597,\n",
       "      6.541172681071542,\n",
       "      4.526965277032419,\n",
       "      6.734867790612308,\n",
       "      2.8754954332655123,\n",
       "      1.9165691421790556,\n",
       "      1.1822862966494125,\n",
       "      1.614201732115312,\n",
       "      2.055245734344829,\n",
       "      2.4593855091116645,\n",
       "      1.0812303486195478,\n",
       "      1.8735000572421334,\n",
       "      4.1695293269374165,\n",
       "      3.6888961629434065,\n",
       "      4.799049090255391],\n",
       "     'val_loss_std': [0.007666411606059596,\n",
       "      5.76824243489729,\n",
       "      84.79873155567071,\n",
       "      71.13908879940118,\n",
       "      98.2243763691128,\n",
       "      8.20908704174776,\n",
       "      29.682968725291634,\n",
       "      23.066409555959023,\n",
       "      71.49614349542252,\n",
       "      10.109124906847608,\n",
       "      1.8816266440811535,\n",
       "      3.1673370049348795,\n",
       "      3.9475797822627228,\n",
       "      3.562964025619672,\n",
       "      9.592688206282078,\n",
       "      3.9636529078985827,\n",
       "      4.644942476195786,\n",
       "      5.504471142230162,\n",
       "      3.088320533412552,\n",
       "      1.7392981468972066,\n",
       "      0.9502183730946318,\n",
       "      0.3820311348548063,\n",
       "      0.6873721379750908,\n",
       "      1.1710471102495708,\n",
       "      2.2961356422336783,\n",
       "      0.565100016036965,\n",
       "      0.981709014589048,\n",
       "      2.1654847777889943,\n",
       "      2.4018103520239786,\n",
       "      4.5270697030968545],\n",
       "     'val_acc_history': [0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.6594594594594595,\n",
       "      0.66006006006006,\n",
       "      0.66006006006006,\n",
       "      0.66006006006006,\n",
       "      0.66006006006006,\n",
       "      0.553153153153153,\n",
       "      0.553153153153153,\n",
       "      0.5537537537537537,\n",
       "      0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.6594594594594595,\n",
       "      0.66006006006006,\n",
       "      0.553153153153153,\n",
       "      0.66006006006006,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.553153153153153,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.66006006006006,\n",
       "      0.6594594594594595,\n",
       "      0.7663663663663662],\n",
       "     'val_acc_std': [0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2133641558503688,\n",
       "      0.2129139717970933,\n",
       "      0.2129139717970933,\n",
       "      0.2129139717970933,\n",
       "      0.2129139717970933,\n",
       "      0.2610102000357858,\n",
       "      0.2610102000357858,\n",
       "      0.260887171340839,\n",
       "      0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.2133641558503688,\n",
       "      0.2129139717970933,\n",
       "      0.2610102000357858,\n",
       "      0.2129139717970933,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2610102000357858,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.2129139717970933,\n",
       "      0.2133641558503688,\n",
       "      0.0007355825053403044]},\n",
       "    'folds': {'1': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5785143157330955,\n",
       "       0.5567012901713209,\n",
       "       0.556840692351504,\n",
       "       0.5506593992070454,\n",
       "       0.5530765020265812,\n",
       "       0.5531625871251269,\n",
       "       0.5476051067433706,\n",
       "       0.5483657172540339,\n",
       "       0.5514140078207341,\n",
       "       0.5463478943196739,\n",
       "       0.549678037806255,\n",
       "       0.5458032343445754,\n",
       "       0.5468264193069644,\n",
       "       0.5485675233166393,\n",
       "       0.5462065673455959,\n",
       "       0.5475865864172215,\n",
       "       0.5479558524562091,\n",
       "       0.5465003644547811,\n",
       "       0.5460304116330496,\n",
       "       0.545506560220951,\n",
       "       0.5429502129554749,\n",
       "       0.5460526710603295,\n",
       "       0.5455252528190613,\n",
       "       0.5447884754436773,\n",
       "       0.545840166690873,\n",
       "       0.5469643393667732,\n",
       "       0.5434311744643421,\n",
       "       0.5471853228603921,\n",
       "       0.5440394922000605,\n",
       "       0.5460214374995813],\n",
       "      'train_acc_history': [0.743140243902439,\n",
       "       0.7660060975609756,\n",
       "       0.7652439024390244,\n",
       "       0.7671493902439024,\n",
       "       0.7644817073170732,\n",
       "       0.7652439024390244,\n",
       "       0.7663871951219512,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.7652439024390244,\n",
       "       0.7667682926829268,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.765625,\n",
       "       0.7652439024390244,\n",
       "       0.7675304878048781,\n",
       "       0.7660060975609756,\n",
       "       0.765625,\n",
       "       0.7671493902439024,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.7667682926829268,\n",
       "       0.7652439024390244,\n",
       "       0.7671493902439024,\n",
       "       0.7644817073170732],\n",
       "      'val_loss_history': [0.565643700686368,\n",
       "       6.962198387492787,\n",
       "       217.62780068137428,\n",
       "       35.011181051080875,\n",
       "       5.1317525776949795,\n",
       "       20.451200225136496,\n",
       "       78.3795464255593,\n",
       "       9.439102953130549,\n",
       "       1.035811418836767,\n",
       "       2.831525065682151,\n",
       "       0.5595219460400668,\n",
       "       9.050452535802668,\n",
       "       0.8459408283233643,\n",
       "       0.7800941575657238,\n",
       "       5.06954702464017,\n",
       "       2.6193203059109775,\n",
       "       1.1848023317076943,\n",
       "       15.305543552745473,\n",
       "       7.170950174331665,\n",
       "       1.0037160976366564,\n",
       "       1.1436290470036594,\n",
       "       0.6604846282438799,\n",
       "       0.5448874804106626,\n",
       "       2.5020041465759277,\n",
       "       0.900580883026123,\n",
       "       0.7859131748026068,\n",
       "       1.0684697953137485,\n",
       "       1.1380697678435931,\n",
       "       2.401554671200839,\n",
       "       1.4376900846307927],\n",
       "      'val_acc_history': [0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7661103620220318,\n",
       "         'recall': 0.9985906618914976,\n",
       "         'f1-score': 0.867037127679484,\n",
       "         'support': 60312.0},\n",
       "        '1': {'precision': 0.19811320754716982,\n",
       "         'recall': 0.0011408083441981746,\n",
       "         'f1-score': 0.0022685535270606026,\n",
       "         'support': 18408.0},\n",
       "        'accuracy': 0.7653455284552846,\n",
       "        'macro avg': {'precision': 0.4821117847846008,\n",
       "         'recall': 0.4998657351178479,\n",
       "         'f1-score': 0.4346528406032723,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6332890762042821,\n",
       "         'recall': 0.7653455284552846,\n",
       "         'f1-score': 0.6648183787846949,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.0020863707177340984},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0,\n",
       "         'loss': 6.173362731933594},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.6011241369131135,\n",
       "       0.5554766676774839,\n",
       "       0.5511678842509665,\n",
       "       0.5575120986961737,\n",
       "       0.5522613212829683,\n",
       "       0.5494289957895512,\n",
       "       0.5504519794045425,\n",
       "       0.5497496978538793,\n",
       "       0.550591140258603,\n",
       "       0.5457463228121037,\n",
       "       0.5493676786015673,\n",
       "       0.5468389173833336,\n",
       "       0.5499328055032869,\n",
       "       0.5487234977687278,\n",
       "       0.5483003760256419,\n",
       "       0.5466897727512732,\n",
       "       0.5449679704701028,\n",
       "       0.5469530791771121,\n",
       "       0.5491107338812293,\n",
       "       0.5450221445502305,\n",
       "       0.5449649269987897,\n",
       "       0.5449878733332564,\n",
       "       0.5462688941781114,\n",
       "       0.5442708439943267,\n",
       "       0.5461756717867967,\n",
       "       0.5434676953932134,\n",
       "       0.5471262757371111,\n",
       "       0.5460135704133569,\n",
       "       0.5434163997812969,\n",
       "       0.5458394637921962],\n",
       "      'train_acc_history': [0.7035060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7652439024390244,\n",
       "       0.7663871951219512,\n",
       "       0.7667682926829268,\n",
       "       0.7648628048780488,\n",
       "       0.7660060975609756,\n",
       "       0.7652439024390244,\n",
       "       0.7679115853658537,\n",
       "       0.7644817073170732,\n",
       "       0.7663871951219512,\n",
       "       0.765625,\n",
       "       0.7641006097560976,\n",
       "       0.7644817073170732,\n",
       "       0.7660060975609756,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.7682926829268293,\n",
       "       0.7652439024390244,\n",
       "       0.7648628048780488,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7667682926829268,\n",
       "       0.7671493902439024,\n",
       "       0.7675304878048781,\n",
       "       0.7652439024390244],\n",
       "      'val_loss_history': [0.5448325628584082,\n",
       "       6.229614214463667,\n",
       "       9.535668806596236,\n",
       "       87.12744279341264,\n",
       "       12.273801240054043,\n",
       "       21.091412024064496,\n",
       "       61.101938941261984,\n",
       "       43.16190026023171,\n",
       "       10.383839563889937,\n",
       "       0.5349538841030814,\n",
       "       3.0635906457901,\n",
       "       6.667400121688843,\n",
       "       11.159605503082275,\n",
       "       1.3587786067615857,\n",
       "       6.162503350864757,\n",
       "       3.3502855084159155,\n",
       "       14.548236500133168,\n",
       "       3.3839796022935347,\n",
       "       4.308435526761142,\n",
       "       0.9535225792364641,\n",
       "       1.5913627472790806,\n",
       "       1.4760565161705017,\n",
       "       1.091331590305675,\n",
       "       2.5822372436523438,\n",
       "       6.948774099349976,\n",
       "       1.0659493533047764,\n",
       "       1.862896740436554,\n",
       "       2.423029563643716,\n",
       "       4.246746756813743,\n",
       "       1.68667380918156],\n",
       "      'val_acc_history': [0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.23273273273273273,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7658858613921954,\n",
       "         'recall': 0.9965012353458139,\n",
       "         'f1-score': 0.8661050780771475,\n",
       "         'support': 60307.0},\n",
       "        '1': {'precision': 0.16929133858267717,\n",
       "         'recall': 0.002335306576875034,\n",
       "         'f1-score': 0.004607060588203782,\n",
       "         'support': 18413.0},\n",
       "        'accuracy': 0.7639608739837398,\n",
       "        'macro avg': {'precision': 0.4675885999874363,\n",
       "         'recall': 0.4994182709613445,\n",
       "         'f1-score': 0.4353560693326757,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.62633943166034,\n",
       "         'recall': 0.7639608739837398,\n",
       "         'f1-score': 0.6645964018065185,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.0005827456479892135},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0,\n",
       "         'loss': 7.44802713394165},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5847658653084825,\n",
       "       0.5624938519989572,\n",
       "       0.554126567229992,\n",
       "       0.5522128488959336,\n",
       "       0.5494923736990952,\n",
       "       0.5449242700890797,\n",
       "       0.5490983840895862,\n",
       "       0.5519533339070111,\n",
       "       0.5490084704829425,\n",
       "       0.5447590285685004,\n",
       "       0.5473774744243156,\n",
       "       0.5473746446574607,\n",
       "       0.545020514872016,\n",
       "       0.5480660883391776,\n",
       "       0.5474524214500334,\n",
       "       0.5449658930301666,\n",
       "       0.5467486555983381,\n",
       "       0.5449482289756217,\n",
       "       0.5450081076563859,\n",
       "       0.5448756079848219,\n",
       "       0.5456935152774904,\n",
       "       0.5419114639119404,\n",
       "       0.5442573813403525,\n",
       "       0.5472659657641155,\n",
       "       0.5453202855296251,\n",
       "       0.5471520431158019,\n",
       "       0.5477400778270349,\n",
       "       0.5422119469177432,\n",
       "       0.5487860106840367,\n",
       "       0.5430639407983641],\n",
       "      'train_acc_history': [0.7358993902439024,\n",
       "       0.7625762195121951,\n",
       "       0.7663871951219512,\n",
       "       0.7667682926829268,\n",
       "       0.765625,\n",
       "       0.7686737804878049,\n",
       "       0.7660060975609756,\n",
       "       0.765625,\n",
       "       0.7675304878048781,\n",
       "       0.7682926829268293,\n",
       "       0.7660060975609756,\n",
       "       0.7663871951219512,\n",
       "       0.7679115853658537,\n",
       "       0.7667682926829268,\n",
       "       0.7648628048780488,\n",
       "       0.7682926829268293,\n",
       "       0.7671493902439024,\n",
       "       0.7675304878048781,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.765625,\n",
       "       0.7652439024390244,\n",
       "       0.7675304878048781,\n",
       "       0.7648628048780488,\n",
       "       0.7682926829268293],\n",
       "      'val_loss_history': [0.5621043579144911,\n",
       "       19.998207265680488,\n",
       "       16.517148863185536,\n",
       "       51.508697509765625,\n",
       "       15.892814983021129,\n",
       "       28.080882029099897,\n",
       "       5.8366312980651855,\n",
       "       0.9242311580614611,\n",
       "       184.02654751864347,\n",
       "       13.864755803888494,\n",
       "       4.116734531792727,\n",
       "       4.120141625404358,\n",
       "       11.716197967529297,\n",
       "       0.7336330440911379,\n",
       "       19.105206056074664,\n",
       "       0.8786565173755992,\n",
       "       3.5599915005943994,\n",
       "       0.6638471010056409,\n",
       "       10.77748879519376,\n",
       "       4.957457108931108,\n",
       "       1.1810751896012912,\n",
       "       1.607242676344785,\n",
       "       2.3885388293049554,\n",
       "       0.8444928635250438,\n",
       "       0.84429078345949,\n",
       "       0.5914466448805549,\n",
       "       0.5331290499730543,\n",
       "       6.456618829206987,\n",
       "       0.5469851873137734,\n",
       "       8.04334888674996],\n",
       "      'val_acc_history': [0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7666272068838228,\n",
       "         'recall': 0.9979288176033935,\n",
       "         'f1-score': 0.8671182682686286,\n",
       "         'support': 60352.0},\n",
       "        '1': {'precision': 0.2138364779874214,\n",
       "         'recall': 0.0018510452961672474,\n",
       "         'f1-score': 0.0036703189939007934,\n",
       "         'support': 18368.0},\n",
       "        'accuracy': 0.7655106707317073,\n",
       "        'macro avg': {'precision': 0.4902318424356221,\n",
       "         'recall': 0.49988993144978033,\n",
       "         'f1-score': 0.4353942936312647,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6376427034746625,\n",
       "         'recall': 0.7655106707317073,\n",
       "         'f1-score': 0.6656470801045254,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0,\n",
       "         'loss': 35.96126937866211},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.6343620143285612,\n",
       "       0.5563943204356403,\n",
       "       0.5541099746052812,\n",
       "       0.5536121711498354,\n",
       "       0.5527471420241565,\n",
       "       0.5459989396537223,\n",
       "       0.5492789287392686,\n",
       "       0.5507159487503331,\n",
       "       0.5474306243221935,\n",
       "       0.5462040646774012,\n",
       "       0.5495277737698904,\n",
       "       0.5497584495602584,\n",
       "       0.549137096579482,\n",
       "       0.5473778247833252,\n",
       "       0.5513574858991112,\n",
       "       0.5477617084980011,\n",
       "       0.5467457393320595,\n",
       "       0.5465455360528899,\n",
       "       0.5430165442024789,\n",
       "       0.5455227610541553,\n",
       "       0.546576069622505,\n",
       "       0.5460782167388172,\n",
       "       0.5451578752296727,\n",
       "       0.5443542577871462,\n",
       "       0.5436696170306787,\n",
       "       0.5441548395447615,\n",
       "       0.5422462406681805,\n",
       "       0.5453842485823283,\n",
       "       0.5448797098020228,\n",
       "       0.5449305714630499],\n",
       "      'train_acc_history': [0.6592987804878049,\n",
       "       0.7637195121951219,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7679115853658537,\n",
       "       0.7671493902439024,\n",
       "       0.7667682926829268,\n",
       "       0.7667682926829268,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7675304878048781,\n",
       "       0.7637195121951219,\n",
       "       0.765625,\n",
       "       0.7667682926829268,\n",
       "       0.765625,\n",
       "       0.7679115853658537,\n",
       "       0.7663871951219512,\n",
       "       0.7667682926829268,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7667682926829268,\n",
       "       0.7667682926829268,\n",
       "       0.765625,\n",
       "       0.7682926829268293,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7671493902439024],\n",
       "      'val_loss_history': [0.5518042390996759,\n",
       "       4.32113102349368,\n",
       "       3.4577818350358442,\n",
       "       14.70489536632191,\n",
       "       97.65318228981711,\n",
       "       22.37738596309315,\n",
       "       3.2532200921665537,\n",
       "       3.120981454849243,\n",
       "       0.5581117678772319,\n",
       "       28.614370519464668,\n",
       "       6.304416049610484,\n",
       "       11.782830498435281,\n",
       "       5.943519657308405,\n",
       "       7.192322167483243,\n",
       "       27.056160840121183,\n",
       "       1.1678003939715298,\n",
       "       8.457362911917947,\n",
       "       0.5390397364442999,\n",
       "       2.305631859736009,\n",
       "       2.699747611175884,\n",
       "       1.9419553496620872,\n",
       "       0.7912632579153235,\n",
       "       2.0990894707766445,\n",
       "       0.6108275326815519,\n",
       "       2.190823186527599,\n",
       "       0.792563262310895,\n",
       "       2.7936979315497656,\n",
       "       4.216294451193376,\n",
       "       7.80514045195146,\n",
       "       12.147267948497426],\n",
       "      'val_acc_history': [0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.23423423423423423,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7665102481811557,\n",
       "         'recall': 0.9933882380234311,\n",
       "         'f1-score': 0.8653252114668437,\n",
       "         'support': 60347.0},\n",
       "        '1': {'precision': 0.2191780821917808,\n",
       "         'recall': 0.0060959015947313995,\n",
       "         'f1-score': 0.01186189366659606,\n",
       "         'support': 18373.0},\n",
       "        'accuracy': 0.7629573170731707,\n",
       "        'macro avg': {'precision': 0.49284416518646823,\n",
       "         'recall': 0.49974206980908126,\n",
       "         'f1-score': 0.4385935525667199,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6387646449580512,\n",
       "         'recall': 0.7629573170731707,\n",
       "         'f1-score': 0.6661295618486532,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0,\n",
       "         'loss': 53.31979751586914},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5926540610266895,\n",
       "       0.5498494258741053,\n",
       "       0.5507177870448042,\n",
       "       0.5554012016552251,\n",
       "       0.5559183737126793,\n",
       "       0.55244581510381,\n",
       "       0.5484804067669845,\n",
       "       0.5504473440530824,\n",
       "       0.550027699005313,\n",
       "       0.5481786320849162,\n",
       "       0.5483287834539646,\n",
       "       0.5465567817048329,\n",
       "       0.5474673931191607,\n",
       "       0.5472486571567815,\n",
       "       0.5478963742895824,\n",
       "       0.5485312284492865,\n",
       "       0.5483259994809221,\n",
       "       0.5453878379449612,\n",
       "       0.5470765800010867,\n",
       "       0.546441533216616,\n",
       "       0.5477714407734755,\n",
       "       0.5458019731975183,\n",
       "       0.5443006172412779,\n",
       "       0.5444061152818727,\n",
       "       0.5453486885966324,\n",
       "       0.545158582489665,\n",
       "       0.5455760446990409,\n",
       "       0.5456291822398581,\n",
       "       0.5496864253427924,\n",
       "       0.5479908985335652],\n",
       "      'train_acc_history': [0.7233231707317073,\n",
       "       0.7686737804878049,\n",
       "       0.7663871951219512,\n",
       "       0.7652439024390244,\n",
       "       0.7652439024390244,\n",
       "       0.7652439024390244,\n",
       "       0.7675304878048781,\n",
       "       0.7660060975609756,\n",
       "       0.7675304878048781,\n",
       "       0.7652439024390244,\n",
       "       0.7671493902439024,\n",
       "       0.7660060975609756,\n",
       "       0.7667682926829268,\n",
       "       0.7667682926829268,\n",
       "       0.7663871951219512,\n",
       "       0.7652439024390244,\n",
       "       0.765625,\n",
       "       0.7675304878048781,\n",
       "       0.7648628048780488,\n",
       "       0.765625,\n",
       "       0.7663871951219512,\n",
       "       0.7667682926829268,\n",
       "       0.7663871951219512,\n",
       "       0.7671493902439024,\n",
       "       0.7663871951219512,\n",
       "       0.7667682926829268,\n",
       "       0.7663871951219512,\n",
       "       0.7663871951219512,\n",
       "       0.7641006097560976,\n",
       "       0.765625],\n",
       "      'val_loss_history': [0.5612593239003961,\n",
       "       13.291537111455744,\n",
       "       127.79511538418856,\n",
       "       214.74333745783025,\n",
       "       263.3932342529297,\n",
       "       42.38460332697088,\n",
       "       38.83979545940053,\n",
       "       57.460711912675336,\n",
       "       10.466411763971502,\n",
       "       16.086002523248847,\n",
       "       2.6299381256103516,\n",
       "       12.642515746029941,\n",
       "       7.726095112887296,\n",
       "       9.042711799794978,\n",
       "       1.9549120393666355,\n",
       "       11.647406578063965,\n",
       "       4.9554701610044996,\n",
       "       2.7424163926731455,\n",
       "       9.111832597038962,\n",
       "       4.763033769347451,\n",
       "       3.72482337734916,\n",
       "       1.3763844045725735,\n",
       "       1.9471612897786228,\n",
       "       3.736666885289279,\n",
       "       1.412458593195135,\n",
       "       2.1702793077989058,\n",
       "       3.1093067689375444,\n",
       "       6.613634022799405,\n",
       "       3.444053747437217,\n",
       "       0.680264722217213],\n",
       "      'val_acc_history': [0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.7662785958118917,\n",
       "         'recall': 0.9972813023224972,\n",
       "         'f1-score': 0.8666508200618017,\n",
       "         'support': 60323.0},\n",
       "        '1': {'precision': 0.22641509433962265,\n",
       "         'recall': 0.002609121052345491,\n",
       "         'f1-score': 0.005158794131871675,\n",
       "         'support': 18397.0},\n",
       "        'accuracy': 0.7648246951219512,\n",
       "        'macro avg': {'precision': 0.4963468450757572,\n",
       "         'recall': 0.49994521168742134,\n",
       "         'f1-score': 0.4359048070968367,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.6401115628268138,\n",
       "         'recall': 0.7648246951219512,\n",
       "         'f1-score': 0.6653186452519323,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.6686566472053528},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.7182523012161255},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}}}},\n",
       "   'simple': {'summary': {'avg_accuracy': 0.7663663663663662,\n",
       "     'std_accuracy': 0.0007355825053403044,\n",
       "     'train_loss_history': [0.5436296673986601],\n",
       "     'train_loss_std': [0.00021841220568158848],\n",
       "     'train_acc_history': [0.7663663663663663],\n",
       "     'train_acc_std': [0.00018389562633506248],\n",
       "     'val_loss_history': [],\n",
       "     'val_loss_std': [],\n",
       "     'val_acc_history': [],\n",
       "     'val_acc_std': []},\n",
       "    'folds': {'1': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5438971666274179],\n",
       "      'train_acc_history': [0.7661411411411412],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7661411411411412,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8675876726886291,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.7661411411411412,\n",
       "        'macro avg': {'precision': 0.3830705705705706,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43379383634431457,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.58697224814905,\n",
       "         'recall': 0.7661411411411412,\n",
       "         'f1-score': 0.6646946095936532,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.7672672672672672,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.543897166627418],\n",
       "      'train_acc_history': [0.7661411411411412],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7661411411411412,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8675876726886291,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.7661411411411412,\n",
       "        'macro avg': {'precision': 0.3830705705705706,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43379383634431457,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.58697224814905,\n",
       "         'recall': 0.7661411411411412,\n",
       "         'f1-score': 0.6646946095936532,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7672672672672672,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8683092608326253,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.7672672672672672,\n",
       "        'macro avg': {'precision': 0.3836336336336336,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43415463041631264,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5886990594197802,\n",
       "         'recall': 0.7672672672672672,\n",
       "         'f1-score': 0.6662252737019092,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5434513345794882],\n",
       "      'train_acc_history': [0.7665165165165165],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7665165165165165,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8678283042923927,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.7665165165165165,\n",
       "        'macro avg': {'precision': 0.38325825825825827,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43391415214619633,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.5875475700926152,\n",
       "         'recall': 0.7665165165165165,\n",
       "         'f1-score': 0.6652047287406403,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5434513345794882],\n",
       "      'train_acc_history': [0.7665165165165165],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7665165165165165,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8678283042923927,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.7665165165165165,\n",
       "        'macro avg': {'precision': 0.38325825825825827,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43391415214619633,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.5875475700926152,\n",
       "         'recall': 0.7665165165165165,\n",
       "         'f1-score': 0.6652047287406403,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.7657657657657657,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.5434513345794882],\n",
       "      'train_acc_history': [0.7665165165165165],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.7665165165165165,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8678283042923927,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.7665165165165165,\n",
       "        'macro avg': {'precision': 0.38325825825825827,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.43391415214619633,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.5875475700926152,\n",
       "         'recall': 0.7665165165165165,\n",
       "         'f1-score': 0.6652047287406403,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.7657657657657657,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.8673469387755102,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.0,\n",
       "         'recall': 0.0,\n",
       "         'f1-score': 0.0,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.7657657657657657,\n",
       "        'macro avg': {'precision': 0.38288288288288286,\n",
       "         'recall': 0.5,\n",
       "         'f1-score': 0.4336734693877551,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.5863972080188297,\n",
       "         'recall': 0.7657657657657657,\n",
       "         'f1-score': 0.6641845927560213,\n",
       "         'support': 666.0}}}}}}},\n",
       "  'population': {'nn': {'summary': {'avg_accuracy': 0.9900900900900901,\n",
       "     'std_accuracy': 0.00451450341692879,\n",
       "     'train_loss_history': [0.42549621419208805,\n",
       "      0.265897331441321,\n",
       "      0.19802825476338223,\n",
       "      0.14422292991018876,\n",
       "      0.11035891063511372,\n",
       "      0.08438138681032309,\n",
       "      0.06547086117107695,\n",
       "      0.048525015691795,\n",
       "      0.0396960022200535,\n",
       "      0.03392030977212438,\n",
       "      0.025184078682668327,\n",
       "      0.021369997083731905,\n",
       "      0.017215657066099527,\n",
       "      0.014877029895646179,\n",
       "      0.013602649221741935,\n",
       "      0.012811383084825625,\n",
       "      0.009537952977073627,\n",
       "      0.00971372574340643,\n",
       "      0.008112804430945798,\n",
       "      0.008426612876242072,\n",
       "      0.006861950936670439,\n",
       "      0.006370272590188172,\n",
       "      0.0057035468186524396,\n",
       "      0.006193036263206655,\n",
       "      0.005593319504987449,\n",
       "      0.004661583927709882,\n",
       "      0.0051911990584169575,\n",
       "      0.0043571600307165306,\n",
       "      0.004221732827562203,\n",
       "      0.0047283527015413085],\n",
       "     'train_loss_std': [0.02840407098397247,\n",
       "      0.010850820590208478,\n",
       "      0.013930276821242506,\n",
       "      0.013700396042780536,\n",
       "      0.009499111953696798,\n",
       "      0.007758410174767355,\n",
       "      0.00736666835033425,\n",
       "      0.008907808040560867,\n",
       "      0.004035408300755226,\n",
       "      0.005891486119646151,\n",
       "      0.004100646802492782,\n",
       "      0.003891805057101032,\n",
       "      0.003886913684189026,\n",
       "      0.005071090989372305,\n",
       "      0.0019396047761520636,\n",
       "      0.0026972481636464256,\n",
       "      0.0015431547834954134,\n",
       "      0.00263593550897213,\n",
       "      0.0016089961567772584,\n",
       "      0.0031587878886718034,\n",
       "      0.001449863107770146,\n",
       "      0.0015340256635699922,\n",
       "      0.0013395372812954342,\n",
       "      0.0006496150311133551,\n",
       "      0.0010100182356249847,\n",
       "      0.0008245658987032931,\n",
       "      0.0011241343606436182,\n",
       "      0.0012052164807508882,\n",
       "      0.0007135374636271761,\n",
       "      0.0012991716661189586],\n",
       "     'train_acc_history': [0.8034298780487805,\n",
       "      0.8797256097560977,\n",
       "      0.9206554878048779,\n",
       "      0.9475609756097562,\n",
       "      0.959375,\n",
       "      0.9705792682926828,\n",
       "      0.9791158536585366,\n",
       "      0.9851371951219512,\n",
       "      0.9876524390243903,\n",
       "      0.990015243902439,\n",
       "      0.9933689024390244,\n",
       "      0.9945884146341463,\n",
       "      0.9960365853658537,\n",
       "      0.9967987804878049,\n",
       "      0.9973323170731707,\n",
       "      0.9970274390243903,\n",
       "      0.9985518292682928,\n",
       "      0.9977896341463414,\n",
       "      0.9983231707317073,\n",
       "      0.9984756097560975,\n",
       "      0.998780487804878,\n",
       "      0.9984756097560975,\n",
       "      0.9991615853658538,\n",
       "      0.9985518292682928,\n",
       "      0.9992378048780488,\n",
       "      0.9990853658536587,\n",
       "      0.998704268292683,\n",
       "      0.999390243902439,\n",
       "      0.999390243902439,\n",
       "      0.998780487804878],\n",
       "     'train_acc_std': [0.023296752932630876,\n",
       "      0.006157291289213616,\n",
       "      0.007566106392517298,\n",
       "      0.006343185448647334,\n",
       "      0.004824159267819103,\n",
       "      0.004225889108423507,\n",
       "      0.0036346399443799877,\n",
       "      0.004139779150609834,\n",
       "      0.002417492610846619,\n",
       "      0.0019039628043897546,\n",
       "      0.0014974758158832484,\n",
       "      0.0008138016960389659,\n",
       "      0.001067073170731706,\n",
       "      0.001813319703257667,\n",
       "      0.0004820545213671139,\n",
       "      0.0007772895599988725,\n",
       "      0.0006985633681335261,\n",
       "      0.0008823046419809579,\n",
       "      0.0007068306780103637,\n",
       "      0.001275396381911695,\n",
       "      0.000560096739965673,\n",
       "      0.0008690361471792389,\n",
       "      0.0006097560975609928,\n",
       "      0.00028518730082121007,\n",
       "      0.0003408640209603217,\n",
       "      0.00045731707317071547,\n",
       "      0.0007467956532875685,\n",
       "      0.00030487804878047696,\n",
       "      0.0005169458828601393,\n",
       "      0.0009760860118038017],\n",
       "     'val_loss_history': [0.29068889753385024,\n",
       "      0.21403710151260552,\n",
       "      0.1544150756502693,\n",
       "      0.12058450456031344,\n",
       "      0.09565488824790175,\n",
       "      0.08283314450897954,\n",
       "      0.06872728678651832,\n",
       "      0.06106898441741412,\n",
       "      0.056727960657074375,\n",
       "      0.05222684457813474,\n",
       "      0.04911098971285603,\n",
       "      0.04723306903827258,\n",
       "      0.04772115746322512,\n",
       "      0.04526950958745808,\n",
       "      0.04441621381133287,\n",
       "      0.0451440237430771,\n",
       "      0.047726487025978384,\n",
       "      0.04455443771122108,\n",
       "      0.046579851213292306,\n",
       "      0.04755422741344029,\n",
       "      0.04766035042385656,\n",
       "      0.049009283391801134,\n",
       "      0.04687713980251415,\n",
       "      0.04681968315377493,\n",
       "      0.04877709236673334,\n",
       "      0.04794980856373017,\n",
       "      0.048081995289381174,\n",
       "      0.04963660577989438,\n",
       "      0.050387216610199,\n",
       "      0.051163249368652366],\n",
       "     'val_loss_std': [0.012400604746509974,\n",
       "      0.01717426498269534,\n",
       "      0.02653437934423359,\n",
       "      0.025054492612517004,\n",
       "      0.02124817266164332,\n",
       "      0.018403146443733323,\n",
       "      0.01783684457199312,\n",
       "      0.019500136952417285,\n",
       "      0.01937192948694162,\n",
       "      0.014918675476453452,\n",
       "      0.016381247165322594,\n",
       "      0.017644983068519113,\n",
       "      0.018822901844158992,\n",
       "      0.018509478948581168,\n",
       "      0.01677429078871038,\n",
       "      0.017013498409792573,\n",
       "      0.017197925245625974,\n",
       "      0.01667585518882741,\n",
       "      0.01718579150484861,\n",
       "      0.01741911044113396,\n",
       "      0.01613632124608232,\n",
       "      0.01837065633664015,\n",
       "      0.01651297222977934,\n",
       "      0.017880247707241097,\n",
       "      0.017775304314797267,\n",
       "      0.01846863392733248,\n",
       "      0.018264213295519187,\n",
       "      0.01857522644284335,\n",
       "      0.018384966560021046,\n",
       "      0.01909931291595555],\n",
       "     'val_acc_history': [0.8735735735735736,\n",
       "      0.9048048048048049,\n",
       "      0.9456456456456456,\n",
       "      0.9525525525525526,\n",
       "      0.9672672672672672,\n",
       "      0.9717717717717719,\n",
       "      0.978078078078078,\n",
       "      0.9795795795795795,\n",
       "      0.9819819819819819,\n",
       "      0.9855855855855855,\n",
       "      0.9873873873873874,\n",
       "      0.9885885885885886,\n",
       "      0.9885885885885886,\n",
       "      0.9897897897897898,\n",
       "      0.9894894894894894,\n",
       "      0.9906906906906908,\n",
       "      0.9897897897897898,\n",
       "      0.9903903903903905,\n",
       "      0.9900900900900901,\n",
       "      0.9903903903903905,\n",
       "      0.9906906906906908,\n",
       "      0.9900900900900902,\n",
       "      0.9906906906906908,\n",
       "      0.9906906906906906,\n",
       "      0.9900900900900901,\n",
       "      0.9903903903903905,\n",
       "      0.990990990990991,\n",
       "      0.990990990990991,\n",
       "      0.9906906906906908,\n",
       "      0.9900900900900901],\n",
       "     'val_acc_std': [0.006538000317440622,\n",
       "      0.01165381856572507,\n",
       "      0.011050071767552017,\n",
       "      0.009804701331720426,\n",
       "      0.007679706818344992,\n",
       "      0.006398581307105876,\n",
       "      0.0038690987167342597,\n",
       "      0.006124948364676023,\n",
       "      0.005930455753192649,\n",
       "      0.002784870419067788,\n",
       "      0.0033708625106071595,\n",
       "      0.0043101201484706435,\n",
       "      0.004514503416928784,\n",
       "      0.004182699182337553,\n",
       "      0.005371935081080572,\n",
       "      0.004393014666164484,\n",
       "      0.004494483347476187,\n",
       "      0.003869098716734245,\n",
       "      0.0038690987167342467,\n",
       "      0.003869098716734245,\n",
       "      0.003603603603603574,\n",
       "      0.0037507495485875936,\n",
       "      0.003726628722519752,\n",
       "      0.003344002620318302,\n",
       "      0.003750749548587594,\n",
       "      0.0038690987167342397,\n",
       "      0.0036779125267014405,\n",
       "      0.0036779125267014405,\n",
       "      0.003603603603603574,\n",
       "      0.00451450341692879]},\n",
       "    'folds': {'1': {'accuracy': 0.9924924924924925,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4362036709378405,\n",
       "       0.2833176255226135,\n",
       "       0.21615604002301286,\n",
       "       0.15394260625286801,\n",
       "       0.11477536335587502,\n",
       "       0.08671077863290542,\n",
       "       0.07376941561517192,\n",
       "       0.05853010196147895,\n",
       "       0.04238123324040959,\n",
       "       0.037291914610782774,\n",
       "       0.028998775305453597,\n",
       "       0.02186385422899592,\n",
       "       0.015646188979877566,\n",
       "       0.012710542491887038,\n",
       "       0.011708476529570251,\n",
       "       0.012676734959979246,\n",
       "       0.0075260440580455995,\n",
       "       0.007026171371363467,\n",
       "       0.009125542244873941,\n",
       "       0.00957720931165102,\n",
       "       0.00844184557691489,\n",
       "       0.008226057727503159,\n",
       "       0.005922718322835863,\n",
       "       0.006312943060836959,\n",
       "       0.006144023506061696,\n",
       "       0.006243093291342985,\n",
       "       0.004660874336855713,\n",
       "       0.004652407960171198,\n",
       "       0.0055488483276127315,\n",
       "       0.004749943979266213],\n",
       "      'train_acc_history': [0.7987804878048781,\n",
       "       0.8704268292682927,\n",
       "       0.9123475609756098,\n",
       "       0.9439786585365854,\n",
       "       0.9557926829268293,\n",
       "       0.9691310975609756,\n",
       "       0.9748475609756098,\n",
       "       0.9847560975609756,\n",
       "       0.9855182926829268,\n",
       "       0.989329268292683,\n",
       "       0.9923780487804879,\n",
       "       0.995045731707317,\n",
       "       0.9965701219512195,\n",
       "       0.9977134146341463,\n",
       "       0.9980945121951219,\n",
       "       0.9973323170731707,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       0.9973323170731707,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976,\n",
       "       0.9988567073170732],\n",
       "      'val_loss_history': [0.29613373902711,\n",
       "       0.23725888268514114,\n",
       "       0.17578872360966422,\n",
       "       0.14165107634934512,\n",
       "       0.10910118438980797,\n",
       "       0.09264190393415364,\n",
       "       0.08574767808683893,\n",
       "       0.08552331291139126,\n",
       "       0.0687008827755397,\n",
       "       0.06385006804273209,\n",
       "       0.05972299386154522,\n",
       "       0.05626429334833202,\n",
       "       0.054676743181930346,\n",
       "       0.05482347161424431,\n",
       "       0.04812554139855572,\n",
       "       0.047378795708275655,\n",
       "       0.05423998258034275,\n",
       "       0.053830342324958605,\n",
       "       0.05553302573124794,\n",
       "       0.06184790561779995,\n",
       "       0.05758172941965644,\n",
       "       0.06607695861550217,\n",
       "       0.052009833495090294,\n",
       "       0.06188254602867263,\n",
       "       0.056668723991606385,\n",
       "       0.05439498420508409,\n",
       "       0.05486360854807903,\n",
       "       0.056445199514696884,\n",
       "       0.05935780465634624,\n",
       "       0.057702326943399385],\n",
       "      'val_acc_history': [0.8663663663663663,\n",
       "       0.8858858858858859,\n",
       "       0.9369369369369369,\n",
       "       0.9459459459459459,\n",
       "       0.963963963963964,\n",
       "       0.9669669669669669,\n",
       "       0.972972972972973,\n",
       "       0.9744744744744744,\n",
       "       0.9819819819819819,\n",
       "       0.984984984984985,\n",
       "       0.9864864864864865,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.9894894894894894,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.987987987987988,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9820682393972296,\n",
       "         'recall': 0.9895731454620804,\n",
       "         'f1-score': 0.9858064089967055,\n",
       "         'support': 60325.0},\n",
       "        '1': {'precision': 0.9649269543883127,\n",
       "         'recall': 0.9407447675998912,\n",
       "         'f1-score': 0.9526824300145889,\n",
       "         'support': 18395.0},\n",
       "        'accuracy': 0.9781631097560975,\n",
       "        'macro avg': {'precision': 0.9734975968927712,\n",
       "         'recall': 0.9651589565309858,\n",
       "         'f1-score': 0.9692444195056471,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9780627269767644,\n",
       "         'recall': 0.9781631097560975,\n",
       "         'f1-score': 0.978066119446705,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.994140625,\n",
       "         'recall': 0.9960861056751468,\n",
       "         'f1-score': 0.9951124144672532,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.010622930712997913},\n",
       "        'treatment': {'precision': 0.987012987012987,\n",
       "         'recall': 0.9806451612903225,\n",
       "         'f1-score': 0.9838187702265372,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.21258503198623657},\n",
       "        'accuracy': 0.9924924924924925,\n",
       "        'macro avg': {'precision': 0.9905768060064934,\n",
       "         'recall': 0.9883656334827347,\n",
       "         'f1-score': 0.9894655923468951,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9924817903333529,\n",
       "         'recall': 0.9924924924924925,\n",
       "         'f1-score': 0.9924840137806001,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.996996996996997,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4168430712164902,\n",
       "       0.27062270045280457,\n",
       "       0.198969374524384,\n",
       "       0.14085622059135902,\n",
       "       0.11567432401565517,\n",
       "       0.07909750892985158,\n",
       "       0.06160038509746877,\n",
       "       0.03787902288320588,\n",
       "       0.032109138873837345,\n",
       "       0.03127881664237598,\n",
       "       0.01988965662468861,\n",
       "       0.01751876368596241,\n",
       "       0.014007164074516878,\n",
       "       0.0100065878217631,\n",
       "       0.012605907679421872,\n",
       "       0.011781376934178718,\n",
       "       0.008568155204468384,\n",
       "       0.008541359986383014,\n",
       "       0.007628808607256449,\n",
       "       0.005571315314893316,\n",
       "       0.008052534849678234,\n",
       "       0.003898654226781573,\n",
       "       0.0037263135493891997,\n",
       "       0.005944295385966032,\n",
       "       0.0059277805281098845,\n",
       "       0.004075538968985401,\n",
       "       0.006441009029314467,\n",
       "       0.003821308223428432,\n",
       "       0.004064992383951548,\n",
       "       0.006288826183086579],\n",
       "      'train_acc_history': [0.807545731707317,\n",
       "       0.8753810975609756,\n",
       "       0.916920731707317,\n",
       "       0.9516006097560976,\n",
       "       0.9603658536585366,\n",
       "       0.9733231707317073,\n",
       "       0.9839939024390244,\n",
       "       0.989329268292683,\n",
       "       0.9916158536585366,\n",
       "       0.9900914634146342,\n",
       "       0.995045731707317,\n",
       "       0.995045731707317,\n",
       "       0.9973323170731707,\n",
       "       0.9984756097560976,\n",
       "       0.9965701219512195,\n",
       "       0.9958079268292683,\n",
       "       0.9992378048780488,\n",
       "       0.9977134146341463,\n",
       "       0.9977134146341463,\n",
       "       1.0,\n",
       "       0.9980945121951219,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9988567073170732,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244,\n",
       "       0.9973323170731707,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9973323170731707],\n",
       "      'val_loss_history': [0.2697911397977309,\n",
       "       0.1887479695406827,\n",
       "       0.11500155959617007,\n",
       "       0.07844075416638092,\n",
       "       0.05608053014359691,\n",
       "       0.055598915215920315,\n",
       "       0.04010471058162776,\n",
       "       0.03087181073020805,\n",
       "       0.021892319729721003,\n",
       "       0.029725685334679754,\n",
       "       0.01920049006796696,\n",
       "       0.01489757918964394,\n",
       "       0.014608810518190941,\n",
       "       0.013492468681017106,\n",
       "       0.017222802916711025,\n",
       "       0.018402979635125535,\n",
       "       0.016497105391780762,\n",
       "       0.01472263266755776,\n",
       "       0.014407215010247786,\n",
       "       0.016430025147697466,\n",
       "       0.016848967283625494,\n",
       "       0.016685554524883628,\n",
       "       0.016816107484787193,\n",
       "       0.01502022699622268,\n",
       "       0.016330260622039947,\n",
       "       0.015410366359653628,\n",
       "       0.015291189266876741,\n",
       "       0.015302546975859017,\n",
       "       0.01649795179466971,\n",
       "       0.014536123841852796],\n",
       "      'val_acc_history': [0.8723723723723724,\n",
       "       0.9144144144144144,\n",
       "       0.963963963963964,\n",
       "       0.9684684684684685,\n",
       "       0.9819819819819819,\n",
       "       0.9804804804804805,\n",
       "       0.984984984984985,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.9894894894894894,\n",
       "       0.993993993993994,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9831520755524112,\n",
       "         'recall': 0.9909454237906502,\n",
       "         'f1-score': 0.9870333663693426,\n",
       "         'support': 60301.0},\n",
       "        '1': {'precision': 0.9695669137729224,\n",
       "         'recall': 0.9444052337260438,\n",
       "         'f1-score': 0.9568206820682068,\n",
       "         'support': 18419.0},\n",
       "        'accuracy': 0.9800558943089431,\n",
       "        'macro avg': {'precision': 0.9763594946626668,\n",
       "         'recall': 0.967675328758347,\n",
       "         'f1-score': 0.9719270242187747,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9799734031080971,\n",
       "         'recall': 0.9800558943089431,\n",
       "         'f1-score': 0.9799641662659048,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 1.0,\n",
       "         'recall': 0.9960861056751468,\n",
       "         'f1-score': 0.9980392156862745,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.019426845014095306},\n",
       "        'treatment': {'precision': 0.9872611464968153,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.9935897435897436,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.0019059928599745035},\n",
       "        'accuracy': 0.996996996996997,\n",
       "        'macro avg': {'precision': 0.9936305732484076,\n",
       "         'recall': 0.9980430528375734,\n",
       "         'f1-score': 0.9958144796380091,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9970352518123218,\n",
       "         'recall': 0.996996996996997,\n",
       "         'f1-score': 0.9970036778860308,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.9894894894894894,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.39382388133828233,\n",
       "       0.25333078750749916,\n",
       "       0.1732226128621799,\n",
       "       0.11912364939727434,\n",
       "       0.09179644268460391,\n",
       "       0.0721869343301145,\n",
       "       0.0550330183582335,\n",
       "       0.03967079338504047,\n",
       "       0.03939318102670879,\n",
       "       0.023595690182069452,\n",
       "       0.02317152307482391,\n",
       "       0.017135306857753455,\n",
       "       0.015609969373610689,\n",
       "       0.010469924823222001,\n",
       "       0.011846156722706993,\n",
       "       0.008394532842652463,\n",
       "       0.009165107380462492,\n",
       "       0.008253497733143954,\n",
       "       0.005586113570621464,\n",
       "       0.006550113673367333,\n",
       "       0.004581881134782168,\n",
       "       0.006330936032676752,\n",
       "       0.004722461650977138,\n",
       "       0.0050950046932529205,\n",
       "       0.0037180791671456,\n",
       "       0.0040391716969812785,\n",
       "       0.004538813705283512,\n",
       "       0.004021366622310314,\n",
       "       0.003789939892069414,\n",
       "       0.0034154155249025947],\n",
       "      'train_acc_history': [0.8338414634146342,\n",
       "       0.8814786585365854,\n",
       "       0.9348323170731707,\n",
       "       0.958079268292683,\n",
       "       0.9683689024390244,\n",
       "       0.9771341463414634,\n",
       "       0.9820884146341463,\n",
       "       0.9881859756097561,\n",
       "       0.9858993902439024,\n",
       "       0.9935213414634146,\n",
       "       0.993140243902439,\n",
       "       0.9954268292682927,\n",
       "       0.9954268292682927,\n",
       "       0.9980945121951219,\n",
       "       0.9973323170731707,\n",
       "       0.9980945121951219,\n",
       "       0.9973323170731707,\n",
       "       0.9980945121951219,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9996189024390244,\n",
       "       0.9984756097560976,\n",
       "       0.9992378048780488,\n",
       "       0.9980945121951219,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244],\n",
       "      'val_loss_history': [0.3004429719664834,\n",
       "       0.20073069564320825,\n",
       "       0.1307677902619947,\n",
       "       0.10663243992762132,\n",
       "       0.0919225261631337,\n",
       "       0.06626682042736899,\n",
       "       0.059363258494572205,\n",
       "       0.048124723813750526,\n",
       "       0.04963313495020636,\n",
       "       0.0418140430172736,\n",
       "       0.04728180769068951,\n",
       "       0.046029601341367445,\n",
       "       0.04341242653423582,\n",
       "       0.037691587045132605,\n",
       "       0.03880756996593184,\n",
       "       0.036318443996027454,\n",
       "       0.045238022165986796,\n",
       "       0.03973273796145804,\n",
       "       0.044856228024317796,\n",
       "       0.04301413132237609,\n",
       "       0.04681781084615399,\n",
       "       0.04218236650657756,\n",
       "       0.04383562953179618,\n",
       "       0.0408494894773784,\n",
       "       0.04500692455489612,\n",
       "       0.04174437631180891,\n",
       "       0.04383113168428694,\n",
       "       0.04716037673907439,\n",
       "       0.04756535126414912,\n",
       "       0.05197081292863004],\n",
       "      'val_acc_history': [0.8843843843843844,\n",
       "       0.9129129129129129,\n",
       "       0.9504504504504504,\n",
       "       0.9594594594594594,\n",
       "       0.9654654654654654,\n",
       "       0.975975975975976,\n",
       "       0.9774774774774775,\n",
       "       0.9804804804804805,\n",
       "       0.978978978978979,\n",
       "       0.987987987987988,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.987987987987988,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9924924924924925,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9894894894894894,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9894894894894894,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.990990990990991,\n",
       "       0.9894894894894894],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9836611492970685,\n",
       "         'recall': 0.9932746939654459,\n",
       "         'f1-score': 0.9884445470130555,\n",
       "         'support': 60369.0},\n",
       "        '1': {'precision': 0.9771409267496199,\n",
       "         'recall': 0.9457250286087951,\n",
       "         'f1-score': 0.9611763402747009,\n",
       "         'support': 18351.0},\n",
       "        'accuracy': 0.9821900406504065,\n",
       "        'macro avg': {'precision': 0.9804010380233442,\n",
       "         'recall': 0.9694998612871205,\n",
       "         'f1-score': 0.9748104436438783,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9821411721125128,\n",
       "         'recall': 0.9821900406504065,\n",
       "         'f1-score': 0.9820878541541181,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9941060903732809,\n",
       "         'recall': 0.9921568627450981,\n",
       "         'f1-score': 0.9931305201177625,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.0529131144285202},\n",
       "        'treatment': {'precision': 0.9745222929936306,\n",
       "         'recall': 0.9807692307692307,\n",
       "         'f1-score': 0.9776357827476039,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.061496905982494354},\n",
       "        'accuracy': 0.9894894894894894,\n",
       "        'macro avg': {'precision': 0.9843141916834557,\n",
       "         'recall': 0.9864630467571645,\n",
       "         'f1-score': 0.9853831514326832,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.98951889459066,\n",
       "         'recall': 0.9894894894894894,\n",
       "         'f1-score': 0.9895011221752028,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.987987987987988,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.47496557235717773,\n",
       "       0.26654413051721526,\n",
       "       0.19877697563752894,\n",
       "       0.15727109698260702,\n",
       "       0.11792476584271687,\n",
       "       0.09339969986822547,\n",
       "       0.0739750717653007,\n",
       "       0.05874205530598396,\n",
       "       0.04352639240735188,\n",
       "       0.039852856170022634,\n",
       "       0.03088908976443657,\n",
       "       0.02780255278935883,\n",
       "       0.024870054353391978,\n",
       "       0.023361238957632605,\n",
       "       0.015511329109757775,\n",
       "       0.01592735303338708,\n",
       "       0.011974447088815817,\n",
       "       0.014594062873185045,\n",
       "       0.010397155615841834,\n",
       "       0.014121016388100276,\n",
       "       0.0074161179874819225,\n",
       "       0.00769714892716942,\n",
       "       0.006685614140090964,\n",
       "       0.007000731525760962,\n",
       "       0.0055057362641957475,\n",
       "       0.004696353003609835,\n",
       "       0.006580193166439308,\n",
       "       0.006463262851622592,\n",
       "       0.004240177842936047,\n",
       "       0.006050441333194967],\n",
       "      'train_acc_history': [0.7629573170731707,\n",
       "       0.8879573170731707,\n",
       "       0.9195884146341463,\n",
       "       0.9413109756097561,\n",
       "       0.9554115853658537,\n",
       "       0.9649390243902439,\n",
       "       0.9752286585365854,\n",
       "       0.977515243902439,\n",
       "       0.9858993902439024,\n",
       "       0.9878048780487805,\n",
       "       0.991234756097561,\n",
       "       0.993140243902439,\n",
       "       0.9942835365853658,\n",
       "       0.9935213414634146,\n",
       "       0.9973323170731707,\n",
       "       0.9965701219512195,\n",
       "       0.9984756097560976,\n",
       "       0.9961890243902439,\n",
       "       0.9977134146341463,\n",
       "       0.9961890243902439,\n",
       "       0.9984756097560976,\n",
       "       0.9977134146341463,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       0.9992378048780488,\n",
       "       1.0,\n",
       "       0.9980945121951219],\n",
       "      'val_loss_history': [0.28376253084702924,\n",
       "       0.22233591838316483,\n",
       "       0.18166196684945712,\n",
       "       0.14553410458293828,\n",
       "       0.1156218908727169,\n",
       "       0.09854454411701723,\n",
       "       0.08854697356847199,\n",
       "       0.07537415907294913,\n",
       "       0.07490217723799023,\n",
       "       0.0711292924613438,\n",
       "       0.06702120128003033,\n",
       "       0.06748122713443908,\n",
       "       0.07144101753576913,\n",
       "       0.06760206192054531,\n",
       "       0.06893737507264384,\n",
       "       0.06865907232912088,\n",
       "       0.06784450663888658,\n",
       "       0.06312463427258824,\n",
       "       0.06392323496666821,\n",
       "       0.06513655366143212,\n",
       "       0.061464957130903546,\n",
       "       0.06556622412013398,\n",
       "       0.06492248555878177,\n",
       "       0.06396007675952702,\n",
       "       0.06790742556966672,\n",
       "       0.06884064660830931,\n",
       "       0.06916075659153814,\n",
       "       0.06931869783015414,\n",
       "       0.07010340483181855,\n",
       "       0.06807146063444881],\n",
       "      'val_acc_history': [0.8768768768768769,\n",
       "       0.9144144144144144,\n",
       "       0.9324324324324325,\n",
       "       0.9459459459459459,\n",
       "       0.9654654654654654,\n",
       "       0.972972972972973,\n",
       "       0.9774774774774775,\n",
       "       0.9774774774774775,\n",
       "       0.9819819819819819,\n",
       "       0.9834834834834835,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.9834834834834835,\n",
       "       0.9864864864864865,\n",
       "       0.984984984984985,\n",
       "       0.9864864864864865,\n",
       "       0.984984984984985,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9826718376103639,\n",
       "         'recall': 0.9870045914899965,\n",
       "         'f1-score': 0.984833449108531,\n",
       "         'support': 60329.0},\n",
       "        '1': {'precision': 0.9567448275862069,\n",
       "         'recall': 0.9429068566146485,\n",
       "         'f1-score': 0.949775440902618,\n",
       "         'support': 18391.0},\n",
       "        'accuracy': 0.9767022357723577,\n",
       "        'macro avg': {'precision': 0.9697083325982854,\n",
       "         'recall': 0.9649557240523225,\n",
       "         'f1-score': 0.9673044450055746,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9766146267191765,\n",
       "         'recall': 0.9767022357723577,\n",
       "         'f1-score': 0.976643004127396,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.990234375,\n",
       "         'recall': 0.9941176470588236,\n",
       "         'f1-score': 0.9921722113502935,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.030261443927884102},\n",
       "        'treatment': {'precision': 0.9805194805194806,\n",
       "         'recall': 0.967948717948718,\n",
       "         'f1-score': 0.9741935483870968,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.20816241204738617},\n",
       "        'accuracy': 0.987987987987988,\n",
       "        'macro avg': {'precision': 0.9853769277597403,\n",
       "         'recall': 0.9810331825037708,\n",
       "         'f1-score': 0.9831828798686952,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9879588141306892,\n",
       "         'recall': 0.987987987987988,\n",
       "         'f1-score': 0.9879609929985538,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.9834834834834835,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4056448751106495,\n",
       "       0.25567141320647263,\n",
       "       0.20301627076980544,\n",
       "       0.14992107632683543,\n",
       "       0.11162365727671762,\n",
       "       0.09051201229051846,\n",
       "       0.0629764150192098,\n",
       "       0.047803104923265734,\n",
       "       0.04107006555195988,\n",
       "       0.037582271255371044,\n",
       "       0.022971348643938943,\n",
       "       0.0225295078565889,\n",
       "       0.015944908549100523,\n",
       "       0.017836855383726154,\n",
       "       0.016341376067252784,\n",
       "       0.015276917653930624,\n",
       "       0.010456011153575851,\n",
       "       0.010153536752956669,\n",
       "       0.007826402116135308,\n",
       "       0.006313409693198415,\n",
       "       0.005817375134494974,\n",
       "       0.005698566036809963,\n",
       "       0.007460626429969036,\n",
       "       0.006612206650216405,\n",
       "       0.006670978059424315,\n",
       "       0.00425376267762991,\n",
       "       0.0037351050541917907,\n",
       "       0.0028274544960501173,\n",
       "       0.003464705691241273,\n",
       "       0.0031371364872561903],\n",
       "      'train_acc_history': [0.8140243902439024,\n",
       "       0.8833841463414634,\n",
       "       0.9195884146341463,\n",
       "       0.9428353658536586,\n",
       "       0.9569359756097561,\n",
       "       0.9683689024390244,\n",
       "       0.979420731707317,\n",
       "       0.9858993902439024,\n",
       "       0.989329268292683,\n",
       "       0.989329268292683,\n",
       "       0.995045731707317,\n",
       "       0.9942835365853658,\n",
       "       0.9965701219512195,\n",
       "       0.9961890243902439,\n",
       "       0.9973323170731707,\n",
       "       0.9973323170731707,\n",
       "       0.9984756097560976,\n",
       "       0.9980945121951219,\n",
       "       0.9980945121951219,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9980945121951219,\n",
       "       0.9984756097560976,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       1.0],\n",
       "      'val_loss_history': [0.30331410603089765,\n",
       "       0.22111204131083054,\n",
       "       0.16885533793406052,\n",
       "       0.13066414777528157,\n",
       "       0.10554830967025323,\n",
       "       0.10111353885043751,\n",
       "       0.06987381320108067,\n",
       "       0.06545091555877165,\n",
       "       0.06851128859191456,\n",
       "       0.05461513403464447,\n",
       "       0.05232845566404814,\n",
       "       0.05149264417758042,\n",
       "       0.05446678954599933,\n",
       "       0.05273795867635107,\n",
       "       0.048987779702821914,\n",
       "       0.05496082704683596,\n",
       "       0.054812818352895025,\n",
       "       0.05136184132954275,\n",
       "       0.05417955233397978,\n",
       "       0.051342521317895844,\n",
       "       0.055588287438943305,\n",
       "       0.054535313191908324,\n",
       "       0.05680164294211532,\n",
       "       0.052386076507073914,\n",
       "       0.05797212709545751,\n",
       "       0.05935866933379492,\n",
       "       0.057263290356125006,\n",
       "       0.05995620783968744,\n",
       "       0.05841157050401142,\n",
       "       0.06353552249493077],\n",
       "      'val_acc_history': [0.8678678678678678,\n",
       "       0.8963963963963963,\n",
       "       0.9444444444444444,\n",
       "       0.9429429429429429,\n",
       "       0.9594594594594594,\n",
       "       0.9624624624624625,\n",
       "       0.9774774774774775,\n",
       "       0.9744744744744744,\n",
       "       0.9744744744744744,\n",
       "       0.9819819819819819,\n",
       "       0.984984984984985,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.984984984984985,\n",
       "       0.9819819819819819,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9894894894894894,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.9834834834834835],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9857572950440019,\n",
       "         'recall': 0.987832573559884,\n",
       "         'f1-score': 0.9867938431986223,\n",
       "         'support': 60325.0},\n",
       "        '1': {'precision': 0.9598204510619662,\n",
       "         'recall': 0.9531938026637673,\n",
       "         'f1-score': 0.9564956495649565,\n",
       "         'support': 18395.0},\n",
       "        'accuracy': 0.9797383130081301,\n",
       "        'macro avg': {'precision': 0.9727888730529841,\n",
       "         'recall': 0.9705131881118256,\n",
       "         'f1-score': 0.9716447463817894,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.979696468760344,\n",
       "         'recall': 0.9797383130081301,\n",
       "         'f1-score': 0.9797138727731741,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.98635477582846,\n",
       "         'recall': 0.9921568627450981,\n",
       "         'f1-score': 0.989247311827957,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.017523745074868202},\n",
       "        'treatment': {'precision': 0.9738562091503268,\n",
       "         'recall': 0.9551282051282052,\n",
       "         'f1-score': 0.9644012944983819,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.22929061949253082},\n",
       "        'accuracy': 0.9834834834834835,\n",
       "        'macro avg': {'precision': 0.9801054924893934,\n",
       "         'recall': 0.9736425339366517,\n",
       "         'f1-score': 0.9768243031631694,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9834271836335818,\n",
       "         'recall': 0.9834834834834835,\n",
       "         'f1-score': 0.9834275239849933,\n",
       "         'support': 666.0}}}}}},\n",
       "   'simple': {'summary': {'avg_accuracy': 0.8627627627627626,\n",
       "     'std_accuracy': 0.012761879496021485,\n",
       "     'train_loss_history': [0.2926997235126339],\n",
       "     'train_loss_std': [0.0027455994404608293],\n",
       "     'train_acc_history': [0.8647147147147146],\n",
       "     'train_acc_std': [0.002733805023428383],\n",
       "     'val_loss_history': [],\n",
       "     'val_loss_std': [],\n",
       "     'val_acc_history': [],\n",
       "     'val_acc_std': []},\n",
       "    'folds': {'1': {'accuracy': 0.8528528528528528,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.2934015871061245],\n",
       "      'train_acc_history': [0.859984984984985],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8564102564102564,\n",
       "         'recall': 0.9818716315531603,\n",
       "         'f1-score': 0.9148596210910751,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.8858024691358025,\n",
       "         'recall': 0.4606741573033708,\n",
       "         'f1-score': 0.6061246040126715,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.859984984984985,\n",
       "        'macro avg': {'precision': 0.8711063627730294,\n",
       "         'recall': 0.7212728944282656,\n",
       "         'f1-score': 0.7604921125518733,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8632838857375893,\n",
       "         'recall': 0.859984984984985,\n",
       "         'f1-score': 0.8426592023073493,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8470588235294118,\n",
       "         'recall': 0.9863013698630136,\n",
       "         'f1-score': 0.9113924050632911,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.9014084507042254,\n",
       "         'recall': 0.4129032258064516,\n",
       "         'f1-score': 0.5663716814159292,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8528528528528528,\n",
       "        'macro avg': {'precision': 0.8742336371168186,\n",
       "         'recall': 0.6996022978347326,\n",
       "         'f1-score': 0.7388820432396102,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8597077607848113,\n",
       "         'recall': 0.8528528528528528,\n",
       "         'f1-score': 0.8310947891994156,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.8813813813813813,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.29664639758151445],\n",
       "      'train_acc_history': [0.8674924924924925],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8619210977701544,\n",
       "         'recall': 0.984811366976972,\n",
       "         'f1-score': 0.9192773839469471,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.9066265060240963,\n",
       "         'recall': 0.48314606741573035,\n",
       "         'f1-score': 0.6303664921465969,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.8674924924924925,\n",
       "        'macro avg': {'precision': 0.8842738018971253,\n",
       "         'recall': 0.7339787171963512,\n",
       "         'f1-score': 0.774821938046772,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8723758535292406,\n",
       "         'recall': 0.8674924924924925,\n",
       "         'f1-score': 0.8517130124786221,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8763066202090593,\n",
       "         'recall': 0.9843444227005871,\n",
       "         'f1-score': 0.9271889400921659,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.9130434782608695,\n",
       "         'recall': 0.5419354838709678,\n",
       "         'f1-score': 0.680161943319838,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8813813813813813,\n",
       "        'macro avg': {'precision': 0.8946750492349644,\n",
       "         'recall': 0.7631399532857774,\n",
       "         'f1-score': 0.8036754417060019,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8848564895754716,\n",
       "         'recall': 0.8813813813813813,\n",
       "         'f1-score': 0.8696976720745822,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.8453453453453453,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.2880784124521485],\n",
       "      'train_acc_history': [0.8663663663663663],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8602564102564103,\n",
       "         'recall': 0.9857982370225269,\n",
       "         'f1-score': 0.9187585577361935,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.9104938271604939,\n",
       "         'recall': 0.4742765273311897,\n",
       "         'f1-score': 0.6236786469344608,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8663663663663663,\n",
       "        'macro avg': {'precision': 0.8853751187084521,\n",
       "         'recall': 0.7300373821768583,\n",
       "         'f1-score': 0.7712186023353271,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8719860173563878,\n",
       "         'recall': 0.8663663663663663,\n",
       "         'f1-score': 0.8498622722562094,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8431703204047217,\n",
       "         'recall': 0.9803921568627451,\n",
       "         'f1-score': 0.9066183136899365,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.863013698630137,\n",
       "         'recall': 0.40384615384615385,\n",
       "         'f1-score': 0.5502183406113537,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8453453453453453,\n",
       "        'macro avg': {'precision': 0.8530920095174294,\n",
       "         'recall': 0.6921191553544495,\n",
       "         'f1-score': 0.728418327150645,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8478183189079722,\n",
       "         'recall': 0.8453453453453453,\n",
       "         'f1-score': 0.823137238914773,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.8633633633633634,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.2930506963920122],\n",
       "      'train_acc_history': [0.8663663663663663],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8602564102564103,\n",
       "         'recall': 0.9857982370225269,\n",
       "         'f1-score': 0.9187585577361935,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.9104938271604939,\n",
       "         'recall': 0.4742765273311897,\n",
       "         'f1-score': 0.6236786469344608,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8663663663663663,\n",
       "        'macro avg': {'precision': 0.8853751187084521,\n",
       "         'recall': 0.7300373821768583,\n",
       "         'f1-score': 0.7712186023353271,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8719860173563878,\n",
       "         'recall': 0.8663663663663663,\n",
       "         'f1-score': 0.8498622722562094,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8605851979345955,\n",
       "         'recall': 0.9803921568627451,\n",
       "         'f1-score': 0.916590284142988,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8823529411764706,\n",
       "         'recall': 0.4807692307692308,\n",
       "         'f1-score': 0.6224066390041494,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8633633633633634,\n",
       "        'macro avg': {'precision': 0.871469069555533,\n",
       "         'recall': 0.7305806938159879,\n",
       "         'f1-score': 0.7694984615735687,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8656839486038636,\n",
       "         'recall': 0.8633633633633634,\n",
       "         'f1-score': 0.8476824032996564,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.8708708708708709,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.29232152403136974],\n",
       "      'train_acc_history': [0.8633633633633634],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8582408198121264,\n",
       "         'recall': 0.9843290891283056,\n",
       "         'f1-score': 0.916970802919708,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.9006211180124224,\n",
       "         'recall': 0.4662379421221865,\n",
       "         'f1-score': 0.614406779661017,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8633633633633634,\n",
       "        'macro avg': {'precision': 0.8794309689122743,\n",
       "         'recall': 0.7252835156252461,\n",
       "         'f1-score': 0.7656887912903625,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8681359194670003,\n",
       "         'recall': 0.8633633633633634,\n",
       "         'f1-score': 0.8463271007924911,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8642611683848798,\n",
       "         'recall': 0.9862745098039216,\n",
       "         'f1-score': 0.9212454212454212,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.9166666666666666,\n",
       "         'recall': 0.4935897435897436,\n",
       "         'f1-score': 0.6416666666666667,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8708708708708709,\n",
       "        'macro avg': {'precision': 0.8904639175257731,\n",
       "         'recall': 0.7399321266968326,\n",
       "         'f1-score': 0.781456043956044,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8765363301445777,\n",
       "         'recall': 0.8708708708708709,\n",
       "         'f1-score': 0.8557585057585056,\n",
       "         'support': 666.0}}}}}}},\n",
       "  'individual': {'nn': {'summary': {'avg_accuracy': 0.9900900900900901,\n",
       "     'std_accuracy': 0.006755088215736931,\n",
       "     'train_loss_history': [0.3904737331518313,\n",
       "      0.2515507095470661,\n",
       "      0.17886317615828862,\n",
       "      0.12465007706749728,\n",
       "      0.0837318028164346,\n",
       "      0.0650583455566226,\n",
       "      0.047277438136317386,\n",
       "      0.039756324880461144,\n",
       "      0.02953564538279685,\n",
       "      0.022576077917318155,\n",
       "      0.01854808077943034,\n",
       "      0.01539012170737473,\n",
       "      0.01235795600067188,\n",
       "      0.011483117752912931,\n",
       "      0.009047151814646473,\n",
       "      0.009454909014152135,\n",
       "      0.008500931373365769,\n",
       "      0.008110126659342246,\n",
       "      0.007360859405215284,\n",
       "      0.00741228352431435,\n",
       "      0.006472611560190959,\n",
       "      0.004965582294900696,\n",
       "      0.004749057680175345,\n",
       "      0.0048703703543784595,\n",
       "      0.004487464124075614,\n",
       "      0.004841404414747093,\n",
       "      0.004323989447069968,\n",
       "      0.003914053644417081,\n",
       "      0.003869219782489647,\n",
       "      0.0038360334856140355],\n",
       "     'train_loss_std': [0.019656007709718454,\n",
       "      0.011197817140169044,\n",
       "      0.01158276258312832,\n",
       "      0.013810183080542614,\n",
       "      0.006124181385587647,\n",
       "      0.007921195855678066,\n",
       "      0.006085670577331955,\n",
       "      0.005876933890601326,\n",
       "      0.0042445527165478995,\n",
       "      0.004359174590848532,\n",
       "      0.0019401372555846936,\n",
       "      0.0035645457318777113,\n",
       "      0.002153747591985541,\n",
       "      0.00265823487586945,\n",
       "      0.001661053705436568,\n",
       "      0.002140334650287306,\n",
       "      0.0007945619187864465,\n",
       "      0.0011642773308666364,\n",
       "      0.0014068111558632706,\n",
       "      0.001235623149004977,\n",
       "      0.0008598969330652014,\n",
       "      0.0004836199965051632,\n",
       "      0.00050353238725743,\n",
       "      0.000989552496837273,\n",
       "      0.0004337397438874347,\n",
       "      0.001066427195045133,\n",
       "      0.0006298241638047121,\n",
       "      0.0010315044651883459,\n",
       "      0.0007728738957187517,\n",
       "      0.000485418707990275],\n",
       "     'train_acc_history': [0.8228658536585366,\n",
       "      0.893673780487805,\n",
       "      0.9352134146341463,\n",
       "      0.9592987804878048,\n",
       "      0.976295731707317,\n",
       "      0.9803353658536584,\n",
       "      0.9865091463414635,\n",
       "      0.9889481707317073,\n",
       "      0.9916920731707318,\n",
       "      0.9939786585365853,\n",
       "      0.9955792682926828,\n",
       "      0.9963414634146343,\n",
       "      0.9974085365853659,\n",
       "      0.9976371951219513,\n",
       "      0.9984756097560975,\n",
       "      0.9979420731707316,\n",
       "      0.998170731707317,\n",
       "      0.9984756097560975,\n",
       "      0.998704268292683,\n",
       "      0.9986280487804878,\n",
       "      0.998780487804878,\n",
       "      0.9996189024390244,\n",
       "      0.999390243902439,\n",
       "      0.999314024390244,\n",
       "      0.9994664634146343,\n",
       "      0.9992378048780489,\n",
       "      0.999390243902439,\n",
       "      0.9995426829268294,\n",
       "      0.9996189024390244,\n",
       "      0.999390243902439],\n",
       "     'train_acc_std': [0.017874085030434257,\n",
       "      0.007667546548755673,\n",
       "      0.007377954292884195,\n",
       "      0.007642502780551886,\n",
       "      0.001993398907107026,\n",
       "      0.0034137493409388112,\n",
       "      0.002319378666875448,\n",
       "      0.002446159532908721,\n",
       "      0.0020649721317292746,\n",
       "      0.0017939942524298285,\n",
       "      0.001288988911988402,\n",
       "      0.000982019720024746,\n",
       "      0.0012616574205218509,\n",
       "      0.0012844740507890742,\n",
       "      0.0006376981909558807,\n",
       "      0.0006192102442558005,\n",
       "      0.0007389755880208084,\n",
       "      0.000681728041920693,\n",
       "      0.0006192102442558032,\n",
       "      0.0005169458828601851,\n",
       "      0.0003733978266437639,\n",
       "      0.00041747146151306907,\n",
       "      0.00018669891332188194,\n",
       "      0.00044443230905831505,\n",
       "      0.0003886447799994363,\n",
       "      0.0003408640209603217,\n",
       "      0.00030487804878047696,\n",
       "      0.00044443230905831505,\n",
       "      0.0003408640209603217,\n",
       "      0.0005169458828601393],\n",
       "     'val_loss_history': [0.3282468327067115,\n",
       "      7.476495187526399,\n",
       "      46.611840802972964,\n",
       "      29.158578890257264,\n",
       "      1.180473152446476,\n",
       "      0.087237571450797,\n",
       "      0.06939016964963893,\n",
       "      0.06209059164605357,\n",
       "      0.05903290389529007,\n",
       "      0.06683321923271499,\n",
       "      0.05079347750976343,\n",
       "      0.0491963760048913,\n",
       "      0.052779990779659294,\n",
       "      0.051671396004332404,\n",
       "      0.049430455994495956,\n",
       "      0.050611391802191395,\n",
       "      0.047162602661939515,\n",
       "      0.0528087893342176,\n",
       "      0.05408857369435612,\n",
       "      0.055682957168160505,\n",
       "      0.055118311662226915,\n",
       "      0.05335046035047113,\n",
       "      0.05349336305993017,\n",
       "      0.05416187269306234,\n",
       "      0.05230853052502922,\n",
       "      0.05269607521371324,\n",
       "      0.053705710128732875,\n",
       "      0.056008191360689866,\n",
       "      0.05541959098852452,\n",
       "      0.052252427577199834],\n",
       "     'val_loss_std': [0.024203200665660292,\n",
       "      5.169501586905834,\n",
       "      54.79226328523909,\n",
       "      48.90823453217657,\n",
       "      2.1059660720055584,\n",
       "      0.025689254834743207,\n",
       "      0.02180666892123856,\n",
       "      0.018511236709836117,\n",
       "      0.017079048575883816,\n",
       "      0.029950674755691842,\n",
       "      0.026804334566641372,\n",
       "      0.026693102862058227,\n",
       "      0.029142093514797428,\n",
       "      0.02970720190056712,\n",
       "      0.026052964470167878,\n",
       "      0.029918723531445535,\n",
       "      0.023805850142096,\n",
       "      0.029907948020169186,\n",
       "      0.032739447550522015,\n",
       "      0.03153924248757169,\n",
       "      0.03378180151578309,\n",
       "      0.03281456987600637,\n",
       "      0.03400343821676506,\n",
       "      0.03216295732565104,\n",
       "      0.031390163648612475,\n",
       "      0.03184429251392663,\n",
       "      0.03349446777130181,\n",
       "      0.032757798398913816,\n",
       "      0.03187181538100791,\n",
       "      0.029682309112263414],\n",
       "     'val_acc_history': [0.8621621621621621,\n",
       "      0.7663663663663662,\n",
       "      0.7663663663663662,\n",
       "      0.8321321321321321,\n",
       "      0.9144144144144144,\n",
       "      0.9756756756756756,\n",
       "      0.9786786786786786,\n",
       "      0.9819819819819819,\n",
       "      0.984984984984985,\n",
       "      0.9864864864864865,\n",
       "      0.9891891891891893,\n",
       "      0.9900900900900902,\n",
       "      0.9885885885885886,\n",
       "      0.9894894894894894,\n",
       "      0.9906906906906908,\n",
       "      0.9906906906906908,\n",
       "      0.9903903903903905,\n",
       "      0.9897897897897897,\n",
       "      0.9891891891891893,\n",
       "      0.9894894894894894,\n",
       "      0.9888888888888889,\n",
       "      0.9903903903903905,\n",
       "      0.9897897897897898,\n",
       "      0.9891891891891893,\n",
       "      0.9891891891891893,\n",
       "      0.9900900900900901,\n",
       "      0.9897897897897898,\n",
       "      0.9894894894894894,\n",
       "      0.9900900900900901,\n",
       "      0.9900900900900901],\n",
       "     'val_acc_std': [0.007561368355564057,\n",
       "      0.0007355825053403044,\n",
       "      0.0007355825053403044,\n",
       "      0.08079919858807051,\n",
       "      0.07879664722763921,\n",
       "      0.009372063984580936,\n",
       "      0.006606606606606606,\n",
       "      0.006006006006006004,\n",
       "      0.0041393539796066685,\n",
       "      0.008001749305990504,\n",
       "      0.006606606606606618,\n",
       "      0.006050883387270241,\n",
       "      0.008091407558743198,\n",
       "      0.00678173560999174,\n",
       "      0.006256056052852744,\n",
       "      0.006256056052852744,\n",
       "      0.00648259253601288,\n",
       "      0.0074411481641824834,\n",
       "      0.007796249240688976,\n",
       "      0.0074168702914284915,\n",
       "      0.00820210227252111,\n",
       "      0.00648259253601288,\n",
       "      0.007068229607171066,\n",
       "      0.007796249240688976,\n",
       "      0.007796249240688976,\n",
       "      0.006755088215736931,\n",
       "      0.007068229607171066,\n",
       "      0.0074168702914284915,\n",
       "      0.006755088215736931,\n",
       "      0.006755088215736931]},\n",
       "    'folds': {'1': {'accuracy': 0.9894894894894894,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.37515244723820107,\n",
       "       0.26657980389711333,\n",
       "       0.19071824848651886,\n",
       "       0.12382537226487951,\n",
       "       0.07972674012729307,\n",
       "       0.06571286648693608,\n",
       "       0.04856864153975394,\n",
       "       0.03768130832510751,\n",
       "       0.033436265319767525,\n",
       "       0.028063939123346312,\n",
       "       0.021075998160352067,\n",
       "       0.020497774483817743,\n",
       "       0.014293226436143967,\n",
       "       0.012851644420950877,\n",
       "       0.010829773209080464,\n",
       "       0.008441572634094372,\n",
       "       0.009068838258205755,\n",
       "       0.009329673598475027,\n",
       "       0.009814531976788691,\n",
       "       0.008780292047914572,\n",
       "       0.006710770602406162,\n",
       "       0.005013944907681789,\n",
       "       0.00561088059529117,\n",
       "       0.006219686643595285,\n",
       "       0.004218499961297777,\n",
       "       0.0039373824625594045,\n",
       "       0.0052798238438099806,\n",
       "       0.004880821563746416,\n",
       "       0.003957956289679448,\n",
       "       0.0043478584436631605],\n",
       "      'train_acc_history': [0.8342225609756098,\n",
       "       0.8841463414634146,\n",
       "       0.9245426829268293,\n",
       "       0.9603658536585366,\n",
       "       0.9790396341463414,\n",
       "       0.9813262195121951,\n",
       "       0.9874237804878049,\n",
       "       0.9900914634146342,\n",
       "       0.9885670731707317,\n",
       "       0.9923780487804879,\n",
       "       0.9939024390243902,\n",
       "       0.9954268292682927,\n",
       "       0.9954268292682927,\n",
       "       0.9969512195121951,\n",
       "       0.9977134146341463,\n",
       "       0.9977134146341463,\n",
       "       0.9973323170731707,\n",
       "       0.9977134146341463,\n",
       "       0.9988567073170732,\n",
       "       0.9980945121951219,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244],\n",
       "      'val_loss_history': [0.3443444696339694,\n",
       "       9.56908848068931,\n",
       "       33.059425527399235,\n",
       "       0.38384582982821897,\n",
       "       0.1292382414368066,\n",
       "       0.13124918260357596,\n",
       "       0.09336012483320454,\n",
       "       0.07486283029852943,\n",
       "       0.06588903827254068,\n",
       "       0.10733231068165465,\n",
       "       0.07168539748950438,\n",
       "       0.07459515284492889,\n",
       "       0.07832207055550745,\n",
       "       0.07611471607180481,\n",
       "       0.07147767178883607,\n",
       "       0.0815199957578443,\n",
       "       0.07056535428008912,\n",
       "       0.07916396835670722,\n",
       "       0.08950945833401586,\n",
       "       0.07878745039289986,\n",
       "       0.09210732275493104,\n",
       "       0.08685199953933162,\n",
       "       0.08363259714149701,\n",
       "       0.08465988818683069,\n",
       "       0.07774486123658293,\n",
       "       0.08021271077450365,\n",
       "       0.0752950005487285,\n",
       "       0.07724053858493624,\n",
       "       0.08195185733281753,\n",
       "       0.07512943897877862],\n",
       "      'val_acc_history': [0.8558558558558559,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.8843843843843844,\n",
       "       0.9684684684684685,\n",
       "       0.9684684684684685,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.984984984984985,\n",
       "       0.9804804804804805,\n",
       "       0.9834834834834835,\n",
       "       0.987987987987988,\n",
       "       0.987987987987988,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.990990990990991,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894,\n",
       "       0.9894894894894894],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9831377818563188,\n",
       "         'recall': 0.9946120689655172,\n",
       "         'f1-score': 0.9888416402953587,\n",
       "         'support': 60320.0},\n",
       "        '1': {'precision': 0.9816342676311031,\n",
       "         'recall': 0.9440760869565218,\n",
       "         'f1-score': 0.9624889184397163,\n",
       "         'support': 18400.0},\n",
       "        'accuracy': 0.9827997967479675,\n",
       "        'macro avg': {'precision': 0.982386024743711,\n",
       "         'recall': 0.9693440779610195,\n",
       "         'f1-score': 0.9756652793675376,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9827863506857909,\n",
       "         'recall': 0.9827997967479675,\n",
       "         'f1-score': 0.9826819593738163,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9941176470588236,\n",
       "         'recall': 0.9921722113502935,\n",
       "         'f1-score': 0.9931439764936337,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.026825934648513794},\n",
       "        'treatment': {'precision': 0.9743589743589743,\n",
       "         'recall': 0.9806451612903225,\n",
       "         'f1-score': 0.977491961414791,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.23784124851226807},\n",
       "        'accuracy': 0.9894894894894894,\n",
       "        'macro avg': {'precision': 0.984238310708899,\n",
       "         'recall': 0.9864086863203081,\n",
       "         'f1-score': 0.9853179689542124,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.989519157166216,\n",
       "         'recall': 0.9894894894894894,\n",
       "         'f1-score': 0.9895012402515608,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.9984984984984985,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.37018777102958866,\n",
       "       0.24344122373476262,\n",
       "       0.1717160280521323,\n",
       "       0.10606850320246161,\n",
       "       0.08128088921671961,\n",
       "       0.05848021400956119,\n",
       "       0.04191650579706198,\n",
       "       0.03428323832681266,\n",
       "       0.023579625015306038,\n",
       "       0.02053260927020413,\n",
       "       0.01593334341376293,\n",
       "       0.012437857358103118,\n",
       "       0.009198829963258126,\n",
       "       0.01363117470428711,\n",
       "       0.007642867154350913,\n",
       "       0.00753574732622904,\n",
       "       0.0077393990600617924,\n",
       "       0.006003030413966172,\n",
       "       0.0074806182585633926,\n",
       "       0.008408949953197251,\n",
       "       0.006166505204263802,\n",
       "       0.005680934821323651,\n",
       "       0.004931486118016992,\n",
       "       0.003230892639981992,\n",
       "       0.004411460498132084,\n",
       "       0.005343466965913228,\n",
       "       0.0034804368326326876,\n",
       "       0.0032544038542432754,\n",
       "       0.002848464446199104,\n",
       "       0.004430346834053081],\n",
       "      'train_acc_history': [0.8456554878048781,\n",
       "       0.895579268292683,\n",
       "       0.9390243902439024,\n",
       "       0.9657012195121951,\n",
       "       0.975609756097561,\n",
       "       0.9817073170731707,\n",
       "       0.9870426829268293,\n",
       "       0.991234756097561,\n",
       "       0.995045731707317,\n",
       "       0.993140243902439,\n",
       "       0.9961890243902439,\n",
       "       0.9965701219512195,\n",
       "       0.9992378048780488,\n",
       "       0.9961890243902439,\n",
       "       0.9988567073170732,\n",
       "       0.9980945121951219,\n",
       "       0.9988567073170732,\n",
       "       0.9984756097560976,\n",
       "       0.9977134146341463,\n",
       "       0.9984756097560976,\n",
       "       0.9984756097560976,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9984756097560976],\n",
       "      'val_loss_history': [0.3644442612474615,\n",
       "       3.4780200069600884,\n",
       "       6.924642107703469,\n",
       "       18.843832449479535,\n",
       "       0.2000876170667735,\n",
       "       0.053884689611467446,\n",
       "       0.02911435355516997,\n",
       "       0.026620334437624973,\n",
       "       0.03855755374851552,\n",
       "       0.016562953845343807,\n",
       "       0.01584313656415113,\n",
       "       0.009710313651753082,\n",
       "       0.013170219428667968,\n",
       "       0.00992957823803987,\n",
       "       0.013809583377389406,\n",
       "       0.008656395080668683,\n",
       "       0.017196603637950666,\n",
       "       0.015377991562920877,\n",
       "       0.009277527929622342,\n",
       "       0.014933591364586557,\n",
       "       0.008773980069566856,\n",
       "       0.012875995119843124,\n",
       "       0.006898190633033995,\n",
       "       0.013018569567727602,\n",
       "       0.009144624475580216,\n",
       "       0.007673999821153385,\n",
       "       0.009146939275193621,\n",
       "       0.013056103245947848,\n",
       "       0.014295903475034389,\n",
       "       0.013061763959492302],\n",
       "      'val_acc_history': [0.8753753753753754,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.7672672672672672,\n",
       "       0.9009009009009009,\n",
       "       0.9924924924924925,\n",
       "       0.987987987987988,\n",
       "       0.9924924924924925,\n",
       "       0.9924924924924925,\n",
       "       0.996996996996997,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985,\n",
       "       0.9984984984984985],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9852830064715351,\n",
       "         'recall': 0.9948916973496533,\n",
       "         'f1-score': 0.9900640390836469,\n",
       "         'support': 60294.0},\n",
       "        '1': {'precision': 0.9827334903016033,\n",
       "         'recall': 0.9513730598067948,\n",
       "         'f1-score': 0.9667990293403926,\n",
       "         'support': 18426.0},\n",
       "        'accuracy': 0.9847052845528456,\n",
       "        'macro avg': {'precision': 0.9840082483865692,\n",
       "         'recall': 0.973132378578224,\n",
       "         'f1-score': 0.9784315342120198,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9846862409107225,\n",
       "         'recall': 0.9847052845528456,\n",
       "         'f1-score': 0.984618395415847,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 1.0,\n",
       "         'recall': 0.9980430528375733,\n",
       "         'f1-score': 0.9990205680705191,\n",
       "         'support': 511.0,\n",
       "         'loss': 0.007709237281233072},\n",
       "        'treatment': {'precision': 0.9935897435897436,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.9967845659163987,\n",
       "         'support': 155.0,\n",
       "         'loss': 0.0005072558997198939},\n",
       "        'accuracy': 0.9984984984984985,\n",
       "        'macro avg': {'precision': 0.9967948717948718,\n",
       "         'recall': 0.9990215264187867,\n",
       "         'f1-score': 0.9979025669934589,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9985081235081236,\n",
       "         'recall': 0.9984984984984985,\n",
       "         'f1-score': 0.9985001771787942,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.996996996996997,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4197837397819612,\n",
       "       0.26353679124901935,\n",
       "       0.19423760001252338,\n",
       "       0.14134396031135465,\n",
       "       0.09499531584541972,\n",
       "       0.07943081646794226,\n",
       "       0.05474811328983888,\n",
       "       0.050897463361119354,\n",
       "       0.035022609640003706,\n",
       "       0.02429907173826927,\n",
       "       0.02035718636087528,\n",
       "       0.015696318439639558,\n",
       "       0.014714750909905245,\n",
       "       0.01331944400217475,\n",
       "       0.01096027699957897,\n",
       "       0.012696507543030127,\n",
       "       0.009008573035982141,\n",
       "       0.00810438881219342,\n",
       "       0.006475184481908999,\n",
       "       0.007936975154912145,\n",
       "       0.008024780499953322,\n",
       "       0.004876823615986944,\n",
       "       0.004568056850807696,\n",
       "       0.005296404598417078,\n",
       "       0.004997664415163965,\n",
       "       0.00667946619299672,\n",
       "       0.004772189316033136,\n",
       "       0.0034430954639413733,\n",
       "       0.004368261593144115,\n",
       "       0.003342052281368524],\n",
       "      'train_acc_history': [0.8010670731707317,\n",
       "       0.8852896341463414,\n",
       "       0.9294969512195121,\n",
       "       0.9519817073170732,\n",
       "       0.9733231707317073,\n",
       "       0.9740853658536586,\n",
       "       0.984375,\n",
       "       0.984375,\n",
       "       0.9919969512195121,\n",
       "       0.9942835365853658,\n",
       "       0.9961890243902439,\n",
       "       0.9961890243902439,\n",
       "       0.9969512195121951,\n",
       "       0.9977134146341463,\n",
       "       0.9977134146341463,\n",
       "       0.9969512195121951,\n",
       "       0.9977134146341463,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9980945121951219,\n",
       "       0.9984756097560976,\n",
       "       1.0,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9988567073170732,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       1.0,\n",
       "       0.9992378048780488,\n",
       "       1.0],\n",
       "      'val_loss_history': [0.32787535000931134,\n",
       "       1.1408347521315922,\n",
       "       152.78252410888672,\n",
       "       125.91926592046565,\n",
       "       5.391636886379936,\n",
       "       0.07639001699333842,\n",
       "       0.06834946996109052,\n",
       "       0.06127948940477588,\n",
       "       0.04562731364487924,\n",
       "       0.05860512297261845,\n",
       "       0.021987980062311344,\n",
       "       0.02600906683470715,\n",
       "       0.02139034312726422,\n",
       "       0.022321395322003147,\n",
       "       0.02230498982085423,\n",
       "       0.020680594924752684,\n",
       "       0.019758023300462148,\n",
       "       0.018704880695705386,\n",
       "       0.02013904319822111,\n",
       "       0.019781599085862665,\n",
       "       0.022230491603047332,\n",
       "       0.017510857607703656,\n",
       "       0.020293391473718326,\n",
       "       0.01980595123446123,\n",
       "       0.019984758501364427,\n",
       "       0.021815202812748877,\n",
       "       0.019951164764775473,\n",
       "       0.021061793580884114,\n",
       "       0.02017918644494123,\n",
       "       0.01968795795057138],\n",
       "      'val_acc_history': [0.8543543543543544,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.975975975975976,\n",
       "       0.975975975975976,\n",
       "       0.9774774774774775,\n",
       "       0.984984984984985,\n",
       "       0.9954954954954955,\n",
       "       0.9954954954954955,\n",
       "       0.9954954954954955,\n",
       "       0.996996996996997,\n",
       "       0.9954954954954955,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997,\n",
       "       0.996996996996997],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9849391138135021,\n",
       "         'recall': 0.9907346018696546,\n",
       "         'f1-score': 0.9878283575306357,\n",
       "         'support': 60332.0},\n",
       "        '1': {'precision': 0.9690012754394721,\n",
       "         'recall': 0.9502936697846421,\n",
       "         'f1-score': 0.9595562999368497,\n",
       "         'support': 18388.0},\n",
       "        'accuracy': 0.9812881097560976,\n",
       "        'macro avg': {'precision': 0.976970194626487,\n",
       "         'recall': 0.9705141358271483,\n",
       "         'f1-score': 0.9736923287337427,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9812162356120073,\n",
       "         'recall': 0.9812881097560976,\n",
       "         'f1-score': 0.9812243611506237,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.99609375,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.9980430528375733,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.0017944542923942208},\n",
       "        'treatment': {'precision': 1.0,\n",
       "         'recall': 0.9871794871794872,\n",
       "         'f1-score': 0.9935483870967742,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.08279246091842651},\n",
       "        'accuracy': 0.996996996996997,\n",
       "        'macro avg': {'precision': 0.998046875,\n",
       "         'recall': 0.9935897435897436,\n",
       "         'f1-score': 0.9957957199671738,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9970087274774775,\n",
       "         'recall': 0.996996996996997,\n",
       "         'f1-score': 0.9969902482496384,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.9819819819819819,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.3793635491917773,\n",
       "       0.2395543049021465,\n",
       "       0.17335820198059082,\n",
       "       0.1387594405834268,\n",
       "       0.0850061829678896,\n",
       "       0.057107860767623274,\n",
       "       0.05240250256185124,\n",
       "       0.039941154136436015,\n",
       "       0.02651649621519737,\n",
       "       0.024704071375109802,\n",
       "       0.018334026936805102,\n",
       "       0.0177372120570664,\n",
       "       0.013070457904576891,\n",
       "       0.011167291256530983,\n",
       "       0.008987766974492044,\n",
       "       0.01125476371905789,\n",
       "       0.009330632525677906,\n",
       "       0.008081614840548576,\n",
       "       0.0056081203851712546,\n",
       "       0.006367294245581256,\n",
       "       0.005864678822435075,\n",
       "       0.004170003578509772,\n",
       "       0.004109654388186045,\n",
       "       0.005116116273977862,\n",
       "       0.004947331664436384,\n",
       "       0.004445336297688234,\n",
       "       0.004054680593005132,\n",
       "       0.002633071277381443,\n",
       "       0.004978478528422917,\n",
       "       0.003786467972620413],\n",
       "      'train_acc_history': [0.8307926829268293,\n",
       "       0.9024390243902439,\n",
       "       0.9375,\n",
       "       0.9493140243902439,\n",
       "       0.975609756097561,\n",
       "       0.984375,\n",
       "       0.9836128048780488,\n",
       "       0.9885670731707317,\n",
       "       0.9916158536585366,\n",
       "       0.9927591463414634,\n",
       "       0.9973323170731707,\n",
       "       0.9954268292682927,\n",
       "       0.9973323170731707,\n",
       "       0.9973323170731707,\n",
       "       0.9988567073170732,\n",
       "       0.9980945121951219,\n",
       "       0.9977134146341463,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9984756097560976,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244],\n",
       "      'val_loss_history': [0.2998955182053826,\n",
       "       16.02162391489202,\n",
       "       37.16329522566362,\n",
       "       0.5268438959663565,\n",
       "       0.09813053787431934,\n",
       "       0.09547339549118822,\n",
       "       0.07422813409092752,\n",
       "       0.07658950341018764,\n",
       "       0.08750095866112546,\n",
       "       0.08247552583502098,\n",
       "       0.0818677567453547,\n",
       "       0.07577132746915925,\n",
       "       0.07730885785581036,\n",
       "       0.08284644887316972,\n",
       "       0.07589795328253372,\n",
       "       0.07461203041020781,\n",
       "       0.06962488380006769,\n",
       "       0.08522441835587168,\n",
       "       0.07733213843841275,\n",
       "       0.08778447477908974,\n",
       "       0.08663739090975882,\n",
       "       0.089291357207747,\n",
       "       0.09186001662270758,\n",
       "       0.09001341408690099,\n",
       "       0.08505389460531826,\n",
       "       0.0858863541295498,\n",
       "       0.09637922978832979,\n",
       "       0.09536160515430807,\n",
       "       0.09034681512275711,\n",
       "       0.08343506048285318],\n",
       "      'val_acc_history': [0.8603603603603603,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.7762762762762763,\n",
       "       0.9684684684684685,\n",
       "       0.9654654654654654,\n",
       "       0.9714714714714715,\n",
       "       0.9774774774774775,\n",
       "       0.9819819819819819,\n",
       "       0.978978978978979,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9804804804804805,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819,\n",
       "       0.9819819819819819],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9859335669512858,\n",
       "         'recall': 0.9929416442986381,\n",
       "         'f1-score': 0.9894251962653855,\n",
       "         'support': 60354.0},\n",
       "        '1': {'precision': 0.976250209065061,\n",
       "         'recall': 0.9534465860829794,\n",
       "         'f1-score': 0.9647136600280969,\n",
       "         'support': 18366.0},\n",
       "        'accuracy': 0.9837271341463415,\n",
       "        'macro avg': {'precision': 0.9810918880081734,\n",
       "         'recall': 0.9731941151908088,\n",
       "         'f1-score': 0.9770694281467411,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9836743627981048,\n",
       "         'recall': 0.9837271341463415,\n",
       "         'f1-score': 0.9836597989770972,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.986328125,\n",
       "         'recall': 0.9901960784313726,\n",
       "         'f1-score': 0.9882583170254403,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.041052721440792084},\n",
       "        'treatment': {'precision': 0.9675324675324676,\n",
       "         'recall': 0.9551282051282052,\n",
       "         'f1-score': 0.9612903225806452,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.24220646917819977},\n",
       "        'accuracy': 0.9819819819819819,\n",
       "        'macro avg': {'precision': 0.9769302962662338,\n",
       "         'recall': 0.9726621417797889,\n",
       "         'f1-score': 0.9747743198030427,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9819255385661636,\n",
       "         'recall': 0.9819819819819819,\n",
       "         'f1-score': 0.9819414894978306,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.9834834834834835,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.4078811585176282,\n",
       "       0.24464142395228874,\n",
       "       0.1642858022596778,\n",
       "       0.11325310897536395,\n",
       "       0.07764988592485102,\n",
       "       0.06455997005105019,\n",
       "       0.03875142749308086,\n",
       "       0.03597846025283017,\n",
       "       0.029123230723709596,\n",
       "       0.015280698079661263,\n",
       "       0.017039849025356334,\n",
       "       0.010581446198246828,\n",
       "       0.010512514789475173,\n",
       "       0.0064460343806209365,\n",
       "       0.006815074735729978,\n",
       "       0.007345953848349248,\n",
       "       0.007357213986901249,\n",
       "       0.00903192563152804,\n",
       "       0.007425841923644085,\n",
       "       0.005567906219966528,\n",
       "       0.005596322671896438,\n",
       "       0.005086204551001329,\n",
       "       0.00452521044857482,\n",
       "       0.0044887516159200815,\n",
       "       0.003862364081347861,\n",
       "       0.0038013701545778753,\n",
       "       0.0040328166498689024,\n",
       "       0.005358876062772896,\n",
       "       0.003192938055002653,\n",
       "       0.0032734418963650015],\n",
       "      'train_acc_history': [0.8025914634146342,\n",
       "       0.9009146341463414,\n",
       "       0.9455030487804879,\n",
       "       0.9691310975609756,\n",
       "       0.9778963414634146,\n",
       "       0.9801829268292683,\n",
       "       0.9900914634146342,\n",
       "       0.9904725609756098,\n",
       "       0.991234756097561,\n",
       "       0.9973323170731707,\n",
       "       0.9942835365853658,\n",
       "       0.9980945121951219,\n",
       "       0.9980945121951219,\n",
       "       1.0,\n",
       "       0.9992378048780488,\n",
       "       0.9988567073170732,\n",
       "       0.9992378048780488,\n",
       "       0.9977134146341463,\n",
       "       0.9984756097560976,\n",
       "       0.9992378048780488,\n",
       "       0.9992378048780488,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       1.0,\n",
       "       0.9996189024390244,\n",
       "       0.9996189024390244,\n",
       "       0.9988567073170732,\n",
       "       1.0,\n",
       "       0.9992378048780488],\n",
       "      'val_loss_history': [0.30467456443743274,\n",
       "       7.172908782958984,\n",
       "       3.129317045211792,\n",
       "       0.11910635554655032,\n",
       "       0.08327247947454453,\n",
       "       0.07919057255441492,\n",
       "       0.08189876580780203,\n",
       "       0.07110080067914995,\n",
       "       0.0575896551493894,\n",
       "       0.06919018282893706,\n",
       "       0.0625831166874956,\n",
       "       0.05989601922390813,\n",
       "       0.07370846293104644,\n",
       "       0.06714484151664445,\n",
       "       0.06366208170286634,\n",
       "       0.0675879428374835,\n",
       "       0.058668148291127924,\n",
       "       0.06557268769988282,\n",
       "       0.07418470057150857,\n",
       "       0.07712767021836374,\n",
       "       0.06584237297383053,\n",
       "       0.060222092277730226,\n",
       "       0.06478261942869391,\n",
       "       0.06331154038939117,\n",
       "       0.06961451380630024,\n",
       "       0.06789210853061046,\n",
       "       0.06775621626663698,\n",
       "       0.07332091623737308,\n",
       "       0.0703241925670723,\n",
       "       0.06994791651430371],\n",
       "      'val_acc_history': [0.8648648648648649,\n",
       "       0.7657657657657657,\n",
       "       0.7657657657657657,\n",
       "       0.9669669669669669,\n",
       "       0.9684684684684685,\n",
       "       0.975975975975976,\n",
       "       0.972972972972973,\n",
       "       0.9774774774774775,\n",
       "       0.9804804804804805,\n",
       "       0.9804804804804805,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.978978978978979,\n",
       "       0.9819819819819819,\n",
       "       0.9864864864864865,\n",
       "       0.9864864864864865,\n",
       "       0.984984984984985,\n",
       "       0.9804804804804805,\n",
       "       0.978978978978979,\n",
       "       0.9804804804804805,\n",
       "       0.9774774774774775,\n",
       "       0.984984984984985,\n",
       "       0.9819819819819819,\n",
       "       0.978978978978979,\n",
       "       0.978978978978979,\n",
       "       0.9834834834834835,\n",
       "       0.9819819819819819,\n",
       "       0.9804804804804805,\n",
       "       0.9834834834834835,\n",
       "       0.9834834834834835],\n",
       "      'classification_report': {'accuracy_report': {'0': {'precision': 0.9866100103760067,\n",
       "         'recall': 0.992774279085184,\n",
       "         'f1-score': 0.9896825462385488,\n",
       "         'support': 60340.0},\n",
       "        '1': {'precision': 0.9757818141420874,\n",
       "         'recall': 0.9557671381936887,\n",
       "         'f1-score': 0.9656707803094852,\n",
       "         'support': 18380.0},\n",
       "        'accuracy': 0.9841336382113821,\n",
       "        'macro avg': {'precision': 0.981195912259047,\n",
       "         'recall': 0.9742707086394364,\n",
       "         'f1-score': 0.977676663274017,\n",
       "         'support': 78720.0},\n",
       "        'weighted avg': {'precision': 0.9840817806150892,\n",
       "         'recall': 0.9841336382113821,\n",
       "         'f1-score': 0.9840761405249284,\n",
       "         'support': 78720.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9788867562380038,\n",
       "         'recall': 1.0,\n",
       "         'f1-score': 0.9893307468477207,\n",
       "         'support': 510.0,\n",
       "         'loss': 0.0035060755908489227},\n",
       "        'treatment': {'precision': 1.0,\n",
       "         'recall': 0.9294871794871795,\n",
       "         'f1-score': 0.9634551495016611,\n",
       "         'support': 156.0,\n",
       "         'loss': 0.304078072309494},\n",
       "        'accuracy': 0.9834834834834835,\n",
       "        'macro avg': {'precision': 0.989443378119002,\n",
       "         'recall': 0.9647435897435898,\n",
       "         'f1-score': 0.9763929481746909,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.9838322007227958,\n",
       "         'recall': 0.9834834834834835,\n",
       "         'f1-score': 0.983269796118013,\n",
       "         'support': 666.0}}}}}},\n",
       "   'simple': {'summary': {'avg_accuracy': 0.881981981981982,\n",
       "     'std_accuracy': 0.008146942922672984,\n",
       "     'train_loss_history': [0.2341198645968631],\n",
       "     'train_loss_std': [0.002527242601366719],\n",
       "     'train_acc_history': [0.883858858858859],\n",
       "     'train_acc_std': [0.0044134950320417555],\n",
       "     'val_loss_history': [],\n",
       "     'val_loss_std': [],\n",
       "     'val_acc_history': [],\n",
       "     'val_acc_std': []},\n",
       "    'folds': {'1': {'accuracy': 0.8828828828828829,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.23714838932722734],\n",
       "      'train_acc_history': [0.8768768768768769],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8891412994093594,\n",
       "         'recall': 0.958843704066634,\n",
       "         'f1-score': 0.9226779820839227,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.8185745140388769,\n",
       "         'recall': 0.608346709470305,\n",
       "         'f1-score': 0.6979742173112339,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.8768768768768769,\n",
       "        'macro avg': {'precision': 0.8538579067241181,\n",
       "         'recall': 0.7835952067684695,\n",
       "         'f1-score': 0.8103260996975783,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8726386315092803,\n",
       "         'recall': 0.8768768768768769,\n",
       "         'f1-score': 0.8701290160728923,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8872987477638641,\n",
       "         'recall': 0.9706457925636007,\n",
       "         'f1-score': 0.9271028037383178,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.8598130841121495,\n",
       "         'recall': 0.5935483870967742,\n",
       "         'f1-score': 0.7022900763358778,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8828828828828829,\n",
       "        'macro avg': {'precision': 0.8735559159380069,\n",
       "         'recall': 0.7820970898301874,\n",
       "         'f1-score': 0.8146964400370977,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.880901934151228,\n",
       "         'recall': 0.8828828828828829,\n",
       "         'f1-score': 0.874781523336849,\n",
       "         'support': 666.0}}}},\n",
       "     '2': {'accuracy': 0.8768768768768769,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.23379102071562283],\n",
       "      'train_acc_history': [0.8825075075075075],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8970588235294118,\n",
       "         'recall': 0.9563939245467908,\n",
       "         'f1-score': 0.9257766184491345,\n",
       "         'support': 2041.0},\n",
       "        'treatment': {'precision': 0.8176229508196722,\n",
       "         'recall': 0.6404494382022472,\n",
       "         'f1-score': 0.7182718271827183,\n",
       "         'support': 623.0},\n",
       "        'accuracy': 0.8825075075075075,\n",
       "        'macro avg': {'precision': 0.857340887174542,\n",
       "         'recall': 0.7984216813745191,\n",
       "         'f1-score': 0.8220242228159265,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8784820409850546,\n",
       "         'recall': 0.8825075075075075,\n",
       "         'f1-score': 0.8772497847558248,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8994413407821229,\n",
       "         'recall': 0.9452054794520548,\n",
       "         'f1-score': 0.9217557251908397,\n",
       "         'support': 511.0},\n",
       "        'treatment': {'precision': 0.7829457364341085,\n",
       "         'recall': 0.6516129032258065,\n",
       "         'f1-score': 0.7112676056338029,\n",
       "         'support': 155.0},\n",
       "        'accuracy': 0.8768768768768769,\n",
       "        'macro avg': {'precision': 0.8411935386081157,\n",
       "         'recall': 0.7984091913389306,\n",
       "         'f1-score': 0.8165116654123212,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8723290004308584,\n",
       "         'recall': 0.8768768768768769,\n",
       "         'f1-score': 0.8727682499185564,\n",
       "         'support': 666.0}}}},\n",
       "     '3': {'accuracy': 0.8693693693693694,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.22960848290765817],\n",
       "      'train_acc_history': [0.8903903903903904],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.9017447199265382,\n",
       "         'recall': 0.9618021547502449,\n",
       "         'f1-score': 0.9308056872037914,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.8395061728395061,\n",
       "         'recall': 0.6559485530546624,\n",
       "         'f1-score': 0.7364620938628159,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8903903903903904,\n",
       "        'macro avg': {'precision': 0.8706254463830221,\n",
       "         'recall': 0.8088753539024536,\n",
       "         'f1-score': 0.8336338905333036,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8872130471457071,\n",
       "         'recall': 0.8903903903903904,\n",
       "         'f1-score': 0.8854296680378428,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.8810810810810811,\n",
       "         'recall': 0.9588235294117647,\n",
       "         'f1-score': 0.9183098591549296,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8108108108108109,\n",
       "         'recall': 0.5769230769230769,\n",
       "         'f1-score': 0.6741573033707865,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8693693693693694,\n",
       "        'macro avg': {'precision': 0.845945945945946,\n",
       "         'recall': 0.7678733031674208,\n",
       "         'f1-score': 0.796233581262858,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8646213781348916,\n",
       "         'recall': 0.8693693693693694,\n",
       "         'f1-score': 0.8611209722144997,\n",
       "         'support': 666.0}}}},\n",
       "     '4': {'accuracy': 0.8918918918918919,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.23445083208208617],\n",
       "      'train_acc_history': [0.8836336336336337],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8954337899543379,\n",
       "         'recall': 0.9603330068560235,\n",
       "         'f1-score': 0.9267485822306238,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.8291139240506329,\n",
       "         'recall': 0.6318327974276527,\n",
       "         'f1-score': 0.7171532846715328,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8836336336336337,\n",
       "        'macro avg': {'precision': 0.8622738570024854,\n",
       "         'recall': 0.7960829021418381,\n",
       "         'f1-score': 0.8219509334510784,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8799491966389833,\n",
       "         'recall': 0.8836336336336337,\n",
       "         'f1-score': 0.87781154203477,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9025735294117647,\n",
       "         'recall': 0.9627450980392157,\n",
       "         'f1-score': 0.9316888045540797,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8442622950819673,\n",
       "         'recall': 0.6602564102564102,\n",
       "         'f1-score': 0.7410071942446043,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8918918918918919,\n",
       "        'macro avg': {'precision': 0.8734179122468659,\n",
       "         'recall': 0.811500754147813,\n",
       "         'f1-score': 0.8363479993993419,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8889150420912717,\n",
       "         'recall': 0.8918918918918919,\n",
       "         'f1-score': 0.887024643580689,\n",
       "         'support': 666.0}}}},\n",
       "     '5': {'accuracy': 0.8888888888888888,\n",
       "      'precision': None,\n",
       "      'recall': None,\n",
       "      'f1_score': None,\n",
       "      'train_loss_history': [0.235600597951721],\n",
       "      'train_acc_history': [0.8858858858858859],\n",
       "      'val_loss_history': [],\n",
       "      'val_acc_history': [],\n",
       "      'classification_report': {'accuracy_report': {'no treatment': {'precision': 0.8968036529680365,\n",
       "         'recall': 0.9618021547502449,\n",
       "         'f1-score': 0.9281663516068053,\n",
       "         'support': 2042.0},\n",
       "        'treatment': {'precision': 0.8354430379746836,\n",
       "         'recall': 0.6366559485530546,\n",
       "         'f1-score': 0.7226277372262774,\n",
       "         'support': 622.0},\n",
       "        'accuracy': 0.8858858858858859,\n",
       "        'macro avg': {'precision': 0.86612334547136,\n",
       "         'recall': 0.7992290516516498,\n",
       "         'f1-score': 0.8253970444165413,\n",
       "         'support': 2664.0},\n",
       "        'weighted avg': {'precision': 0.8824769628306998,\n",
       "         'recall': 0.8858858858858859,\n",
       "         'f1-score': 0.8801764799308712,\n",
       "         'support': 2664.0}},\n",
       "       'loss_report': {'no treatment': {'precision': 0.9052044609665427,\n",
       "         'recall': 0.9549019607843138,\n",
       "         'f1-score': 0.9293893129770993,\n",
       "         'support': 510.0},\n",
       "        'treatment': {'precision': 0.8203125,\n",
       "         'recall': 0.6730769230769231,\n",
       "         'f1-score': 0.7394366197183099,\n",
       "         'support': 156.0},\n",
       "        'accuracy': 0.8888888888888888,\n",
       "        'macro avg': {'precision': 0.8627584804832713,\n",
       "         'recall': 0.8139894419306184,\n",
       "         'f1-score': 0.8344129663477046,\n",
       "         'support': 666.0},\n",
       "        'weighted avg': {'precision': 0.8853198574969021,\n",
       "         'recall': 0.8888888888888888,\n",
       "         'f1-score': 0.8848958893308965,\n",
       "         'support': 666.0}}}}}}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_json_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
