{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae50bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c236a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.getenv(\"DATA_PATH\", \"data\")\n",
    "\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"models\")\n",
    "\n",
    "LOGS_PATH = os.getenv(\"LOGS_PATH\", \"logs\")\n",
    "\n",
    "RESULTS_PATH = os.getenv(\"RESULTS_PATH\", \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9edebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e806ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id', 'session_id', 'user_id', 'crisis_intro', 'crisis_option_a', 'crisis_option_b'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da766c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient Profile (Xp)\n",
    "xp_features = [\n",
    "    'age',\n",
    "    'location', 'eol_preference',\n",
    "    'family_preference', 'biological_sex', 'gender_identity', 'political_leaning',\n",
    "    'marital_status', 'religion', 'religious_importance', 'annual_income',\n",
    "    'education', 'family_history_dementia', 'personal_history_dementia',\n",
    "    'dementia_worry'\n",
    "]\n",
    "\n",
    "# Xt Modular Components\n",
    "xt_parts = {\n",
    "    'medical': ['crisis_type'],\n",
    "    'patient_condition': ['crisis_chance','emotional_state', 'agitation_frequency', 'agitation_severity',\n",
    "    'family_visit_frequency', 'family_inconvenience', 'interaction_ability',\n",
    "    'functional_ability', 'behavior', 'affordability'],\n",
    "    'treatment': ['crisis_wean', 'crisis_tube'],\n",
    "    'prognosis': ['crisis_comfort', 'resuscitation_chance', 'leave_hospital', 'internal_damage', 'future_arrest']\n",
    "}\n",
    "\n",
    "\n",
    "columns_to_drop = list(set(df.columns) - set(xp_features) - set(sum(xt_parts.values(), [])) - {\"choice\"})\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d720801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoding completed. Stored encoders for all categorical features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make a copy of the original dataframe\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Build list of categorical features to encode\n",
    "categorical_features = [col for col in xp_features if col not in ['religious_importance', 'dementia_worry']]\n",
    "\n",
    "# Add Xt features to encoding list\n",
    "for part in xt_parts.values():\n",
    "    for feature in part:\n",
    "        categorical_features.append(feature)\n",
    "\n",
    "# Drop duplicates just in case\n",
    "categorical_features = list(set(categorical_features))\n",
    "\n",
    "# Initialize encoder dictionary\n",
    "encoders = {}\n",
    "\n",
    "# Apply Label Encoding\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    encoders[col] = le  # Save encoder for later use\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "df_encoded['choice'] = target_encoder.fit_transform(df_encoded['choice'])\n",
    "\n",
    "# Map for later decoding\n",
    "print(\"‚úÖ Encoding completed. Stored encoders for all categorical features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79846568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix CPR variables to specific options across all rows\n",
    "# df_encoded['resuscitation_chance'] = encoders['resuscitation_chance'].transform(['low (around 9-in-100)'])[0]\n",
    "# df_encoded['leave_hospital'] = encoders['leave_hospital'].transform(['low (around 9-in-100)'])[0]\n",
    "# df_encoded['internal_damage'] = encoders['internal_damage'].transform(['strong chance (around 80-in-100)'])[0]\n",
    "# df_encoded['future_arrest'] = encoders['future_arrest'].transform(['moderate to high chance (around 70-in-100)'])[0]\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# for col in ['resuscitation_chance', 'leave_hospital', 'internal_damage', 'future_arrest']:\n",
    "#     le = LabelEncoder()\n",
    "#     df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "#     encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345fc2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2331 | Val size: 499 | Test size: 500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop(columns=['choice'])\n",
    "y = df_encoded['choice']\n",
    "\n",
    "# First split: Train + Temp (Val + Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Second split: Validation + Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(X_train)} | Val size: {len(X_val)} | Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e309e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiClassTreatmentDataset(Dataset):\n",
    "    def __init__(self, X, y, xp_features, xt_parts):\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y.reset_index(drop=True)\n",
    "        self.xp_features = xp_features\n",
    "        self.xt_parts = xt_parts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "\n",
    "        # Extract Xp\n",
    "        Xp = {feature: torch.tensor(row[feature], dtype=torch.float32).unsqueeze(0) for feature in self.xp_features}\n",
    "\n",
    "        # Extract Xt parts\n",
    "        Xt_parts = {}\n",
    "        for part_name, features in self.xt_parts.items():\n",
    "            Xt_parts[part_name] = {feature: torch.tensor(row[feature], dtype=torch.float32).unsqueeze(0) for feature in features}\n",
    "\n",
    "        # Target\n",
    "        y_target = torch.tensor(self.y.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        return Xp, Xt_parts, y_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b07f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultiClassTreatmentDataset(X_train, y_train, xp_features, xt_parts)\n",
    "val_dataset = MultiClassTreatmentDataset(X_val, y_val, xp_features, xt_parts)\n",
    "test_dataset = MultiClassTreatmentDataset(X_test, y_test, xp_features, xt_parts)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60987bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModularMultiClassTreatmentModel(nn.Module):\n",
    "    def __init__(self, xp_features, xt_parts, embedding_sizes, num_classes):\n",
    "        super(ModularMultiClassTreatmentModel, self).__init__()\n",
    "\n",
    "        self.xp_features = xp_features\n",
    "        self.xt_parts = xt_parts\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # ===== Embeddings =====\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for feature, num_categories in embedding_sizes.items():\n",
    "            self.embeddings[feature] = nn.Embedding(num_embeddings=num_categories, embedding_dim=4)\n",
    "\n",
    "        # ===== Patient Encoder =====\n",
    "        patient_input_dim = self.calculate_total_embedding_dim(xp_features)\n",
    "        self.patient_encoder = nn.Sequential(\n",
    "            nn.Linear(patient_input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # ===== Xt Part Encoders =====\n",
    "        self.part_encoders = nn.ModuleDict()\n",
    "        for part_name, features in xt_parts.items():\n",
    "            part_input_dim = self.calculate_total_embedding_dim(features)\n",
    "            self.part_encoders[part_name] = nn.Sequential(\n",
    "                nn.Linear(part_input_dim, 64),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # ===== Attention Mechanism =====\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        # ===== Final Decision Head =====\n",
    "        self.decision_head = nn.Sequential(\n",
    "            nn.Linear(64 + 64, 128),  # patient vector + weighted Xt vector\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)  # Multi-class output\n",
    "        )\n",
    "\n",
    "    def calculate_total_embedding_dim(self, features):\n",
    "        dim = 0\n",
    "        for feature in features:\n",
    "            dim += 4 if feature in self.embeddings else 1  # Categorical: embedding | Continuous: 1-dim\n",
    "        return dim\n",
    "\n",
    "    def forward(self, Xp, Xt_parts):\n",
    "        # ===== Process Patient Features =====\n",
    "        patient_embeds = []\n",
    "        for feature in self.xp_features:\n",
    "            if feature in self.embeddings:\n",
    "                patient_embeds.append(self.embeddings[feature](Xp[feature].long()).squeeze(1))\n",
    "            else:\n",
    "                patient_embeds.append(Xp[feature].unsqueeze(1))  # üî• FIXED HERE\n",
    "\n",
    "        patient_embeds = torch.cat(patient_embeds, dim=1)\n",
    "        patient_vector = self.patient_encoder(patient_embeds)\n",
    "\n",
    "        # ===== Process Xt Parts =====\n",
    "        part_vectors = []\n",
    "        attention_scores = []\n",
    "\n",
    "        for part_name, features in self.xt_parts.items():\n",
    "            part_embeds = []\n",
    "            for feature in features:\n",
    "                if feature in self.embeddings:\n",
    "                    part_embeds.append(self.embeddings[feature](Xt_parts[part_name][feature].long()).squeeze(1))\n",
    "                else:\n",
    "                    part_embeds.append(Xt_parts[part_name][feature].unsqueeze(1))  # üî• FIXED HERE\n",
    "\n",
    "            part_embeds = torch.cat(part_embeds, dim=1)\n",
    "            part_vector = self.part_encoders[part_name](part_embeds)\n",
    "            part_vectors.append(part_vector)\n",
    "\n",
    "            attn_score = self.attention(part_vector)\n",
    "            attention_scores.append(attn_score)\n",
    "\n",
    "        # ===== Attention Weighted Sum =====\n",
    "        attention_scores = torch.cat(attention_scores, dim=1)\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        weighted_Xt = torch.stack(part_vectors, dim=1)  # Shape: [batch_size, num_parts, 64]\n",
    "        weighted_Xt = (attn_weights.unsqueeze(2) * weighted_Xt).sum(dim=1)\n",
    "\n",
    "        # ===== Final Decision =====\n",
    "        combined = torch.cat([patient_vector, weighted_Xt], dim=1)\n",
    "        logits = self.decision_head(combined)\n",
    "\n",
    "        return logits  # Raw logits for CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f342f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding sizes for all categorical features\n",
    "embedding_sizes = {}\n",
    "\n",
    "# Go through each categorical feature we encoded earlier\n",
    "for col in categorical_features:\n",
    "    num_categories = df_encoded[col].nunique()\n",
    "    embedding_sizes[col] = num_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541f8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ModularMultiClassTreatmentModel(xp_features, xt_parts, embedding_sizes, num_classes=len(df_encoded['choice'].unique()))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function for multi-class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1813ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30, device='cpu'):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for Xp_batch, Xt_parts_batch, y_batch in train_loader:\n",
    "            Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "            Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xp_batch, Xt_parts_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xp_batch, Xt_parts_batch, y_batch in val_loader:\n",
    "                Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "                Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                logits = model(Xp_batch, Xt_parts_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "                correct += (preds == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'models/best_base_model.pth')\n",
    "            print(\"‚úÖ Best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"‚èπÔ∏è Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f566674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "def test_model(model, test_loader, criterion, encoder, type = \"Test\", device='cpu'):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xp_batch, Xt_parts_batch, y_batch in test_loader:\n",
    "            Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "            Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(Xp_batch, Xt_parts_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"{type} Loss: {avg_loss:.4f}\")\n",
    "    print(f\"{type} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{type} Precision: {precision:.4f}\")\n",
    "    print(f\"{type} Recall: {recall:.4f}\")\n",
    "    print(f\"{type} F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "    print(\"===== Classification Report =====\\n\")\n",
    "    print(classification_report(all_labels, all_preds, zero_division=0, target_names=encoder.classes_))\n",
    "\n",
    "    print(\"===== Confusion Matrix =====\\n\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e936664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7628\n",
      "Test Accuracy: 0.2455\n",
      "Test Precision: 0.0603\n",
      "Test Recall: 0.2455\n",
      "Test F1 Score: 0.0968\n",
      "\n",
      "===== Classification Report =====\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "no treatment       0.00      0.00      0.00       338\n",
      "   treatment       0.25      1.00      0.39       110\n",
      "\n",
      "    accuracy                           0.25       448\n",
      "   macro avg       0.12      0.50      0.20       448\n",
      "weighted avg       0.06      0.25      0.10       448\n",
      "\n",
      "===== Confusion Matrix =====\n",
      "\n",
      "[[  0 338]\n",
      " [  0 110]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7628026945250375,\n",
       " 0.24553571428571427,\n",
       " 0.060287786989795915,\n",
       " 0.24553571428571427,\n",
       " 0.09680619559651818)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, test_loader, criterion, target_encoder, type = \"Test\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e88b78db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Train Loss: 0.4404 | Val Loss: 0.3088 | Val Acc: 0.8549\n",
      "‚úÖ Best model saved!\n",
      "Epoch [2/50] Train Loss: 0.2751 | Val Loss: 0.2384 | Val Acc: 0.8996\n",
      "‚úÖ Best model saved!\n",
      "Epoch [3/50] Train Loss: 0.2139 | Val Loss: 0.1838 | Val Acc: 0.9375\n",
      "‚úÖ Best model saved!\n",
      "Epoch [4/50] Train Loss: 0.1575 | Val Loss: 0.1390 | Val Acc: 0.9509\n",
      "‚úÖ Best model saved!\n",
      "Epoch [5/50] Train Loss: 0.1210 | Val Loss: 0.1174 | Val Acc: 0.9665\n",
      "‚úÖ Best model saved!\n",
      "Epoch [6/50] Train Loss: 0.0935 | Val Loss: 0.0917 | Val Acc: 0.9777\n",
      "‚úÖ Best model saved!\n",
      "Epoch [7/50] Train Loss: 0.0760 | Val Loss: 0.0774 | Val Acc: 0.9754\n",
      "‚úÖ Best model saved!\n",
      "Epoch [8/50] Train Loss: 0.0713 | Val Loss: 0.0736 | Val Acc: 0.9799\n",
      "‚úÖ Best model saved!\n",
      "Epoch [9/50] Train Loss: 0.0505 | Val Loss: 0.0670 | Val Acc: 0.9799\n",
      "‚úÖ Best model saved!\n",
      "Epoch [10/50] Train Loss: 0.0443 | Val Loss: 0.0692 | Val Acc: 0.9799\n",
      "Epoch [11/50] Train Loss: 0.0331 | Val Loss: 0.0541 | Val Acc: 0.9844\n",
      "‚úÖ Best model saved!\n",
      "Epoch [12/50] Train Loss: 0.0282 | Val Loss: 0.0560 | Val Acc: 0.9844\n",
      "Epoch [13/50] Train Loss: 0.0238 | Val Loss: 0.0625 | Val Acc: 0.9799\n",
      "Epoch [14/50] Train Loss: 0.0272 | Val Loss: 0.0558 | Val Acc: 0.9821\n",
      "Epoch [15/50] Train Loss: 0.0221 | Val Loss: 0.0565 | Val Acc: 0.9821\n",
      "Epoch [16/50] Train Loss: 0.0188 | Val Loss: 0.0636 | Val Acc: 0.9799\n",
      "‚èπÔ∏è Early stopping triggered!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36625ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0258\n",
      "Test Accuracy: 0.9911\n",
      "Test Precision: 0.9911\n",
      "Test Recall: 0.9911\n",
      "Test F1 Score: 0.9911\n",
      "\n",
      "===== Classification Report =====\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "no treatment       0.99      0.99      0.99       338\n",
      "   treatment       0.98      0.98      0.98       110\n",
      "\n",
      "    accuracy                           0.99       448\n",
      "   macro avg       0.99      0.99      0.99       448\n",
      "weighted avg       0.99      0.99      0.99       448\n",
      "\n",
      "===== Confusion Matrix =====\n",
      "\n",
      "[[336   2]\n",
      " [  2 108]]\n"
     ]
    }
   ],
   "source": [
    "baseline_loss, baseline_accuracy, baseline_precision, baseline_recall, baseline_f1 = test_model(model, test_loader, criterion, target_encoder, type = \"Test\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4faabf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choice\n",
       "0    2552\n",
       "1     778\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded[\"choice\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e9dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0067\n",
      "Train Accuracy: 0.9987\n",
      "Train Precision: 0.9987\n",
      "Train Recall: 0.9987\n",
      "Train F1 Score: 0.9987\n",
      "\n",
      "===== Classification Report =====\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "no treatment       1.00      1.00      1.00      1766\n",
      "   treatment       0.99      1.00      1.00       538\n",
      "\n",
      "    accuracy                           1.00      2304\n",
      "   macro avg       1.00      1.00      1.00      2304\n",
      "weighted avg       1.00      1.00      1.00      2304\n",
      "\n",
      "===== Confusion Matrix =====\n",
      "\n",
      "[[1763    3]\n",
      " [   0  538]]\n"
     ]
    }
   ],
   "source": [
    "baseline_loss, baseline_accuracy, baseline_precision, baseline_recall, baseline_f1 = test_model(model, train_loader, criterion, target_encoder, type = \"Train\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67dfefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def loss_based_classification_report(model, data_loader, criterion, encoder, device='cpu', type='Test'):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_losses = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xp_batch, Xt_parts_batch, y_batch in data_loader:\n",
    "            Xp_batch = {k: v.to(device).squeeze(1) for k, v in Xp_batch.items()}\n",
    "            Xt_parts_batch = {part: {f: v.to(device).squeeze(1) for f, v in features.items()} for part, features in Xt_parts_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(Xp_batch, Xt_parts_batch)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_losses.extend([loss.item()] * y_batch.size(0))\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_losses = np.array(all_losses)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Per-sample loss using negative log likelihood for true class\n",
    "    per_sample_losses = -np.log(np.choose(all_labels, all_probs.T) + 1e-12)\n",
    "\n",
    "    # Group losses by class\n",
    "    loss_by_class = defaultdict(list)\n",
    "    for label, sample_loss in zip(all_labels, per_sample_losses):\n",
    "        loss_by_class[label].append(sample_loss)\n",
    "\n",
    "    # Build Loss Classification Report\n",
    "    print(f\"\\n===== {type} Loss-Based Classification Report =====\\n\")\n",
    "    print(f\"{'Class':<25}{'Average Loss':<15}{'Support':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for class_idx, losses in loss_by_class.items():\n",
    "        avg_loss = np.mean(losses)\n",
    "        support = len(losses)\n",
    "        class_name = encoder.inverse_transform([class_idx])[0]\n",
    "        print(f\"{class_name:<25}{avg_loss:<15.4f}{support:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7212230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Test Loss-Based Classification Report =====\n",
      "\n",
      "Class                    Average Loss   Support   \n",
      "--------------------------------------------------\n",
      "treatment                0.0432         110       \n",
      "no treatment             0.0201         338       \n"
     ]
    }
   ],
   "source": [
    "loss_based_classification_report(model, test_loader, criterion, target_encoder, device=device, type=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61c564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
